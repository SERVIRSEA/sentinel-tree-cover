{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree segmentation with multitemporal Sentinel 1/2 imagery\n",
    "\n",
    "## John Brandt\n",
    "## December 2023\n",
    "\n",
    "## This notebook finetunes the TTC decoder for a new task\n",
    "\n",
    "## Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/work/mambaforge/envs/tf2.9/lib/python3.10/site-packages (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/work/mambaforge/envs/tf2.9/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Metal device set to: Apple M3 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 16:19:41.200238: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-15 16:19:41.200445: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "sess = tf.Session()\n",
    "#from keras import backend as K\n",
    "#K.set_session(sess)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "#from tensorflow.compat.v1.ops import array_ops\n",
    "\n",
    "from tensorflow.compat.v1.layers import *\n",
    "from tensorflow.compat.v1.initializers import glorot_normal, lecun_normal\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "#from tensorflow.contrib.framework import arg_scope\n",
    "#from keras.regularizers import l1\n",
    "#from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/layers/zoneout.py\n",
    "%run ../src/layers/losses.py\n",
    "%run ../src/layers/adabound.py\n",
    "%run ../src/layers/adabelief.py\n",
    "\n",
    "%run ../src/layers/convgru.py\n",
    "%run ../src/layers/dropblock.py\n",
    "%run ../src/layers/extra_layers.py\n",
    "%run ../src/layers/stochastic_weight_averaging.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/preprocessing/slope.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.90\n",
    "ACTIVATION_FUNCTION = 'swish'\n",
    "\n",
    "INITIAL_LR = 1e-3\n",
    "DROPBLOCK_MAXSIZE = 5\n",
    "\n",
    "N_CONV_BLOCKS = 1\n",
    "FINAL_ALPHA = 0.33\n",
    "LABEL_SMOOTHING = 0.03\n",
    "\n",
    "L2_REG = 0.\n",
    "BATCH_SIZE = 32\n",
    "MAX_DROPBLOCK = 0.6\n",
    "\n",
    "FRESH_START = True\n",
    "best_val = 0.2\n",
    "\n",
    "START_EPOCH = 1\n",
    "END_EPOCH = 100\n",
    "\n",
    "n_bands = 17\n",
    "initial_flt = 32\n",
    "mid_flt = 32 * 2\n",
    "high_flt = 32 * 2 * 2\n",
    "\n",
    "temporal_model = True\n",
    "bi = True\n",
    "input_size_x = 684\n",
    "input_size_y = 220\n",
    "\n",
    "output_size = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layer definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv GRU Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_block(inp, length, size, flt, scope, train, normalize = True, bi = False):\n",
    "    '''Bidirectional convolutional GRU block with \n",
    "       zoneout and CSSE blocks in each time step\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): (B, T, H, W, C) layer\n",
    "          length (tf.Variable): (B, T) layer denoting number of\n",
    "                                steps per sample\n",
    "          size (int): kernel size of convolution\n",
    "          flt (int): number of convolution filters\n",
    "          scope (str): tensorflow variable scope\n",
    "          train (tf.Bool): flag to differentiate between train/test ops\n",
    "          normalize (bool): whether to compute layer normalization\n",
    "\n",
    "         Returns:\n",
    "          gru (tf.Variable): (B, H, W, flt*2) bi-gru output\n",
    "          steps (tf.Variable): (B, T, H, W, flt*2) output of each step\n",
    "    '''\n",
    "    with tf.variable_scope(scope):\n",
    "        print(f\"GRU input shape {inp.shape}, zoneout: {ZONE_OUT_PROB}\")\n",
    "        \n",
    "        # normalize is internal group normalization within the reset gate\n",
    "        # sse is internal SSE block within the state cell\n",
    "\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', \n",
    "                           normalize = normalize, sse = True)\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "           cell_fw, zoneout_drop_prob = 0.75, is_training = train)\n",
    "        if bi:\n",
    "            cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                               kernel = [3, 3], padding = 'VALID',\n",
    "                               normalize = normalize, sse = True)\n",
    "            cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = 0.75, is_training = train)\n",
    "                \n",
    "            steps, out = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        else:\n",
    "            steps, out = fconvGRU(inp, cell_fw, length)\n",
    "        gru = tf.concat(out, axis = -1)\n",
    "        steps = tf.concat(steps, axis = -1)\n",
    "        print(f\"GRU block output shape {gru.shape}\")\n",
    "    return gru, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg = tf.contrib.layers.l2_regularizer(0.)\n",
    "n_bands = 17\n",
    "output_size_x = input_size_x - 14\n",
    "output_size_y = input_size_y - 14\n",
    "\n",
    "if temporal_model:\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, 5, input_size_x, input_size_y, n_bands))\n",
    "    length = tf.placeholder_with_default(np.full((1,), 4), shape = (None,))\n",
    "else:\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, input_size, input_size, n_bands  * 5))\n",
    "    length = tf.placeholder_with_default(np.full((1,), 4), shape = (None,))\n",
    "    \n",
    "labels = tf.placeholder(tf.float32, shape=(None, output_size_x, output_size_y))#, 1))\n",
    "mask = tf.placeholder(tf.float32, shape = (None, output_size_x, output_size_y))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training') # For DropBlock\n",
    "alpha = tf.placeholder(tf.float32, shape = ()) # For loss scheduling\n",
    "ft_lr = tf.placeholder_with_default(0.001, shape = ()) # For loss scheduling\n",
    "loss_weight = tf.placeholder_with_default(1.0, shape = ())\n",
    "beta_ = tf.placeholder_with_default(0.0, shape = ()) # For loss scheduling, not currently implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU input shape (?, 4, 684, 220, 17), zoneout: 0.9\n",
      "(3, 3, 49, 64)\n",
      "(3, 3, 49, 64)\n",
      "GRU block output shape (?, 684, 220, 64)\n",
      "conv_median 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv_median_conv/conv_median/x/mul:0\", shape=(?, 684, 220, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"IdentityN:0\", shape=(?, 684, 220, 64), dtype=float32)\n",
      "WARNING:tensorflow:From /Users/work/Documents/GitHub/sentinel-tree-cover/src/layers/extra_layers.py:316: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/work/Documents/GitHub/sentinel-tree-cover/src/layers/extra_layers.py:405: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  update_mask = tf.layers.conv2d(mask, filters=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_concat 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv_concat_conv/conv_concat/x/mul:0\", shape=(?, 684, 220, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"IdentityN_1:0\", shape=(?, 684, 220, 64), dtype=float32)\n",
      "Concat: (?, 684, 220, 64)\n",
      "conv1 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv1_conv/conv1/ws_conv2d_2/WSConv2D:0\", shape=(?, 340, 108, 128), dtype=float32)\n",
      "The non normalized feats are Tensor(\"IdentityN_2:0\", shape=(?, 340, 108, 128), dtype=float32)\n",
      "Conv1: (?, 340, 108, 128)\n",
      "conv2 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv2_conv/conv2/ws_conv2d_3/WSConv2D:0\", shape=(?, 168, 52, 256), dtype=float32)\n",
      "The non normalized feats are Tensor(\"IdentityN_3:0\", shape=(?, 168, 52, 256), dtype=float32)\n",
      "Encoded (?, 168, 52, 256)\n",
      "up2 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"up2_conv/up2/x/mul:0\", shape=(?, 336, 104, 128), dtype=float32)\n",
      "The non normalized feats are Tensor(\"IdentityN_4:0\", shape=(?, 336, 104, 128), dtype=float32)\n",
      "(?, 336, 104, 128)\n",
      "up2_out 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"up2_out_conv/up2_out/x/mul:0\", shape=(?, 336, 104, 128), dtype=float32)\n",
      "The non normalized feats are Tensor(\"IdentityN_5:0\", shape=(?, 336, 104, 128), dtype=float32)\n",
      "up3 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"up3_conv/up3/x/mul:0\", shape=(?, 672, 208, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"IdentityN_6:0\", shape=(?, 672, 208, 64), dtype=float32)\n",
      "(?, 672, 208, 64)\n",
      "(?, 672, 208, 64)\n",
      "out 3 Conv 2D Group Norm RELU CSSE NoBias NoDrop\n",
      "The non normalized feats are Tensor(\"out_conv/out/ws_conv2d_7/WSConv2D:0\", shape=(?, 670, 206, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"IdentityN_7:0\", shape=(?, 670, 206, 64), dtype=float32)\n",
      "The output is (?, 336, 104, 128), with a receptive field of 1\n",
      "Tensor(\"conv2d/Sigmoid:0\", shape=(?, 670, 206, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# master modmel is 32, 64, 96, 230k paramms\n",
    "initial_flt = 64\n",
    "mid_flt = initial_flt * 2\n",
    "high_flt = initial_flt * 2 * 2\n",
    "\n",
    "if bi:\n",
    "    divider = 2\n",
    "else:\n",
    "    divider = 1\n",
    "\n",
    "if temporal_model:\n",
    "    gru_input = inp[:, :-1, ...]\n",
    "    gru, steps = gru_block(inp = gru_input, length = length,\n",
    "                                size = [input_size_x, input_size_y, ], # + 2 here for refleclt pad\n",
    "                                flt = initial_flt // divider,\n",
    "                                scope = 'down_16',\n",
    "                                train = is_training, bi = bi)\n",
    "    with tf.variable_scope(\"gru_drop\"):\n",
    "        drop_block = DropBlock2D(keep_prob=keep_rate, block_size=4)\n",
    "        gru = drop_block(gru, is_training)\n",
    "        \n",
    "    # Median conv\n",
    "    median_input = inp[:, -1, ...]\n",
    "else:\n",
    "    median_input = inp\n",
    "    \n",
    "median_conv = conv_swish_gn(inp = median_input, is_training = False, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_median', filters = initial_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None,\n",
    "                            window_size = 104)\n",
    "#print(f\"Median conv: {median_conv.shape}\")\n",
    "\n",
    "if temporal_model:\n",
    "    concat1 = tf.concat([gru, median_conv], axis = -1)\n",
    "\n",
    "else:\n",
    "    concat1 = median_conv\n",
    "\n",
    "concat = conv_swish_gn(inp = concat1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_concat', filters = initial_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, padding = \"SAME\",\n",
    "                       window_size = 104)\n",
    "print(f\"Concat: {concat.shape}\")\n",
    "\n",
    "# MaxPool-conv-swish-GroupNorm-csse\n",
    "pool1 = MaxPool2D()(concat)\n",
    "conv1 = conv_swish_gn(inp = pool1, is_training = False, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv1', filters = mid_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True, padding = \"VALID\",\n",
    "            csse = True, dropblock = True, weight_decay = None)\n",
    "print(f\"Conv1: {conv1.shape}\")\n",
    "\n",
    "# MaxPool-conv-swish-csse-DropBlock\n",
    "pool2 = MaxPool2D()(conv1)\n",
    "conv2 = conv_swish_gn(inp = pool2, is_training = False, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv2', filters = high_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, block_size = 4, padding = \"VALID\",\n",
    "                     window_size = 24)\n",
    "print(\"Encoded\", conv2.shape)\n",
    "\n",
    "# Decoder 4 - 8, upsample-conv-swish-csse-concat-conv-swish\n",
    "up2 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(conv2)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2', filters = mid_flt, \n",
    "                    keep_rate = keep_rate, activation = True,use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "conv1_crop = Cropping2D(2)(conv1)\n",
    "print(conv1_crop.shape)\n",
    "up2 = tf.concat([up2, conv1_crop], -1)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2_out', filters = mid_flt, \n",
    "                    keep_rate =  keep_rate, activation = True,use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "\n",
    "# Decoder 8 - 14 upsample-conv-swish-csse-concat-conv-swish\n",
    "up3 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(up2)\n",
    "#up3 = ReflectionPadding2D((1, 1,))(up3)\n",
    "up3 = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up3', filters = initial_flt, \n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None, \n",
    "                    window_size = 104)\n",
    "gru_crop = Cropping2D(6)(concat)\n",
    "print(up3.shape)\n",
    "print(gru_crop.shape)\n",
    "up3 = tf.concat([up3, gru_crop], -1)\n",
    "\n",
    "up3out = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'out', filters = initial_flt, \n",
    "                    keep_rate  = keep_rate, activation = True,use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = False, weight_decay = None, padding = \"VALID\",\n",
    "                       window_size = 104)\n",
    "\n",
    "init = tf.constant_initializer([-np.log(0.7/0.3)]) # For focal loss\n",
    "print(f\"The output is {up2.shape}, with a receptive field of {1}\")\n",
    "fm = tf.layers.Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1),\n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init,)(up3out)#, # name = 'conv2d'\n",
    "print(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is best to finetune the decoder, freeze the convolutional layers in the encoder,\n",
    "# and finetune the attention layers and the normalization in the encoder\n",
    "# Decoder\n",
    "finetune_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_5\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up3\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up3_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up3\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_out_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up2_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up2\")# + \\\n",
    "\n",
    "# Encoder, 4x downsample norm + csse\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2_norm\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"csse_conv2\")\n",
    "\n",
    "# Encoder, 2x downsample norm + csse\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv1_norm\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"csse_conv1\")\n",
    "#finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv_concat\")\n",
    "\n",
    "# Concat conv high res norm + csse\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv_concat_norm\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"csse_conv_concat_conv\")\n",
    "\n",
    "# Median conv high res norm + csse\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv_median_norm\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"csse_conv_median_conv\")\n",
    "\n",
    "# Conv GRU, norm only\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/candidate/candidate_y_norm/\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_u_norm/\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_r_norm\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/candidate/candidate_y_norm/\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_u_norm/\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_r_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'down_16/bidirectional_rnn/fw/conv_gru_cell/gates/kernel:0' shape=(3, 3, 49, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'down_16/bidirectional_rnn/fw/conv_gru_cell/candidate/kernel:0' shape=(3, 3, 49, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'down_16/bidirectional_rnn/fw/conv_gru_cell/candidate/kernel_1:0' shape=(1, 1, 32, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'down_16/bidirectional_rnn/bw/conv_gru_cell/gates/kernel:0' shape=(3, 3, 49, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'down_16/bidirectional_rnn/bw/conv_gru_cell/candidate/kernel:0' shape=(3, 3, 49, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'down_16/bidirectional_rnn/bw/conv_gru_cell/candidate/kernel_1:0' shape=(1, 1, 32, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'conv_median_conv/conv_median/x/ws_conv2d/kernel:0' shape=(3, 3, 17, 64) dtype=float32>,\n",
       " <tf.Variable 'conv_concat_conv/conv_concat/x/ws_conv2d_1/kernel:0' shape=(3, 3, 128, 64) dtype=float32>,\n",
       " <tf.Variable 'conv1_conv/conv1/ws_conv2d_2/kernel:0' shape=(3, 3, 64, 128) dtype=float32>,\n",
       " <tf.Variable 'conv2_conv/conv2/ws_conv2d_3/kernel:0' shape=(3, 3, 128, 256) dtype=float32>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The frozen variables are the convolutional layers in the encoder\n",
    "[x for x in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"\") if x not in finetune_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 16:19:45.814823: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2024-04-15 16:19:45.924321: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2024-04-15 16:19:45.927078: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "def grad_norm(gradients):\n",
    "    norm = tf.compat.v1.norm(\n",
    "        tf.stack([\n",
    "            tf.compat.v1.norm(grad) for grad in gradients if grad is not None\n",
    "        ])\n",
    "    )\n",
    "    return norm\n",
    "\n",
    "FRESH_START = True\n",
    "#print(f\"Starting model with: \\n {ZONE_OUT_PROB} zone out \\n {L2_REG} l2 \\n\"\n",
    " #     f\"{INITIAL_LR} initial LR \\n {total_parameters} parameters\")  \n",
    "\n",
    "#OUT = input_size - 14\n",
    "if FRESH_START:\n",
    "    # We use the Adabound optimizer\n",
    "    optimizer = AdaBoundOptimizer(1e-4, 1e-2)#2e-4, 2e-2)\n",
    "    #optimizer = AdaBeliefOptimizer(1e-3)#2e-4, 2e-2)\n",
    "    #train_loss1 = logcosh(tf.reshape(labels, (-1, 14, 14, 1)), output) \n",
    "    \n",
    "    train_loss2 = bce_surface_loss(tf.reshape(labels, (-1, input_size_x - 14, input_size_y - 14, 1)), fm,\n",
    "                                  weight = loss_weight, \n",
    "                             alpha = alpha, beta = beta_, mask = mask)\n",
    "\n",
    "    train_loss = train_loss2# + train_loss2\n",
    "    \n",
    "    # If there is any L2 regularization, add it. Current model does not use\n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    if len(tf.losses.get_regularization_losses()) > 0:\n",
    "        train_loss = train_loss + l2_loss\n",
    "        \n",
    "    test_loss = bce_surface_loss(tf.reshape(labels, (-1, input_size_x - 14, input_size_y - 14, 1)),\n",
    "                            fm, weight = loss_weight, \n",
    "                            alpha = alpha, beta = beta_, mask = mask)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss)#, var_list = finetune_vars)   \n",
    "        #ft_op = ft_optimizer.minimize(train_loss)\n",
    "    \n",
    "    # The following code blocks are for sharpness aware minimization\n",
    "    # Adapted from https://github.com/sayakpaul/Sharpness-Aware-Minimization-TensorFlow\n",
    "    # For tensorflow 1.15\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "    gradient_norm = grad_norm(gradients)\n",
    "    scale = 0.05 / (gradient_norm + 1e-12)\n",
    "    e_ws = []\n",
    "    for (grad, param) in gradients:\n",
    "        e_w = grad * scale\n",
    "        param.assign_add(e_w)\n",
    "        e_ws.append(e_w)\n",
    "\n",
    "    sam_gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "    for (param, e_w) in zip(trainable_params, e_ws):\n",
    "        param.assign_sub(e_w)\n",
    "    train_step = optimizer.apply_gradients(sam_gradients)\n",
    "    \n",
    "    # Create a saver to save the model each epoch\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 150)#, var_list = all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_saver_varlist(path):\n",
    "\n",
    "    current_items = []\n",
    "    vars_dict = {}\n",
    "    for var_current in tf.global_variables():\n",
    "        current_items.append(var_current) \n",
    "    names = [x.op.name for x in current_items]\n",
    "    names = np.argsort(names)\n",
    "    current_items = [current_items[x] for x in names]\n",
    "    \n",
    "    ckpt_items = []\n",
    "    for var_ckpt in tf.train.list_variables(path):\n",
    "        if 'save' in var_ckpt[0]:\n",
    "            print(var_ckpt[0])\n",
    "        if 'BackupVariables' not in var_ckpt[0]:\n",
    "            if 'StochasticWeightAveraging' not in var_ckpt[0]:\n",
    "                if 'global_step' not in var_ckpt[0]:\n",
    "                    if 'is_training' not in var_ckpt[0]:\n",
    "                         if 'Momentum' not in var_ckpt[0]:\n",
    "                             if 'n_models' not in var_ckpt[0]:\n",
    "                                if 'save' not in var_ckpt[0]:\n",
    "                                    ckpt_items.append(var_ckpt[0])\n",
    "    \n",
    "    ckptdict = {}\n",
    "    for y, x in zip(ckpt_items, current_items):\n",
    "        ckptdict[y] = x\n",
    "    return ckptdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckptdict = make_saver_varlist( '../models/master-76-60-28/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting anew\n",
      "INFO:tensorflow:Restoring parameters from ../models/master-76-60-28/-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 16:19:46.328419: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(ckptdict)\n",
    "#model_path  = \"../models/nov6-5-96-avg-continue-2/RESWA26-15-87-5/\"\n",
    "model_path = '../models/master-76-60-28/'\n",
    "#model_path = '../models/tf2-nov6-96-bi-master/'\n",
    "FRESH_START = False\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")\n",
    "else:\n",
    "    print(\"Starting anew\")\n",
    "#if not FRESH_START:\n",
    "path = model_path\n",
    "saver.restore(sess, tf.train.latest_checkpoint(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/tf2-ard-up/predict_graph\n"
     ]
    }
   ],
   "source": [
    "output_node_names = ['conv2d/Sigmoid']\n",
    "frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess,\n",
    "    sess.graph_def,\n",
    "    output_node_names)\n",
    "\n",
    "\n",
    "# Save the frozen graph\n",
    "print(f'../models/tf2-ard-up/predict_graph')\n",
    "with open(f'../models/tf2-ard-up/predict_graph.pb', 'wb') as f:\n",
    "    f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-9",
   "language": "python",
   "name": "tf2-9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
