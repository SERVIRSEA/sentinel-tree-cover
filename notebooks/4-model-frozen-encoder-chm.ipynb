{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree segmentation with multitemporal Sentinel 1/2 imagery\n",
    "\n",
    "\n",
    "## Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/Documents/GitHub/sentinel-tree-cover/src/layers/convgru.py:27: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ../src/layers/zoneout.py\n",
    "%run ../src/layers/adabound.py\n",
    "%run ../src/layers/convgru.py\n",
    "%run ../src/layers/dropblock.py\n",
    "%run ../src/layers/extra_layers.py\n",
    "%run ../src/layers/stochastic_weight_averaging.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/preprocessing/slope.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.90\n",
    "ACTIVATION_FUNCTION = 'swish'\n",
    "\n",
    "INITIAL_LR = 1e-3\n",
    "DROPBLOCK_MAXSIZE = 5\n",
    "\n",
    "N_CONV_BLOCKS = 1\n",
    "FINAL_ALPHA = 0.33\n",
    "LABEL_SMOOTHING = 0.03\n",
    "\n",
    "L2_REG = 0.\n",
    "BATCH_SIZE = 32\n",
    "MAX_DROPBLOCK = 0.6\n",
    "\n",
    "FRESH_START = True\n",
    "best_val = 0.2\n",
    "\n",
    "START_EPOCH = 1\n",
    "END_EPOCH = 100\n",
    "\n",
    "n_bands = 17\n",
    "initial_flt = 40\n",
    "mid_flt = 40 * 2\n",
    "high_flt = 40 * 2 * 2\n",
    "\n",
    "temporal_model = True\n",
    "input_size = 28\n",
    "output_size = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layer definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility blocks (Batch norm, cSSE, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse_block(prevlayer, prefix):\n",
    "    '''Spatial excitation and channel squeeze layer.\n",
    "       Calculates a 1x1 convolution with sigmoid activation to create a \n",
    "       spatial map that is multiplied by the input layer\n",
    "\n",
    "         Parameters:\n",
    "          prevlayer (tf.Variable): input layer\n",
    "          prefix (str): prefix for tensorflow scope\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): output of the sse_block\n",
    "    '''\n",
    "    conv = Conv2D(1, (1, 1), padding=\"same\", kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    '''Adapted from https://github.com/mronta/CycleGAN-in-Keras/blob/master/reflection_padding.py\n",
    "       This is used instead of zero padding where possible to reduce boundary artifacts\n",
    "    '''\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv GRU Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_block(inp, length, size, flt, scope, train, normalize = True):\n",
    "    '''Bidirectional convolutional GRU block with \n",
    "       zoneout and CSSE blocks in each time step\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): (B, T, H, W, C) layer\n",
    "          length (tf.Variable): (B, T) layer denoting number of\n",
    "                                steps per sample\n",
    "          size (int): kernel size of convolution\n",
    "          flt (int): number of convolution filters\n",
    "          scope (str): tensorflow variable scope\n",
    "          train (tf.Bool): flag to differentiate between train/test ops\n",
    "          normalize (bool): whether to compute layer normalization\n",
    "\n",
    "         Returns:\n",
    "          gru (tf.Variable): (B, H, W, flt*2) bi-gru output\n",
    "          steps (tf.Variable): (B, T, H, W, flt*2) output of each step\n",
    "    '''\n",
    "    with tf.variable_scope(scope):\n",
    "        print(f\"GRU input shape {inp.shape}, zoneout: {ZONE_OUT_PROB}\")\n",
    "        \n",
    "        # normalize is internal group normalization within the reset gate\n",
    "        # sse is internal SSE block within the state cell\n",
    "\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', \n",
    "                           normalize = normalize, sse = True)\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID',\n",
    "                           normalize = normalize, sse = True)\n",
    "        \n",
    "        cell_fw = ZoneoutWrapper(\n",
    "           cell_fw, zoneout_drop_prob = 0.75, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = 0.75, is_training = train)\n",
    "        steps, out = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        gru = tf.concat(out, axis = -1)\n",
    "        steps = tf.concat(steps, axis = -1)\n",
    "        print(f\"GRU block output shape {gru.shape}\")\n",
    "    return gru, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_conv(x, channels, kernel=3, stride=1, use_bias=False, padding='SAME', scope='conv_0'):\n",
    "    \"\"\"Implementation of https://arxiv.org/abs/1804.07723\n",
    "       Partial conv is used in place of \"SAME\" padding to reduce boundary artifacts\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        if padding.lower() == 'SAME'.lower() :\n",
    "            with tf.variable_scope('mask'):\n",
    "                _, h, w, _ = x.get_shape().as_list()\n",
    "\n",
    "                slide_window = kernel * kernel\n",
    "                mask = tf.ones(shape=[1, h, w, 1])\n",
    "\n",
    "                update_mask = tf.layers.conv2d(mask, filters=1,\n",
    "                                               kernel_size=kernel,\n",
    "                                               kernel_initializer=tf.constant_initializer(1.0),\n",
    "                                               strides=stride, \n",
    "                                               padding=padding, \n",
    "                                               use_bias=False,\n",
    "                                               trainable=False)\n",
    "\n",
    "                mask_ratio = slide_window / (update_mask + 1e-8)\n",
    "                update_mask = tf.clip_by_value(update_mask, 0.0, 1.0)\n",
    "                mask_ratio = mask_ratio * update_mask\n",
    "\n",
    "            with tf.variable_scope('x'):\n",
    "                x = tf.layers.conv2d(x, filters=channels,\n",
    "                                     kernel_size=kernel, \n",
    "                                     kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                     strides=stride,\n",
    "                                     padding=padding, \n",
    "                                     use_bias=False)\n",
    "                x = x * mask_ratio\n",
    "\n",
    "                if use_bias:\n",
    "                    bias = tf.get_variable(\"bias\", \n",
    "                                           [channels],\n",
    "                                           initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "                    x = tf.nn.bias_add(x, bias)\n",
    "                    x = x * update_mask\n",
    "\n",
    "        else :\n",
    "            x = tf.layers.conv2d(x, filters=channels,\n",
    "                                 kernel_size=kernel, \n",
    "                                 kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                 strides=stride,\n",
    "                                 padding=padding,\n",
    "                                 use_bias=use_bias)\n",
    "\n",
    "        return x\n",
    "\n",
    "def conv_swish_gn(inp, \n",
    "                 is_training, \n",
    "                 kernel_size,\n",
    "                 scope,\n",
    "                 filters, \n",
    "                 keep_rate,\n",
    "                 stride = (1, 1),\n",
    "                 activation = True,\n",
    "                 use_bias = False,\n",
    "                 norm = True,\n",
    "                 dropblock = True,\n",
    "                 csse = True,\n",
    "                 weight_decay = None,\n",
    "                 block_size = 5,\n",
    "                 padding = \"SAME\",\n",
    "                 partial = True):\n",
    "    '''2D convolution, group normalization, SWISH activation, drop block, SSE. \n",
    "       DropBlock performs best when applied last, according to original paper.\n",
    "       This is the core CONV block for this model.\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): input layer\n",
    "          is_training (str): flag to differentiate between train/test ops\n",
    "          kernel_size (int): size of convolution\n",
    "          scope (str): tensorflow variable scope\n",
    "          filters (int): number of filters for convolution\n",
    "          keep_rate (float): Keep rate for dropblock\n",
    "          stride (tuple): Conv2D stride parameter\n",
    "          activation (bool): Whether or not to apply Swish activation\n",
    "          use_bias (bool): whether to use bias. Should always be false\n",
    "          norm (bool): whether or not to apply group normalization\n",
    "          dropblock (bool): whether or not to apply dropblock\n",
    "          csse (bool): whether or not to apply SSE block\n",
    "          weight_decay (bool): not currently implemented\n",
    "          block_size (int): Block size for dropblock\n",
    "          padding (str): padding parameter for conv2d\n",
    "          partial (bool): Whether or not to use partial conv or Conv2D\n",
    "\n",
    "         Returns:\n",
    "          conv (tf.Variable): output of the block\n",
    "        \n",
    "         References:\n",
    "          http://papers.nips.cc/paper/8271-dropblock-a-regularization-\n",
    "              method-for-convolutional-networks.pdf\n",
    "          https://arxiv.org/abs/1702.03275\n",
    "          \n",
    "    '''\n",
    "    \n",
    "    gn_flag = \"Group Norm\" if norm else \"\"\n",
    "    activation_flag = \"RELU\" if activation else \"Linear\"\n",
    "    sse_flag = \"SSE\" if csse else \"No SSE\"\n",
    "    bias_flag = \"Bias\" if use_bias else \"NoBias\"\n",
    "    drop_flag = \"DropBlock\" if dropblock else \"NoDrop\"\n",
    "        \n",
    "    print(f\"{scope} Conv: Kernel: {kernel_size}, {gn_flag}, {activation_flag}, {sse_flag}, {drop_flag}\")\n",
    "\n",
    "    with tf.variable_scope(scope + \"_conv\"):\n",
    "        if not partial:\n",
    "            conv = Conv2D(filters = filters, \n",
    "                          kernel_size = (kernel_size, kernel_size), \n",
    "                          strides = stride,\n",
    "                          activation = None,\n",
    "                          padding = 'valid',\n",
    "                          use_bias = use_bias,\n",
    "                          kernel_initializer = tf.keras.initializers.he_normal()\n",
    "                         )(inp)\n",
    "        if partial:\n",
    "            conv = partial_conv(inp, filters,\n",
    "                                kernel=kernel_size, \n",
    "                                stride=1, \n",
    "                                use_bias=False,\n",
    "                                padding=padding, \n",
    "                                scope = scope)\n",
    "    if activation:\n",
    "        conv = tf.nn.swish(conv)\n",
    "    if norm:\n",
    "        conv = group_norm(x = conv, scope = scope, G = 8)\n",
    "    if csse:\n",
    "        conv = sse_block(conv, \"csse_\" + scope)\n",
    "    if dropblock: \n",
    "        with tf.variable_scope(scope + \"_drop\"):\n",
    "            drop_block = DropBlock2D(keep_prob=keep_rate, block_size= block_size)\n",
    "            conv = drop_block(conv, is_training)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "reg = tf.contrib.layers.l2_regularizer(0.)\n",
    "temporal_model = True\n",
    "input_size = 28\n",
    "n_bands = 17\n",
    "output_size = 14\n",
    "\n",
    "if temporal_model:\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, 5, input_size, input_size, n_bands))\n",
    "    length = tf.placeholder_with_default(np.full((1,), 12), shape = (None,))\n",
    "else:\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, input_size, input_size, n_bands))\n",
    "    \n",
    "labels = tf.placeholder(tf.float32, shape=(None, output_size, output_size))#, 1))\n",
    "labels_height = tf.placeholder(tf.float32, shape=(None, output_size, output_size))\n",
    "labels_gedi = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training') # For DropBlock\n",
    "alpha = tf.placeholder(tf.float32, shape = ()) # For loss scheduling\n",
    "ft_lr = tf.placeholder_with_default(0.001, shape = ()) # For loss scheduling\n",
    "loss_weight = tf.placeholder_with_default(1.0, shape = ())\n",
    "beta_ = tf.placeholder_with_default(0.0, shape = ()) # For loss scheduling, not currently implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model uses a UNet architecture where the encoder extracts increasingly abstract features and the decoder upsamples the features to the target resolution.\n",
    "\n",
    "The encoder consists of three blocks:\n",
    "\n",
    "- GRU: A bidirectional convolutional GRU with channel squeeze and spatial excitation, and group normalization, extracts 3x3 features from the multitemporal imagery\n",
    "- Conv1: A MaxPool-conv-swish-groupNorm-csse layer takes the output of the GRU (size 28) and reduces to size 12\n",
    "- Conv2: The output of the MaxPool-conv-swish-csse-DropBlock is a 4x4x128 encoded feature map\n",
    "\n",
    "The decoder consists of two blocks:\n",
    "\n",
    "- Upconv1: upsample-conv-swish-csse-concat-conv-swish\n",
    "- Upconv2: upsample-conv-swish-csse-concat-conv-swish\n",
    "- Output sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU input shape (?, 4, 28, 28, 17), zoneout: 0.9\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "(3, 3, 37, 40)\n",
      "(3, 3, 37, 40)\n",
      "GRU block output shape (?, 28, 28, 40)\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "conv_median Conv: Kernel: 3, Group Norm, RELU, SSE, DropBlock\n",
      "Median conv: (?, 28, 28, 40)\n",
      "conv_concat Conv: Kernel: 3, Group Norm, RELU, SSE, DropBlock\n",
      "Concat: (?, 28, 28, 40)\n",
      "conv1 Conv: Kernel: 3, Group Norm, RELU, SSE, DropBlock\n",
      "Conv1: (?, 12, 12, 80)\n",
      "conv2 Conv: Kernel: 3, Group Norm, RELU, SSE, DropBlock\n",
      "Encoded (?, 4, 4, 160)\n",
      "up2 Conv: Kernel: 3, Group Norm, RELU, SSE, DropBlock\n",
      "(?, 8, 8, 80)\n",
      "up2_out Conv: Kernel: 3, Group Norm, RELU, SSE, DropBlock\n",
      "up3 Conv: Kernel: 3, Group Norm, RELU, SSE, DropBlock\n",
      "(?, 16, 16, 40)\n",
      "(?, 16, 16, 40)\n",
      "out Conv: Kernel: 3, Group Norm, RELU, SSE, NoDrop\n",
      "The output is (?, 8, 8, 80), with a receptive field of 1\n",
      "outregressor Conv: Kernel: 3, Group Norm, RELU, SSE, DropBlock\n",
      "outregressor2 Conv: Kernel: 3, Group Norm, RELU, SSE, NoDrop\n"
     ]
    }
   ],
   "source": [
    "# master modmel is 32, 64, 96, 230k paramms\n",
    "initial_flt = 40\n",
    "mid_flt = 40 * 2\n",
    "high_flt = 40 * 2 * 2\n",
    "INPUT_SIZE = 28\n",
    "SIZE_X = 28\n",
    "\n",
    "#inp = ReflectionPadding5D((1, 1))(inp)\n",
    "gru_input = inp[:, :-1, ...]\n",
    "gru, steps = gru_block(inp = gru_input, length = length,\n",
    "                            size = [INPUT_SIZE, SIZE_X, ], # + 2 here for refleclt pad\n",
    "                            flt = initial_flt // 2,\n",
    "                            scope = 'down_16',\n",
    "                            train = is_training)\n",
    "with tf.variable_scope(\"gru_drop\"):\n",
    "    drop_block = DropBlock2D(keep_prob=keep_rate, block_size=4)\n",
    "    gru = drop_block(gru, is_training)\n",
    "    \n",
    "# Median conv\n",
    "median_input = inp[:, -1, ...]\n",
    "median_conv = conv_swish_gn(inp = median_input, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_median', filters = initial_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None)\n",
    "\n",
    "print(f\"Median conv: {median_conv.shape}\")\n",
    "\n",
    "concat1 = tf.concat([gru, median_conv], axis = -1)\n",
    "concat = conv_swish_gn(inp = concat1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_concat', filters = initial_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, padding = \"SAME\")\n",
    "print(f\"Concat: {concat.shape}\")\n",
    "\n",
    "    \n",
    "# MaxPool-conv-swish-GroupNorm-csse\n",
    "pool1 = MaxPool2D()(concat)\n",
    "conv1 = conv_swish_gn(inp = pool1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv1', filters = mid_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True, padding = \"VALID\",\n",
    "            csse = True, dropblock = True, weight_decay = None)\n",
    "print(f\"Conv1: {conv1.shape}\")\n",
    "\n",
    "# MaxPool-conv-swish-csse-DropBlock\n",
    "pool2 = MaxPool2D()(conv1)\n",
    "conv2 = conv_swish_gn(inp = pool2, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv2', filters = high_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, block_size = 4, padding = \"VALID\")\n",
    "print(\"Encoded\", conv2.shape)\n",
    "\n",
    "# Decoder 4 - 8, upsample-conv-swish-csse-concat-conv-swish\n",
    "up2 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(conv2)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2', filters = mid_flt, \n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "conv1_crop = Cropping2D(2)(conv1)\n",
    "print(conv1_crop.shape)\n",
    "up2 = tf.concat([up2, conv1_crop], -1)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2_out', filters = mid_flt, \n",
    "                    keep_rate =  keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "\n",
    "# Decoder 8 - 14 upsample-conv-swish-csse-concat-conv-swish\n",
    "up3 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(up2)\n",
    "#up3 = ReflectionPadding2D((1, 1,))(up3)\n",
    "up3 = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up3', filters = initial_flt, \n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "gru_crop = Cropping2D(6)(concat)\n",
    "print(up3.shape)\n",
    "print(gru_crop.shape)\n",
    "up3 = tf.concat([up3, gru_crop], -1)\n",
    "\n",
    "up3out = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'out', filters = initial_flt, \n",
    "                    keep_rate  = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = False, weight_decay = None, padding = \"VALID\")\n",
    "\n",
    "\n",
    "#print(\"Initializing last sigmoid bias with -2.94 constant\")\n",
    "init = tf.constant_initializer([-np.log(0.7/0.3)]) # For focal loss\n",
    "print(f\"The output is {up2.shape}, with a receptive field of {1}\")\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1),\n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init,\n",
    "           )(up3out) # For focal loss\n",
    "\n",
    "\n",
    "up4 = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'outregressor', filters = mid_flt, \n",
    "                    keep_rate  = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock =True, weight_decay = None, padding = \"SAME\")\n",
    "#up4 = ReflectionPadding2D((1, 1,))(up4)\n",
    "up5 = conv_swish_gn(inp = up4, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'outregressor2', filters = mid_flt, \n",
    "                    keep_rate  = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = False, weight_decay = None, padding = \"VALID\")\n",
    "\n",
    "height = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1),\n",
    "            padding = 'valid',\n",
    "            activation = 'linear',\n",
    "            #bias_initializer = init,\n",
    "           )(up5)\n",
    "\n",
    "\n",
    "\n",
    "height = tf.clip_by_value(height, 0, 60)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_15\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_17\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"outregressor\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_outregressor\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"outregressor_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"outregressor2\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_outregressor2\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_13\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up3\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up3_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up3\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_out_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up2_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up2\")# + \\\n",
    "\n",
    "\n",
    "new_varsinit = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_15\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_17\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"outregressor\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_outregressor\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"outregressor_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"outregressor2\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_outregressor2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 624652 parameters\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(f\"This model has {total_parameters} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "import math\n",
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight, mask = True, smooth = 0.03):\n",
    "    '''Calculates the weighted binary cross entropy loss between y_true and\n",
    "       y_pred with optional masking and smoothing for regularization\n",
    "       \n",
    "       For smoothing, we want to weight false positives as less important than\n",
    "       false negatives, so we smooth false negatives 2x as much. \n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          y_pred (arr):\n",
    "          weight (float):\n",
    "          mask (arr): DEPRECATED\n",
    "          smooth (float):\n",
    "\n",
    "         Returns:\n",
    "          loss (float):\n",
    "    '''\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    y_true = K.clip(y_true, smooth, 1. - smooth)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        y_true,\n",
    "        logit_y_pred,\n",
    "        weight,\n",
    "    )\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_dist_map(seg):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is a modified version of surface loss to reduce\n",
    "    the loss importance of samples at the boundary. THe original\n",
    "    paper specifies a boundary loss of 0, but the low-resolution of\n",
    "    Sentinel data requires some amount of boundary importance, and\n",
    "    this function works well to accomplish that task\n",
    "    \"\"\"\n",
    "\n",
    "    res = np.zeros_like(seg)\n",
    "    posmask = seg.astype(np.bool)\n",
    "\n",
    "    mults = np.ones_like(seg)\n",
    "    ones = np.ones_like(seg)\n",
    "    for x in range(1, res.shape[0] -1 ):\n",
    "        for y in range(1, res.shape[0] - 1):\n",
    "            # If > 1 px distance, double the weight of the positive\n",
    "            # If == 1 px, half the weight of the negative\n",
    "            # This is important because the calc_mask fn\n",
    "            # leaves borders with 0 weight otherwise\n",
    "            if seg[x, y] == 1:\n",
    "                l = seg[x - 1, y]\n",
    "                r = seg[x + 1, y]\n",
    "                u = seg[x, y + 1]\n",
    "                d = seg[x, y - 1]\n",
    "                lu = seg[x - 1, y + 1]\n",
    "                ru = seg[x + 1, y + 1]\n",
    "                rd = seg[x + 1, y - 1]\n",
    "                ld = seg[x -1, y - 1]\n",
    "                \n",
    "                sums = (l + r + u + d)\n",
    "                sums2 = (l + r + u + d + lu + ru +rd + ld)\n",
    "                if sums >= 2:\n",
    "                    mults[x, y] = 2\n",
    "                if sums2 <= 1:\n",
    "                    ones[x - 1, y] = 0.5\n",
    "                    ones[x + 1, y] = 0.5\n",
    "                    ones[x, y + 1] = 0.5\n",
    "                    ones[x, y - 1] = 0.5\n",
    "                    ones[x - 1, y + 1] = 0.5\n",
    "                    ones[x + 1, y + 1] = 0.5\n",
    "                    ones[x + 1, y - 1] = 0.5\n",
    "                    ones[x -1, y - 1] = 0.5\n",
    "\n",
    "    if posmask.any():\n",
    "        negmask = ~posmask\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n",
    "        # When % = 1, 0 -> 1.75\n",
    "        # When % = 100, 0 -> 0\n",
    "        res = np.round(res, 0)\n",
    "        res[np.where(np.isclose(res, -.41421356, rtol = 1e-2))] = -1\n",
    "        res[np.where(res == -1)] = -1 * mults[np.where(res == -1)]\n",
    "        res[np.where(res == 0)] = -1  * mults[np.where(res == 0)]\n",
    "        # When % = 1, 1 -> 0\n",
    "        # When % = 100, 1 -> 1.75\n",
    "        res[np.where(res == 1)] = 1 * ones[np.where(res == 1)]\n",
    "        res[np.where(res == 1)] *= 0.67\n",
    "        \n",
    "    # Empirically capping the loss at -3 to 3 is better\n",
    "    res[np.where(res < -3)] = -3\n",
    "    res[np.where(res > 3)] = 3\n",
    "    if np.sum(seg) == 196:\n",
    "        res = np.ones_like(seg)\n",
    "        res *= -1\n",
    "    if np.sum(seg) == 0:\n",
    "        res = np.ones_like(seg)\n",
    "    return res\n",
    "\n",
    "\n",
    "def calc_dist_map_batch(y_true):\n",
    "    '''Applies calc_dist_map to each sample in an input batch\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "    '''\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    return np.array([calc_dist_map(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "\n",
    "\n",
    "def surface_loss(y_true, y_pred):\n",
    "    '''Calculates the mean surface loss for the input batch\n",
    "       by multiplying the distance map by y_pred\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          y_pred (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "        \n",
    "         References:\n",
    "          https://arxiv.org/abs/1812.07032\n",
    "    '''\n",
    "    y_true_dist_map = tf.py_function(func=calc_dist_map_batch,\n",
    "                                     inp=[y_true],\n",
    "                                     Tout=tf.float32)\n",
    "    y_true_dist_map = tf.stack(y_true_dist_map, axis = 0)\n",
    "    multipled = y_pred * y_true_dist_map\n",
    "    loss = tf.reduce_mean(multipled, axis = (1, 2, 3))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def bce_surface_loss(y_true, y_pred, alpha, weight, beta):\n",
    "    \n",
    "    \"\"\" Combines surface loss and binary cross entropy with a \n",
    "    alpha weighting factor. Beta is currently deprecated.\"\"\"\n",
    "    \n",
    "    bce = weighted_bce_loss(y_true = y_true, \n",
    "                             y_pred = y_pred, \n",
    "                             weight = weight,\n",
    "                             smooth = 0.03)\n",
    "\n",
    "    bce = tf.reduce_mean(bce, axis = (1, 2, 3))\n",
    "    surface = surface_loss(tf.cast(tf.math.greater(y_true, 0.10), tf.float32), y_pred)\n",
    "    surface = tf.reduce_mean(surface)\n",
    "\n",
    "    bce = tf.reduce_mean(bce)\n",
    "    bce = (1 - alpha) * bce\n",
    "    surface_portion = alpha * surface\n",
    "    result = bce + surface_portion\n",
    "    return result\n",
    "\n",
    "def logcosh(y_true, y_pred, gedi):\n",
    "      \n",
    "  \n",
    "    y_gedi = tf.math.reduce_max(y_pred[:, 6:-6, 6:-6], axis = (1, 2))\n",
    "    y_gedi = ops.convert_to_tensor(y_gedi)\n",
    "    gedi = math_ops.cast(gedi, y_gedi.dtype)\n",
    "    y_pred = ops.convert_to_tensor(y_pred)\n",
    "    y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "\n",
    "\n",
    "    def _logcosh(x):\n",
    "        return x + tf.nn.softplus(-2. * x) - math_ops.log(2.)\n",
    "    sq_error = tf.squared_difference(y_true, y_pred)\n",
    "    mask = y_true < 255\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    sq_error *= mask\n",
    "    \n",
    "    sq_error_gedi = tf.squared_difference(gedi, y_gedi)\n",
    "    \n",
    "    return (0.8*tf.reduce_mean(sq_error)) + (0.2 * tf.reduce_mean(sq_error_gedi))\n",
    "\n",
    "def logit(x):\n",
    "    \"\"\" Computes the logit function, i.e. the logistic sigmoid inverse. \"\"\"\n",
    "    return - tf.log(1. / x - 1.)\n",
    "\n",
    "def nplogit(x):\n",
    "    \"\"\" Computes the logit function, i.e. the logistic sigmoid inverse. \"\"\"\n",
    "    return - np.log(1. / x - 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model with: \n",
      " 0.9 zone out \n",
      " 0.0 l2 \n",
      "0.001 initial LR \n",
      " 624652 parameters\n"
     ]
    }
   ],
   "source": [
    "def grad_norm(gradients):\n",
    "    norm = tf.compat.v1.norm(\n",
    "        tf.stack([\n",
    "            tf.compat.v1.norm(grad) for grad in gradients if grad is not None\n",
    "        ])\n",
    "    )\n",
    "    return norm\n",
    "\n",
    "FRESH_START = True\n",
    "print(f\"Starting model with: \\n {ZONE_OUT_PROB} zone out \\n {L2_REG} l2 \\n\"\n",
    "      f\"{INITIAL_LR} initial LR \\n {total_parameters} parameters\")  \n",
    "\n",
    "if FRESH_START:\n",
    "    # We use the Adabound optimizer\n",
    "    optimizer = AdaBoundOptimizer(INITIAL_LR, ft_lr)\n",
    "\n",
    "    train_loss = bce_surface_loss(tf.reshape(labels, (-1, 14, 14, 1)), fm,\n",
    "                                  weight = loss_weight, \n",
    "                             alpha = alpha, beta = beta_)\n",
    "    \n",
    "    train_loss1 = logcosh(tf.reshape(labels_height, (-1, 14, 14, 1)), height, labels_gedi) \n",
    "    # 3 * 0.15 + 0.33 * 3 \n",
    "    # 0.45 1\n",
    "    train_loss = (4 * train_loss) + (0.25 * train_loss1)\n",
    "\n",
    "    \n",
    "    # If there is any L2 regularization, add it. Current model does not use\n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    if len(tf.losses.get_regularization_losses()) > 0:\n",
    "        train_loss = train_loss + l2_loss\n",
    "\n",
    "    \n",
    "    test_loss = bce_surface_loss(tf.reshape(labels, (-1, 14, 14, 1)),\n",
    "                            fm, weight = loss_weight, \n",
    "                            alpha = alpha, beta = beta_)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss, var_list = finetune_vars)   \n",
    "        #ft_op = ft_optimizer.minimize(train_loss)\n",
    "    \"\"\"\n",
    "    # The following code blocks are for sharpness aware minimization\n",
    "    # Adapted from https://github.com/sayakpaul/Sharpness-Aware-Minimization-TensorFlow\n",
    "    # For tensorflow 1.15\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "    gradient_norm = grad_norm(gradients)\n",
    "    scale = 0.05 / (gradient_norm + 1e-12)\n",
    "    e_ws = []\n",
    "    for (grad, param) in gradients:\n",
    "        e_w = grad * scale\n",
    "        param.assign_add(e_w)\n",
    "        e_ws.append(e_w)\n",
    "\n",
    "    sam_gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "    for (param, e_w) in zip(trainable_params, e_ws):\n",
    "        param.assign_sub(e_w)\n",
    "    train_step = optimizer.apply_gradients(sam_gradients)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a saver to save the model each epoch\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    all_vars = [x for x in all_vars if 'outregressor' not in x.name]\n",
    "    all_vars = [x for x in all_vars if 'conv2d_15' not in x.name]\n",
    "    all_vars = [x for x in all_vars if 'conv2d_17' not in x.name]\n",
    "    all_vars = [x for x in all_vars if 'outregressor2' not in x.name]\n",
    "    saver = tf.train.Saver(max_to_keep = 150, var_list = all_vars)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting anew\n",
      "INFO:tensorflow:Restoring parameters from ../models/tml-swa/-0\n"
     ]
    }
   ],
   "source": [
    "FRESH_START = False\n",
    "model_path  = \"../models/tml-swa/\"\n",
    "saver = tf.train.Saver(max_to_keep = 150, var_list = all_vars)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")\n",
    "else:\n",
    "    print(\"Starting anew\")\n",
    "    metrics = np.zeros((6, 300))\n",
    "\n",
    "if not FRESH_START:\n",
    "    path = model_path\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(path))\n",
    "\n",
    "sess.run(tf.variables_initializer(new_varsinit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of stochastic weight averaging\n",
    "\n",
    "\n",
    "        \n",
    "model_vars = tf.trainable_variables()\n",
    "swa = StochasticWeightAveraging()\n",
    "swa_op = swa.apply(var_list=model_vars)\n",
    "with tf.variable_scope('BackupVariables'):\n",
    "    # force tensorflow to keep theese new variables on the CPU ! \n",
    "    backup_vars = [tf.get_variable(var.op.name, dtype=var.value().dtype, trainable=False,\n",
    "                                   initializer=var.initialized_value())\n",
    "                   for var in model_vars]\n",
    "\n",
    "# operation to assign SWA weights to model\n",
    "swa_to_weights = tf.group(*(tf.assign(var, swa.average(var).read_value()) for var in model_vars))\n",
    "# operation to store model into backup variables\n",
    "save_weight_backups = tf.group(*(tf.assign(bck, var.read_value()) for var, bck in zip(model_vars, backup_vars)))\n",
    "# operation to get back values from backup variables to model\n",
    "restore_weight_backups = tf.group(*(tf.assign(var, bck.read_value()) for var, bck in zip(model_vars, backup_vars)))\n",
    "\n",
    "initialize_uninitialized(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "*  Load in CSV data from Collect Earth\n",
    "*  Reconstruct the X, Y grid for the Y data per sample\n",
    "*  Calculate remote sensing indices\n",
    "*  Stack X, Y, length data\n",
    "*  Apply median filter to DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "\n",
    "train_x = hkl.load(\"../data/train/train_x2.hkl\")#[:1000]\n",
    "train_y = hkl.load(\"../data/train/train_y2.hkl\")#[:1000]\n",
    "train_y = train_y / 255\n",
    "train_chm = hkl.load(\"../data/train/train_chm2.hkl\")#[:1000]\n",
    "data = pd.read_csv(\"../data/train/train_plot_ids2.csv\")#[:1000]\n",
    "\n",
    "train_chm[train_y < 0.05] = 0.\n",
    "\n",
    "train_x = np.delete(train_x, 11, -1)\n",
    "train_y = train_y #/ 255.\n",
    "if not isinstance(train_x.flat[0], np.floating):\n",
    "    assert np.max(train_x) > 1\n",
    "    train_x = np.float32(train_x) / 65535.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val in data.iterrows():\n",
    "    p95 = np.percentile(train_chm[i, 6:-6, 6:-6], 95)\n",
    "    vh95 = data.gedi_rh95[i]\n",
    "    shift = vh95 - p95\n",
    "    if shift > 0:\n",
    "        train_chm[i][train_chm[i] > 3] += shift\n",
    "\n",
    "for i, val in data.iterrows():\n",
    "    is_bad = (train_y[i] > 0.5) * (train_chm[i] < 3)\n",
    "    train_chm[i][is_bad] = 255\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_db(x, min_db):\n",
    "    \"\"\"Converts sigma backscatter to decibel\"\"\"\n",
    "    x = 10 * np.log10(x + 1/65535)\n",
    "    x[x < -min_db] = -min_db\n",
    "    x = x + min_db\n",
    "    x = x / min_db\n",
    "    x = np.clip(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def grndvi(array):\n",
    "    nir = np.clip(array[..., 3], 0, 1)\n",
    "    green = np.clip(array[..., 1], 0, 1)\n",
    "    red = np.clip(array[..., 2], 0, 1)\n",
    "    denominator = (nir+(green+red)) + 1e-5\n",
    "    return (nir-(green+red)) / denominator\n",
    "\n",
    "\n",
    "def evi(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates the enhanced vegetation index\n",
    "    2.5 x (08 - 04) / (08 + 6 * 04 - 7.5 * 02 + 1)\n",
    "    '''\n",
    "\n",
    "    BLUE = x[..., 0]\n",
    "    GREEN = x[..., 1]\n",
    "    RED = x[..., 2]\n",
    "    NIR = x[..., 3]\n",
    "    evis = 2.5 * ( (NIR-RED) / (NIR + (6*RED) - (7.5*BLUE) + 1))\n",
    "    evis = np.clip(evis, -1.5, 1.5)\n",
    "    return evis\n",
    "\n",
    "\n",
    "def msavi2(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates the modified soil-adjusted vegetation index 2\n",
    "    (2 * NIR + 1 - sqrt((2*NIR + 1)^2 - 8*(NIR-RED)) / 2\n",
    "    '''\n",
    "    BLUE = x[..., 0]\n",
    "    GREEN = x[..., 1]\n",
    "    RED = np.clip(x[..., 2], 0, 1)\n",
    "    NIR = np.clip(x[..., 3], 0, 1)\n",
    "\n",
    "    msavis = (2 * NIR + 1 - np.sqrt( (2*NIR+1)**2 - 8*(NIR-RED) )) / 2\n",
    "    return msavis\n",
    "\n",
    "\n",
    "def bi(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    \"\"\"Bare soil index\"\"\"\n",
    "    B11 = np.clip(x[..., 8], 0, 1)\n",
    "    B4 = np.clip(x[..., 2], 0, 1)\n",
    "    B8 = np.clip(x[..., 3], 0, 1)\n",
    "    B2 = np.clip(x[..., 0], 0, 1)\n",
    "    bis = ((B11 + B4) - (B8 + B2)) / ((B11 + B4) + (B8 + B2))\n",
    "    return bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[..., -1] = convert_to_db(train_x[..., -1], 22)\n",
    "train_x[..., -2] = convert_to_db(train_x[..., -2], 22)\n",
    "\n",
    "indices = np.zeros((train_x.shape[0], 12, 28, 28, 4), dtype = np.float32)\n",
    "indices[..., 0] = evi(train_x)\n",
    "indices[..., 1] = bi(train_x)\n",
    "indices[..., 2] = msavi2(train_x)\n",
    "indices[..., 3] = grndvi(train_x)\n",
    "\n",
    "train_x = np.concatenate([train_x, indices], axis = -1)\n",
    "train_x = np.float32(train_x)\n",
    "med = np.median(train_x, axis = 1)\n",
    "\n",
    "med = med[:, np.newaxis, :, :, :]\n",
    "train_x = np.concatenate([train_x, med], axis = 1)\n",
    "train_x = np.float32(train_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:14: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3012ec48bd2a4952945b0b9702a2f2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_all = [0.006576638437476157, 0.0162050812542916, 0.010040436408026246, \n",
    "           0.013351644159609368, 0.01965362020294499, 0.014229037918669413, \n",
    "           0.015289539940489814, 0.011993591210803388, 0.008239871824216068, \n",
    "           0.006546120393682765, 0.0, 0.0, 0.0, -0.1409399364817101, \n",
    "           -0.4973397113668104, -0.09731556326714398, -0.7193834232943873]\n",
    "\n",
    "max_all = [0.2691233691920348, 0.3740291447318227, 0.5171435111009385, 0.6027466239414053,\n",
    "           0.5650263218127718, 0.5747005416952773, 0.5933928435187305, 0.6034943160143434, \n",
    "           0.7472037842374304, 0.7000076295109483, 0.509269855802243, 0.948334642387533, \n",
    "           0.6729257769285485, 0.8177635298774327, 0.35768999002433816, 0.7545951919107605, \n",
    "           0.7602693339366691]\n",
    "\n",
    "# Min all, and max all are the 0.1 and 99.9 percentiles of each band\n",
    "for band in tnrange(0, train_x.shape[-1]):\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    train_x[..., band] = np.clip(train_x[..., band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (train_x[..., band] - midrange) / (rng / 2)\n",
    "    train_x[..., band] = standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "outliers = np.argwhere(np.sum(np.isnan(train_x), axis = (1, 2, 3, 4)) > 0).flatten()\n",
    "print(outliers)\n",
    "train_x = np.delete(train_x , outliers, 0)\n",
    "train_y = np.delete(train_y, outliers, 0)\n",
    "train_chm = np.delete(train_chm, outliers, 0)\n",
    "data = data.drop(outliers, 0)\n",
    "data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "outliers = np.argwhere((np.array(data.fn) > 20000)).flatten()\n",
    "train_x = np.delete(train_x , outliers, 0)\n",
    "train_y = np.delete(train_y, outliers, 0)\n",
    "train_chm = np.delete(train_chm, outliers, 0)\n",
    "data = data.drop(outliers, 0)\n",
    "data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_x = hkl.load(\"../data/train/train_x.hkl\")\\ntest_y = hkl.load(\"../data/train/train_y.hkl\")\\ntest_data = pd.read_csv(\"../data/train/train_plot_ids.csv\")\\n\\ntest_y = test_y / 255.\\n\\ntest_x = np.delete(test_x, 11, -1)\\n\\nif not isinstance(test_x.flat[0], np.floating):\\n    assert np.max(test_x) > 1\\n    test_x = test_x / 65535.\\n    \\ntest_x[..., -1] = convert_to_db(test_x[..., -1], 22)\\ntest_x[..., -2] = convert_to_db(test_x[..., -2], 22)\\n\\nindices = np.empty((test_x.shape[0], 12, 28, 28, 4))\\nindices[..., 0] = evi(test_x)\\nindices[..., 1] = bi(test_x)\\nindices[..., 2] = msavi2(test_x)\\nindices[..., 3] = grndvi(test_x)\\n\\ntest_x = np.concatenate([test_x, indices], axis = -1)\\nmed = np.median(test_x, axis = 1)\\nmed = med[:, np.newaxis, :, :, :]\\ntest_x = np.concatenate([test_x, med], axis = 1)\\n\\nbelow_1 = [i for i, val in enumerate(test_x[..., :-2]) if np.min(val) < -1.66]\\nabove_1 = [i for i, val in enumerate(test_x[..., :-2]) if np.max(val) > 1.66]\\nnans = [i for i, val in enumerate(test_x) if np.sum(np.isnan(val)) > 0]\\noutliers = below_1 + above_1 + nans\\noutliers = list(set(outliers))\\noutliers = [61, 141, 218, 296, 365, 386, 594, 679, 712, 976, 1909, 2015]\\nprint([x for x in test_data[\\'plotid\\'].iloc[outliers]])\\n\\ntest_x = np.delete(test_x, outliers, 0)\\ntest_y = np.delete(test_y, outliers, 0)\\ntest_data = test_data.drop(outliers, 0)\\ntest_data = test_data.reset_index(drop = True)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hickle as hkl\n",
    "\"\"\"\n",
    "test_x = hkl.load(\"../data/train/train_x.hkl\")\n",
    "test_y = hkl.load(\"../data/train/train_y.hkl\")\n",
    "test_data = pd.read_csv(\"../data/train/train_plot_ids.csv\")\n",
    "\n",
    "test_y = test_y / 255.\n",
    "\n",
    "test_x = np.delete(test_x, 11, -1)\n",
    "\n",
    "if not isinstance(test_x.flat[0], np.floating):\n",
    "    assert np.max(test_x) > 1\n",
    "    test_x = test_x / 65535.\n",
    "    \n",
    "test_x[..., -1] = convert_to_db(test_x[..., -1], 22)\n",
    "test_x[..., -2] = convert_to_db(test_x[..., -2], 22)\n",
    "\n",
    "indices = np.empty((test_x.shape[0], 12, 28, 28, 4))\n",
    "indices[..., 0] = evi(test_x)\n",
    "indices[..., 1] = bi(test_x)\n",
    "indices[..., 2] = msavi2(test_x)\n",
    "indices[..., 3] = grndvi(test_x)\n",
    "\n",
    "test_x = np.concatenate([test_x, indices], axis = -1)\n",
    "med = np.median(test_x, axis = 1)\n",
    "med = med[:, np.newaxis, :, :, :]\n",
    "test_x = np.concatenate([test_x, med], axis = 1)\n",
    "\n",
    "below_1 = [i for i, val in enumerate(test_x[..., :-2]) if np.min(val) < -1.66]\n",
    "above_1 = [i for i, val in enumerate(test_x[..., :-2]) if np.max(val) > 1.66]\n",
    "nans = [i for i, val in enumerate(test_x) if np.sum(np.isnan(val)) > 0]\n",
    "outliers = below_1 + above_1 + nans\n",
    "outliers = list(set(outliers))\n",
    "outliers = [61, 141, 218, 296, 365, 386, 594, 679, 712, 976, 1909, 2015]\n",
    "print([x for x in test_data['plotid'].iloc[outliers]])\n",
    "\n",
    "test_x = np.delete(test_x, outliers, 0)\n",
    "test_y = np.delete(test_y, outliers, 0)\n",
    "test_data = test_data.drop(outliers, 0)\n",
    "test_data = test_data.reset_index(drop = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor band in range(0, test_x.shape[-1]):\\n    mins = min_all[band]\\n    maxs = max_all[band]\\n    test_x[..., band] = np.clip(test_x[..., band], mins, maxs)\\n    midrange = (maxs + mins) / 2\\n    rng = maxs - mins\\n    standardized = (test_x[..., band] - midrange) / (rng / 2)\\n    test_x[..., band] = standardized\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for band in range(0, test_x.shape[-1]):\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    test_x[..., band] = np.clip(test_x[..., band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (test_x[..., band] - midrange) / (rng / 2)\n",
    "    test_x[..., band] = standardized\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equibatch creation\n",
    "\n",
    "The modelling approach uses equibatch sampling to ensure that there is a near constant standard deviation of the percent tree cover in the output labels for each batch. This helps ensure that the model performs equally well across gradients of tree cover, by mitigating the random possibility that many batches in a row near the end of sampling may be randomly biased towards a tree cover range.\n",
    "\n",
    "It was actually very important for stable training to implement equibatch as the risk of diverging based on bad batches was very high without equibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score_at_tolerance(true, pred, tolerance = 1):\n",
    "    \"\"\"Because of coregistration errors, we evaluate the model\n",
    "    where false positives/negatives must be >1px away from a true positive\n",
    "    \"\"\"\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    tp = np.zeros_like(true)\n",
    "    fp = np.zeros_like(true)\n",
    "    fn = np.zeros_like(true)\n",
    "    \n",
    "    for x in range(true.shape[0]):\n",
    "        for y in range(true.shape[1]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([true.shape[0], y+2])\n",
    "            max_x = np.min([true.shape[0], x+2])\n",
    "            if true[x, y] == 1:\n",
    "                if np.sum(pred[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    tp[x, y] = 1\n",
    "                else:\n",
    "                    fn[x, y] = 1\n",
    "            if pred[x, y] == 1:\n",
    "                if np.sum(true[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    if true[x, y] == 1:\n",
    "                        tp[x, y] = 1\n",
    "                else:\n",
    "                    fp[x, y] = 1                \n",
    "                \n",
    "    return np.sum(tp), np.sum(fp), np.sum(fn)\n",
    "\n",
    "def calculate_metrics(al = 0.4, canopy_thresh = 100):\n",
    "    '''Calculates the following metrics\n",
    "       \n",
    "         - Loss\n",
    "         - F1\n",
    "         - Precision\n",
    "         - Recall\n",
    "         - Dice\n",
    "         - Mean surface distance\n",
    "         - Average error\n",
    "    \n",
    "         Parameters:\n",
    "          al (float):\n",
    "          canopy_thresh (int)\n",
    "          \n",
    "         Returns:\n",
    "          val_loss (float):\n",
    "          best_dice (float):\n",
    "          error (float):\n",
    "    '''\n",
    "    start_idx = 0\n",
    "    stop_idx = len(test_x)\n",
    "    best_f1, best_thresh, relaxed_f1 = 0, 0, 0\n",
    "    preds, trues, vls = [], [], []\n",
    "\n",
    "    test_ids = [x for x in range(len(test_x))]\n",
    "    for test_sample in test_ids[start_idx:stop_idx]:\n",
    "        if np.sum(test_y[test_sample]) < ((canopy_thresh/100) * 197):\n",
    "            x_input = test_x[test_sample].reshape(1, 5, 28, 28, n_bands)\n",
    "            x_median_input = calc_median_input(x_input)\n",
    "            y, vl = sess.run([fm, test_loss], feed_dict={inp: x_input,\n",
    "                                                          length: np.full((1,), 4),\n",
    "                                                          is_training: False,\n",
    "                                                          labels: test_y[test_sample].reshape(1, 14, 14),\n",
    "                                                          loss_weight: 1.0,\n",
    "                                                          alpha: 0.33,\n",
    "                                                          })\n",
    "            preds.append(y.reshape((14, 14)))\n",
    "            vls.append(vl)\n",
    "            trues.append(test_y[test_sample].reshape((14, 14)))\n",
    "            \n",
    "    # These threshes are just for ROC\n",
    "    for thresh in range(7, 9):\n",
    "        tps_relaxed = np.empty((len(preds), ))\n",
    "        fps_relaxed = np.empty((len(preds), ))\n",
    "        fns_relaxed = np.empty((len(preds), ))\n",
    "        abs_error = np.empty((len(preds), ))\n",
    "        \n",
    "        for sample in range(len(preds)):\n",
    "            pred = np.copy(preds[sample])\n",
    "            true = trues[sample]\n",
    "        \n",
    "            pred[np.where(pred >= thresh*0.05)] = 1\n",
    "            pred[np.where(pred < thresh*0.05)] = 0\n",
    "            \n",
    "            true_s = np.sum(true[1:-1])\n",
    "            pred_s = np.sum(pred[1:-1])\n",
    "            abs_error[sample] = abs(true_s - pred_s)\n",
    "            tp_relaxed, fp_relaxed, fn_relaxed = compute_f1_score_at_tolerance(true, pred)\n",
    "            tps_relaxed[sample] = tp_relaxed\n",
    "            fps_relaxed[sample] = fp_relaxed\n",
    "            fns_relaxed[sample] = fn_relaxed                   \n",
    "            \n",
    "        oa_error = np.mean(abs_error)\n",
    "        precision_r = np.sum(tps_relaxed) / (np.sum(tps_relaxed) + np.sum(fps_relaxed))\n",
    "        recall_r = np.sum(tps_relaxed) / (np.sum(tps_relaxed) + np.sum(fns_relaxed))\n",
    "        f1_r = 2*((precision_r* recall_r) / (precision_r + recall_r))\n",
    "        \n",
    "        if f1_r > best_f1:\n",
    "            best_f1 = f1_r\n",
    "            p = precision_r\n",
    "            r = recall_r\n",
    "            error = oa_error\n",
    "            best_thresh = thresh*0.05\n",
    "\n",
    "    print(f\"Val loss: {np.around(np.mean(vls), 3)}\"\n",
    "          f\" Thresh: {np.around(best_thresh, 2)}\"\n",
    "          f\" F1: {np.around(best_f1, 3)} R: {np.around(p, 3)} P: {np.around(r, 3)}\"\n",
    "          f\" Error: {np.around(error, 3)}\")\n",
    "    return np.mean(vls), best_f1, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code block implements cut mix where random samples are spliced together where the output labels have similar tree cover distributions (within the same kmeans cluster). Not super necessary but does give a small performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "LEN = 4\n",
    "def augment_batch(batch_ids, batch_size):\n",
    "    '''Performs random flips and rotations of the X and Y\n",
    "       data for a total of 4 x augmentation\n",
    "    \n",
    "         Parameters:\n",
    "          batch_ids (list):\n",
    "          batch_size (int):\n",
    "          \n",
    "         Returns:\n",
    "          x_batch (arr):\n",
    "          y_batch (arr):\n",
    "    '''\n",
    "    x = np.copy(train_x[batch_ids])\n",
    "    samples_to_median = np.random.randint(0, 12, size=(batch_size, 12)) #[32, 6]\n",
    "    samples_to_select = np.zeros((batch_size, 4))\n",
    "    samples_to_select[:, 0] = np.random.randint(0, 4, size=(batch_size))\n",
    "    samples_to_select[:, 1] = np.random.randint(3, 7, size=(batch_size))\n",
    "    samples_to_select[:, 2] = np.random.randint(6, 10, size=(batch_size))\n",
    "    samples_to_select[:, 3] = np.random.randint(8, 12, size=(batch_size))\n",
    "    #samples_to_select = np.sort(samples_to_select, axis = 1)\n",
    "    samples_to_select = samples_to_select.astype(np.int)\n",
    "    #samples_to_median = np.sort(samples_to_median, axis = 1)\n",
    "    n_samples = np.random.randint(2, 5, size=(batch_size)) \n",
    "    \n",
    "    x_batch = np.zeros((x.shape[0], LEN + 1, 28, 28, 17))\n",
    "    for samp in range(batch_size):\n",
    "        samps = samples_to_median[samp, :]#:np.random.randint(6, 12)]\n",
    "        lower_samp = np.min(samps)\n",
    "        upper_samp = np.max(samps)\n",
    "        #print(np.unique(samps))\n",
    "        x_samp = x[samp]\n",
    "        samps = np.unique(samps)\n",
    "        med_samp = np.median(x_samp[samps], axis = 0)\n",
    "        #med_samp = np.median(x[samp, np.unique(samps)], axis = 0)\n",
    "        \n",
    "        \n",
    "       \n",
    "        if x_batch.shape[1] == 5:\n",
    "            #print(samples_to_select[samp])\n",
    "            x_batch[samp, :-1, ...] = x[samp, samples_to_select[samp]]\n",
    "        else:\n",
    "            x[samp, :lower_samp] = x[samp, lower_samp]\n",
    "            x[samp, upper_samp:] = x[samp, upper_samp]\n",
    "            x_batch[samp, :-1] = x[samp]\n",
    "        x_batch[samp, -1, ...] = med_samp\n",
    "        \n",
    "    x = x_batch\n",
    "    \n",
    "    #xmed = x[:, -1]\n",
    "\n",
    "    #x = np.concatenate([x, xmed[:, np.newaxis]], axis = 1)\n",
    "        \n",
    "    y = train_y[batch_ids]\n",
    "    chmy = train_chm[batch_ids]\n",
    "    \n",
    "    y_batch = np.zeros_like(y)\n",
    "    chmy_batch = np.zeros_like(y)\n",
    "    \n",
    "    flips = np.random.choice(np.array([0, 1, 2, 3]), batch_size, replace = True)\n",
    "    for i in range(x.shape[0]):\n",
    "        current_flip = flips[i]\n",
    "        if current_flip == 0:\n",
    "            x_batch[i] = x[i]\n",
    "            y_batch[i] = y[i]\n",
    "            chmy_batch[i] = chmy[i]\n",
    "            \n",
    "        if current_flip == 1:\n",
    "            x_batch[i] = np.flip(x[i], 1)\n",
    "            y_batch[i] = np.flip(y[i], 0)\n",
    "            chmy_batch[i] = np.flip(chmy[i], 0)\n",
    "        if current_flip == 2:\n",
    "            x_batch[i] = np.flip(x[i], [2, 1])\n",
    "            y_batch[i] = np.flip(y[i], [1, 0])\n",
    "            chmy_batch[i] = np.flip(chmy[i], [1, 0])\n",
    "        if current_flip == 3:\n",
    "            x_batch[i] = np.flip(x[i], 2)\n",
    "            y_batch[i] = np.flip(y[i], 1)\n",
    "            chmy_batch[i] = np.flip(chmy[i], 1)\n",
    "\n",
    "    y_batch = y_batch.reshape((batch_size, 14, 14))\n",
    "    chmy_batch = chmy_batch.reshape((batch_size, 14, 14))\n",
    "    #x_batch, y_batch = cut_mix(x_batch, y_batch, batch_ids, 0.5, batch_size)\n",
    "    #x_batch = np.delete(x_batch, [11, 12], axis = -1)\n",
    "    return x_batch, y_batch, chmy_batch\n",
    "\n",
    "x_batch_test, y_batch_test, chmy_batch_test = augment_batch([x for x in range(32)], 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 75\n",
    "end = 125\n",
    "f, ((c1r1, c1r2), (c2r1, c2r2)) = plt.subplots(2, 2, sharey=False)\n",
    "f.set_size_inches(15, 12)\n",
    "\n",
    "c1r1.set_title(f\"Train loss - {model_path}\")\n",
    "l1 = sns.scatterplot(y = metrics[0, start:end], x = np.arange(start, end), ax = c1r1)\n",
    "l1.set(ylim=(0.30, .40))\n",
    "\n",
    "c1r2.set_title(\"F1 score\")\n",
    "f =sns.scatterplot(y = metrics[5, start:end], x = np.arange(start, end), ax = c1r2)\n",
    "f.set(ylim=(0.84, .91))\n",
    "\n",
    "c2r1.set_title(\"Test loss\")\n",
    "l = sns.scatterplot(y = metrics[1, start:end], x = np.arange(start, end), ax = c2r1)\n",
    "l.set(ylim=(0.140, .165)) \n",
    "\n",
    "c2r2.set_title(\"Absolute % error\")\n",
    "e = sns.scatterplot(y = metrics[2, start:end] / 2, x = np.arange(start, end), ax = c2r2)\n",
    "e.set(ylim=(2.2, 3.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 100, alpha: 0.33, beta: 0.0, drop: 0.9 Learning rate: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f08a32b01404321a8a7c13e44aaa933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:48: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss 11.642999649047852\n",
      "starting epoch 101, alpha: 0.33, beta: 0.0, drop: 0.9 Learning rate: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f389350772664bd296b2a64f9a982439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101: Loss 10.923999786376953\n",
      "starting epoch 102, alpha: 0.33, beta: 0.0, drop: 0.9 Learning rate: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d716bdadd4a43aa9a9da5193c62846f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102: Loss 10.972999572753906\n",
      "starting epoch 103, alpha: 0.33, beta: 0.0, drop: 0.9 Learning rate: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820e7ff70cf1470b91bc4d142e9d47af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103: Loss 10.418999671936035\n",
      "starting epoch 104, alpha: 0.33, beta: 0.0, drop: 0.9 Learning rate: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4b054e16d8434db97c87a6548fa601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104: Loss 10.482000350952148\n",
      "starting epoch 105, alpha: 0.33, beta: 0.0, drop: 0.9 Learning rate: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eb54da520a4736902a2c631c843d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105: Loss 10.152000427246094\n",
      "starting epoch 106, alpha: 0.33, beta: 0.0, drop: 0.9 Learning rate: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c9fbaedf894b888eba67f82f043e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106: Loss 9.815999984741211\n",
      "starting epoch 107, alpha: 0.33, beta: 0.0, drop: 0.9 Learning rate: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3513b920314627994ac7bda2641f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d8a985142e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m                                      \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                      \u001b[0mbeta_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                      \u001b[0mft_lr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mft_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                                      })\n\u001b[1;32m     53\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "best_val = 0.72\n",
    "fine_tune = False\n",
    "ft_epochs = 0\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "\n",
    "for i in range(100, 200):\n",
    "    if i >= 200:\n",
    "        SWA = True# set to true to start SWA\n",
    "    else:\n",
    "        SWA = False\n",
    "    al = FINAL_ALPHA\n",
    "    ft_learning_rate = .1\n",
    "    be = 0.0\n",
    "    test_al = al\n",
    "    op = train_op# if fine_tune else train_op\n",
    "        \n",
    "    train_ids = [x for x in range(len(train_y))]\n",
    "    np.random.shuffle(train_ids)\n",
    "    randomize = train_ids\n",
    "\n",
    "    print(f\"starting epoch {i}, \" \n",
    "          f\"alpha: {al}, beta: {be}, \"\n",
    "          f\"drop: {np.max(((1. - (i * 0.005)), 0.9))} \"\n",
    "          f\"Learning rate: {ft_learning_rate}\"\n",
    "         )\n",
    "    \n",
    "    loss = train_loss\n",
    "    losses = []\n",
    "    \n",
    "    for k in tqdm.notebook.tnrange(int(len(randomize) // BATCH_SIZE)):\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        x_batch, y_batch, y_batch_height = augment_batch(batch_ids, BATCH_SIZE)\n",
    "        #print(np.sum(np.sum(y_batch, axis = (1, 2)) <= 2))\n",
    "        gedi_batch = np.array(data.gedi_rh95.iloc[batch_ids])[:, np.newaxis]\n",
    "        gedi_batch[gedi_batch < 2.5] = 0.\n",
    "        opt, tr = sess.run([op, loss],\n",
    "                          feed_dict={inp: x_batch,\n",
    "                                     length: np.full((BATCH_SIZE,), 4),\n",
    "                                     labels: y_batch,\n",
    "                                     labels_height: y_batch_height,\n",
    "                                     labels_gedi: gedi_batch,\n",
    "                                     is_training: True,\n",
    "                                     loss_weight: 1.0,\n",
    "                                     keep_rate: 0.999,#np.max(((1. - (i * 0.01)), MAX_DROPBLOCK)),\n",
    "                                     alpha: al,\n",
    "                                     beta_: be,\n",
    "                                     ft_lr: ft_learning_rate,\n",
    "                                     })\n",
    "        losses.append(tr)\n",
    "    \n",
    "    print(f\"Epoch {i}: Loss {np.around(np.mean(losses[:-1]), 3)}\")\n",
    "    if SWA:\n",
    "        sess.run(swa_op)\n",
    "        sess.run(save_weight_backups)\n",
    "        sess.run(swa_to_weights)\n",
    "        \n",
    "    #metrics[0, i] = np.mean(losses[:-1])\n",
    "    #val_loss, f1, error = calculate_metrics('all', al = test_al, canopy_thresh = 75)\n",
    "    #metrics[1, i] = val_loss\n",
    "    #metrics[2, i] = error\n",
    "    #metrics[5, i] = f1\n",
    "    \n",
    "    #if f1 < (best_val - 0.002):\n",
    "    #    ft_epochs += 1\n",
    "        \n",
    "    #if f1 > (best_val - 0.02):\n",
    "    #    print(f\"Saving model with {f1}\")\n",
    "        #np.save(f\"{model_path}metrics.npy\", metrics)\n",
    "        #os.mkdir(f\"{model_path}{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}/\")\n",
    "        #save_path = saver.save(sess, f\"{model_path}/{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}/model\")\n",
    "     #   if f1 > best_val:\n",
    "     #       best_val = f1\n",
    "    if SWA:\n",
    "        sess.run(restore_weight_backups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(swa_to_weights)\n",
    "saver = tf.train.Saver(max_to_keep = 150)\n",
    "#os.mkdir(f\"../models/plantation2/\")\n",
    "save_path = saver.save(sess, f\"../models/plantation/model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test prediction visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start =0\n",
    "#test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "def multiplot(matrices, nrows = 2, ncols = 4):\n",
    "    '''Docstring\n",
    "    \n",
    "         Parameters:\n",
    "          matrices (list):\n",
    "          nrows (int):\n",
    "          \n",
    "         Returns:\n",
    "          None\n",
    "    '''\n",
    "    fig, axs = plt.subplots(ncols=4, nrows = nrows)\n",
    "    fig.set_size_inches(18, 4*nrows)\n",
    "    to_iter = [[x for x in range(i, i + ncols + 1)] for i in range(0, nrows*ncols, ncols)]\n",
    "    for r in range(1, nrows + 1):\n",
    "        min_i = min(to_iter[r-1])\n",
    "        max_i = max(to_iter[r-1])\n",
    "        for i, matrix in enumerate(matrices[min_i:max_i]):\n",
    "            sns.heatmap(data = matrix, ax = axs[r - 1, i], cbar = False)\n",
    "            axs[r - 1, i].set_xlabel(\"\")\n",
    "            axs[r - 1, i].set_ylabel(\"\")\n",
    "            axs[r - 1, i].set_yticks([])\n",
    "            axs[r - 1, i].set_xticks([])\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [x for x in range(test_x.shape[0])]\n",
    "matrix_ids = np.arange(start, start + 8, 1)\n",
    "matrix_ids = test_ids[matrix_ids]\n",
    "preds, trues = [], []\n",
    "\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    x_input = test_x[idx].reshape(1, 5, 28, 28, n_bands)\n",
    "    median_input = calc_median_input(x_input)\n",
    "    y = sess.run([fm], feed_dict={inp: x_input,\n",
    "                                  length: np.full((1,), 12),\n",
    "                                  is_training: False,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(14, 14)\n",
    "    print(i, (list(test_data.iloc[idx, 1])[0], list(test_data.iloc[idx, 2])[0]), diffs[i[0]])\n",
    "    preds.append(y)\n",
    "    y2 = np.copy(y)\n",
    "    true = tes\n",
    "    t_y[idx].reshape(14, 14)\n",
    "    trues.append(true)\n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4] + trues[4:] + preds[4:]\n",
    "multiplot(to_plot, nrows = 4, ncols = 4)\n",
    "\n",
    "start = start + 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train prediction visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 1250\n",
    "x_batch_test2, y_batch_test2, chm_batch_test2 = augment_batch([x for x in range(offset, offset + 100)], 100)\n",
    "train_ids = np.arange(100)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array([x for x in range(train_x.shape[0])])\n",
    "\n",
    "matrix_ids = np.arange(start, start + 8, 1)\n",
    "matrix_ids = train_ids[matrix_ids]\n",
    "preds, trues = [], []\n",
    "\n",
    "print(matrix_ids)\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    x_input = x_batch_test2[idx].reshape(1, 5, 28, 28, n_bands)\n",
    "    y = sess.run([height], feed_dict={inp: x_input,\n",
    "                                  length: np.full((1,), 4),\n",
    "                                  is_training: False,\n",
    "                                    })\n",
    "    y = np.array(y).reshape(14, 14)  \n",
    "    preds.append(y)\n",
    "    true = chm_batch_test2[idx].reshape(14, 14)\n",
    "    trues.append(true)\n",
    "    print(np.percentile(y[5:-5, 5:-5], 95), data.iloc[offset + i].gedi_rh95 , data.iloc[offset + i].lat, data.iloc[offset + i].lon)\n",
    "    \n",
    "start += 8\n",
    "to_plot = trues[0:4] + preds[0:4] + trues[4:] + preds[4:]\n",
    "multiplot(to_plot, nrows = 4, ncols = 4)\n",
    "# 2006, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.mean(train_chm, axis = (1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch_test2[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(data.fn) > 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(data.fn.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
