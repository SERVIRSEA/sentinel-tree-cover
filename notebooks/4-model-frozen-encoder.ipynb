{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree segmentation with multitemporal Sentinel 1/2 imagery\n",
    "\n",
    "## John Brandt\n",
    "## December 2023\n",
    "\n",
    "## This notebook finetunes the TTC decoder for a new task\n",
    "\n",
    "## Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_5685/3754813325.py:4: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2023-12-19 09:30:13.067345: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-19 09:30:13.067818: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_5685/3754813325.py:16: The name tf.initializers.lecun_normal is deprecated. Please use tf.compat.v1.initializers.lecun_normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jbrandt.terminal/Documents/GitHub/sentinel-tree-cover/src/layers/zoneout.py:8: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ../src/layers/zoneout.py\n",
    "%run ../src/layers/losses.py\n",
    "%run ../src/layers/adabound.py\n",
    "%run ../src/layers/convgru.py\n",
    "%run ../src/layers/dropblock.py\n",
    "%run ../src/layers/extra_layers.py\n",
    "%run ../src/layers/stochastic_weight_averaging.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/preprocessing/slope.py\n",
    "#%run ../src/utils/metrics.py\n",
    "#%run ../src/utils/lovasz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.90\n",
    "ACTIVATION_FUNCTION = 'swish'\n",
    "\n",
    "INITIAL_LR = 1e-3\n",
    "DROPBLOCK_MAXSIZE = 5\n",
    "\n",
    "N_CONV_BLOCKS = 1\n",
    "FINAL_ALPHA = 0.33\n",
    "LABEL_SMOOTHING = 0.03\n",
    "\n",
    "L2_REG = 0.\n",
    "BATCH_SIZE = 32\n",
    "MAX_DROPBLOCK = 0.6\n",
    "\n",
    "FRESH_START = True\n",
    "best_val = 0.2\n",
    "\n",
    "START_EPOCH = 1\n",
    "END_EPOCH = 100\n",
    "\n",
    "n_bands = 17\n",
    "initial_flt = 32\n",
    "mid_flt = 32 * 2\n",
    "high_flt = 32 * 2 * 2\n",
    "\n",
    "temporal_model = True\n",
    "input_size = 28\n",
    "output_size = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layer definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv GRU Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_block(inp, length, size, flt, scope, train, normalize = True):\n",
    "    '''Bidirectional convolutional GRU block with \n",
    "       zoneout and CSSE blocks in each time step\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): (B, T, H, W, C) layer\n",
    "          length (tf.Variable): (B, T) layer denoting number of\n",
    "                                steps per sample\n",
    "          size (int): kernel size of convolution\n",
    "          flt (int): number of convolution filters\n",
    "          scope (str): tensorflow variable scope\n",
    "          train (tf.Bool): flag to differentiate between train/test ops\n",
    "          normalize (bool): whether to compute layer normalization\n",
    "\n",
    "         Returns:\n",
    "          gru (tf.Variable): (B, H, W, flt*2) bi-gru output\n",
    "          steps (tf.Variable): (B, T, H, W, flt*2) output of each step\n",
    "    '''\n",
    "    with tf.variable_scope(scope):\n",
    "        print(f\"GRU input shape {inp.shape}, zoneout: {ZONE_OUT_PROB}\")\n",
    "        \n",
    "        # normalize is internal group normalization within the reset gate\n",
    "        # sse is internal SSE block within the state cell\n",
    "\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', \n",
    "                           normalize = normalize, sse = True)\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID',\n",
    "                           normalize = normalize, sse = True)\n",
    "        \n",
    "        cell_fw = ZoneoutWrapper(\n",
    "           cell_fw, zoneout_drop_prob = 0.75, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = 0.75, is_training = train)\n",
    "        steps, out = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        gru = tf.concat(out, axis = -1)\n",
    "        steps = tf.concat(steps, axis = -1)\n",
    "        print(f\"GRU block output shape {gru.shape}\")\n",
    "    return gru, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_5685/2359912968.py:8: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_5685/2359912968.py:9: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg = tf.contrib.layers.l2_regularizer(0.)\n",
    "temporal_model = True\n",
    "input_size = 124\n",
    "n_bands = 17\n",
    "output_size = input_size - 14\n",
    "\n",
    "if temporal_model:\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, 5, input_size, input_size, n_bands))\n",
    "    length = tf.placeholder_with_default(np.full((1,), 4), shape = (None,))\n",
    "else:\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, input_size, input_size, n_bands))\n",
    "    \n",
    "labels = tf.placeholder(tf.float32, shape=(None, output_size, output_size))#, 1))\n",
    "mask = tf.placeholder(tf.float32, shape = (None, output_size, output_size))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training') # For DropBlock\n",
    "alpha = tf.placeholder(tf.float32, shape = ()) # For loss scheduling\n",
    "ft_lr = tf.placeholder_with_default(0.001, shape = ()) # For loss scheduling\n",
    "loss_weight = tf.placeholder_with_default(1.0, shape = ())\n",
    "beta_ = tf.placeholder_with_default(0.0, shape = ()) # For loss scheduling, not currently implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_5685/2476193274.py:19: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "GRU input shape (?, 4, 124, 124, 17), zoneout: 0.9\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "(3, 3, 49, 64)\n",
      "(3, 3, 49, 64)\n",
      "GRU block output shape (?, 124, 124, 64)\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/Documents/GitHub/sentinel-tree-cover/src/layers/dropblock.py:154: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "conv_median 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv_median_conv/conv_median/x/mul:0\", shape=(?, 124, 124, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32:0\", shape=(?, 124, 124, 64), dtype=float32)\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/Documents/GitHub/sentinel-tree-cover/src/layers/extra_layers.py:309: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.\n",
      "\n",
      "Median conv: (?, 124, 124, 64)\n",
      "conv_concat 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv_concat_conv/conv_concat/x/mul:0\", shape=(?, 124, 124, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_1:0\", shape=(?, 124, 124, 64), dtype=float32)\n",
      "Concat: (?, 124, 124, 64)\n",
      "conv1 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv1_conv/conv1/ws_conv2d_2/Conv2D:0\", shape=(?, 60, 60, 128), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_2:0\", shape=(?, 60, 60, 128), dtype=float32)\n",
      "Conv1: (?, 60, 60, 128)\n",
      "conv2 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv2_conv/conv2/ws_conv2d_3/Conv2D:0\", shape=(?, 28, 28, 256), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_3:0\", shape=(?, 28, 28, 256), dtype=float32)\n",
      "Encoded (?, 28, 28, 256)\n",
      "up2 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"up2_conv/up2/x/mul:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_4:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "(?, 56, 56, 128)\n",
      "up2_out 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"up2_out_conv/up2_out/x/mul:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_5:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "up3 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"up3_conv/up3/x/mul:0\", shape=(?, 112, 112, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_6:0\", shape=(?, 112, 112, 64), dtype=float32)\n",
      "out 3 Conv 2D Group Norm RELU CSSE NoBias NoDrop\n",
      "The non normalized feats are Tensor(\"out_conv/out/ws_conv2d_7/Conv2D:0\", shape=(?, 110, 110, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_7:0\", shape=(?, 110, 110, 64), dtype=float32)\n",
      "The output is (?, 110, 110, 64), with a receptive field of 1\n",
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_5685/2278055326.py:82: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "The output, sigmoid is (?, 110, 110, 1), with a receptive field of 1\n"
     ]
    }
   ],
   "source": [
    "initial_flt = 64\n",
    "mid_flt = initial_flt * 2\n",
    "high_flt = 64 * 2 * 2\n",
    "INPUT_SIZE =124\n",
    "SIZE_X = 124\n",
    "\n",
    "gru_input = inp[:, :-1, ...]\n",
    "gru, steps = gru_block(inp = gru_input, length = length,\n",
    "                            size = [INPUT_SIZE, SIZE_X, ],\n",
    "                            flt = initial_flt // 2,\n",
    "                            scope = 'down_16',\n",
    "                            train = is_training)\n",
    "with tf.variable_scope(\"gru_drop\"):\n",
    "    drop_block = DropBlock2D(keep_prob=keep_rate, block_size=4)\n",
    "    gru = drop_block(gru, is_training)\n",
    "    \n",
    "# Median conv\n",
    "median_input = inp[:, -1, ...]\n",
    "median_conv = conv_swish_gn(inp = median_input, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_median', filters = initial_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None)\n",
    "print(f\"Median conv: {median_conv.shape}\")\n",
    "\n",
    "concat1 = tf.concat([gru, median_conv], axis = -1)\n",
    "concat = conv_swish_gn(inp = concat1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_concat', filters = initial_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, padding = \"SAME\")\n",
    "print(f\"Concat: {concat.shape}\")\n",
    "\n",
    "# MaxPool-conv-swish-GroupNorm-csse\n",
    "pool1 = MaxPool2D()(concat)\n",
    "conv1 = conv_swish_gn(inp = pool1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv1', filters = mid_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True, padding = \"VALID\",\n",
    "            csse = True, dropblock = True, weight_decay = None)\n",
    "print(f\"Conv1: {conv1.shape}\")\n",
    "\n",
    "# MaxPool-conv-swish-csse-DropBlock\n",
    "pool2 = MaxPool2D()(conv1)\n",
    "conv2 = conv_swish_gn(inp = pool2, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv2', filters = high_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, block_size = 4, padding = \"VALID\")\n",
    "print(\"Encoded\", conv2.shape)\n",
    "\n",
    "# Decoder 4 - 8, upsample-conv-swish-csse-concat-conv-swish\n",
    "up2 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(conv2)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2', filters = mid_flt, \n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "conv1_crop = Cropping2D(2)(conv1)\n",
    "print(conv1_crop.shape)\n",
    "up2 = tf.concat([up2, conv1_crop], -1)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2_out', filters = mid_flt, \n",
    "                    keep_rate =  keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "\n",
    "# Decoder 8 - 14 upsample-conv-swish-csse-concat-conv-swish\n",
    "up3 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(up2)\n",
    "#up3 = ReflectionPadding2D((1, 1,))(up3)\n",
    "up3 = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up3', filters = initial_flt, \n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "gru_crop = Cropping2D(6)(concat)\n",
    "\n",
    "up3 = tf.concat([up3, gru_crop], -1)\n",
    "up3out = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'out', filters = initial_flt, \n",
    "                    keep_rate  = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = False, weight_decay = None, padding = \"VALID\")\n",
    "\n",
    "init = tf.constant_initializer([-np.log(0.7/0.3)]) # For focal loss\n",
    "print(f\"The output is {up3out.shape}, with a receptive field of {1}\")\n",
    "\n",
    "fm = tf.layers.Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1),\n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init, name = 'conv2d')(up3out)#,\n",
    "\n",
    "print(f\"The output, sigmoid is {fm.shape}, with a receptive field of {1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_5\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up3\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up3_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up3\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_out_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up2_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up2\")# + \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 110, 110, 1)\n",
      "(?, 1)\n",
      "<unknown>\n",
      "(?, 110, 110)\n",
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_5685/1596829350.py:26: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_5685/1596829350.py:27: The name tf.losses.get_regularization_losses is deprecated. Please use tf.compat.v1.losses.get_regularization_losses instead.\n",
      "\n",
      "(?, 110, 110, 1)\n",
      "(?, 1)\n",
      "<unknown>\n",
      "(?, 110, 110)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 09:30:18.124326: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_5685/1596829350.py:60: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def grad_norm(gradients):\n",
    "    norm = tf.compat.v1.norm(\n",
    "        tf.stack([\n",
    "            tf.compat.v1.norm(grad) for grad in gradients if grad is not None\n",
    "        ])\n",
    "    )\n",
    "    return norm\n",
    "\n",
    "FRESH_START = True\n",
    "#print(f\"Starting model with: \\n {ZONE_OUT_PROB} zone out \\n {L2_REG} l2 \\n\"\n",
    " #     f\"{INITIAL_LR} initial LR \\n {total_parameters} parameters\")  \n",
    "\n",
    "OUT = 110\n",
    "if FRESH_START:\n",
    "    # We use the Adabound optimizer\n",
    "    optimizer = AdaBoundOptimizer(5e-5, 5e-3)\n",
    "    #train_loss1 = logcosh(tf.reshape(labels, (-1, 14, 14, 1)), output) \n",
    "    \n",
    "    train_loss2 = bce_surface_loss(tf.reshape(labels, (-1, OUT, OUT, 1)), fm,\n",
    "                                  weight = loss_weight, \n",
    "                             alpha = alpha, beta = beta_, mask = mask)\n",
    "\n",
    "    train_loss = train_loss2# + train_loss2\n",
    "    \n",
    "    # If there is any L2 regularization, add it. Current model does not use\n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    if len(tf.losses.get_regularization_losses()) > 0:\n",
    "        train_loss = train_loss + l2_loss\n",
    "        \n",
    "    test_loss = bce_surface_loss(tf.reshape(labels, (-1, OUT, OUT, 1)),\n",
    "                            fm, weight = loss_weight, \n",
    "                            alpha = alpha, beta = beta_, mask = mask)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss)#, var_list = finetune_vars)   \n",
    "        #ft_op = ft_optimizer.minimize(train_loss)\n",
    "    \n",
    "    # The following code blocks are for sharpness aware minimization\n",
    "    # Adapted from https://github.com/sayakpaul/Sharpness-Aware-Minimization-TensorFlow\n",
    "    # For tensorflow 1.15\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "    gradient_norm = grad_norm(gradients)\n",
    "    scale = 0.05 / (gradient_norm + 1e-12)\n",
    "    e_ws = []\n",
    "    for (grad, param) in gradients:\n",
    "        e_w = grad * scale\n",
    "        param.assign_add(e_w)\n",
    "        e_ws.append(e_w)\n",
    "\n",
    "    sam_gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "    for (param, e_w) in zip(trainable_params, e_ws):\n",
    "        param.assign_sub(e_w)\n",
    "    train_step = optimizer.apply_gradients(sam_gradients)\n",
    "    \n",
    "    # Create a saver to save the model each epoch\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 150)#, var_list = all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_saver_varlist(path):\n",
    "\n",
    "    current_items = []\n",
    "    vars_dict = {}\n",
    "    for var_current in tf.global_variables():\n",
    "        current_items.append(var_current) \n",
    "    names = [x.op.name for x in current_items]\n",
    "    names = np.argsort(names)\n",
    "    current_items = [current_items[x] for x in names]\n",
    "    \n",
    "    ckpt_items = []\n",
    "    for var_ckpt in tf.train.list_variables(path):\n",
    "        if 'BackupVariables' not in var_ckpt[0]:\n",
    "            if 'StochasticWeightAveraging' not in var_ckpt[0]:\n",
    "                if 'global_step' not in var_ckpt[0]:\n",
    "                    if 'is_training' not in var_ckpt[0]:\n",
    "                        if 'n_models' not in var_ckpt[0]:\n",
    "                            ckpt_items.append(var_ckpt[0])\n",
    "    \n",
    "    ckptdict = {}\n",
    "    for y, x in zip(ckpt_items, current_items):\n",
    "        ckptdict[y] = x\n",
    "    return ckptdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckptdict = make_saver_varlist('../models/loss-avg-tf2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting anew\n",
      "INFO:tensorflow:Restoring parameters from ../models/loss-avg/-0\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(ckptdict)\n",
    "model_path  = \"../models/loss-avg/\"\n",
    "FRESH_START = False\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")\n",
    "else:\n",
    "    print(\"Starting anew\")\n",
    "    metrics = np.zeros((6, 300))\n",
    "\n",
    "if not FRESH_START:\n",
    "    path = model_path\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/loss-avg-tf2/model'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, '../models/loss-avg-tf2/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting anew\n",
      "INFO:tensorflow:Restoring parameters from ../models/epoch31/model\n",
      "INFO:tensorflow:Froze 65 variables.\n",
      "INFO:tensorflow:Converted 65 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "FRESH_START = False\n",
    "model_path  = \"../models/loss-avg/\"\n",
    "all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "all_vars = [x for x in all_vars if 'Momentum' not in x.name]\n",
    "all_vars = [x for x in all_vars if 'Backup' not in x.name]\n",
    "all_vars = [x for x in all_vars if 'StochasticWeight' not in x.name]\n",
    "all_vars = [x for x in all_vars if 'is_training' not in x.name]\n",
    "all_vars = [x for x in all_vars if 'n_models' not in x.name]\n",
    "\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep = 150, var_list = all_vars)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")\n",
    "else:\n",
    "    print(\"Starting anew\")\n",
    "    metrics = np.zeros((6, 300))\n",
    "\n",
    "if not FRESH_START:\n",
    "    path = model_path\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(path))\n",
    "\n",
    "output_node_names = ['conv2d_5/Sigmoid']\n",
    "frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess,\n",
    "    sess.graph_def,\n",
    "    output_node_names)\n",
    "\n",
    "#Save the frozen graph\n",
    "#with open('../models/epoch31/predict_graph.pb', 'wb') as f:\n",
    "#    f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nft_optimizer = tf.train.MomentumOptimizer(ft_lr, momentum = 0.8, use_nesterov = True)\\ntrain_loss = bce_surface_loss(tf.reshape(labels, (-1, 14, 14, 1)), \\n                             fm, weight = loss_weight, \\n                             alpha = alpha, beta = beta_)\\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\\n    \\nwith tf.control_dependencies(update_ops):\\n    #train_op = optimizer.minimize(train_loss)   \\n    ft_op = ft_optimizer.minimize(train_loss, var_list = finetune_vars)\\n\\ntrainable_params = tf.trainable_variables()\\ngradients = ft_optimizer.compute_gradients(loss=train_loss, var_list=None)\\ngradient_norm = grad_norm(gradients)\\nscale = 0.05 / (gradient_norm + 1e-12)\\ne_ws = []\\nfor (grad, param) in gradients:\\n    e_w = grad * scale\\n    param.assign_add(e_w)\\n    e_ws.append(e_w)\\n\\nsam_gradients = ft_optimizer.compute_gradients(loss=train_loss, var_list=None)\\nfor (param, e_w) in zip(trainable_params, e_ws):\\n    param.assign_sub(e_w)\\ntrain_step_ft = ft_optimizer.apply_gradients(sam_gradients)\\n\\ninitialize_uninitialized(sess)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "\n",
    "\"\"\"\n",
    "ft_optimizer = tf.train.MomentumOptimizer(ft_lr, momentum = 0.8, use_nesterov = True)\n",
    "train_loss = bce_surface_loss(tf.reshape(labels, (-1, 14, 14, 1)), \n",
    "                             fm, weight = loss_weight, \n",
    "                             alpha = alpha, beta = beta_)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "with tf.control_dependencies(update_ops):\n",
    "    #train_op = optimizer.minimize(train_loss)   \n",
    "    ft_op = ft_optimizer.minimize(train_loss, var_list = finetune_vars)\n",
    "\n",
    "trainable_params = tf.trainable_variables()\n",
    "gradients = ft_optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "gradient_norm = grad_norm(gradients)\n",
    "scale = 0.05 / (gradient_norm + 1e-12)\n",
    "e_ws = []\n",
    "for (grad, param) in gradients:\n",
    "    e_w = grad * scale\n",
    "    param.assign_add(e_w)\n",
    "    e_ws.append(e_w)\n",
    "\n",
    "sam_gradients = ft_optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "for (param, e_w) in zip(trainable_params, e_ws):\n",
    "    param.assign_sub(e_w)\n",
    "train_step_ft = ft_optimizer.apply_gradients(sam_gradients)\n",
    "\n",
    "initialize_uninitialized(sess)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of stochastic weight averaging\n",
    "\n",
    "   \n",
    "model_vars = tf.trainable_variables()\n",
    "swa = StochasticWeightAveraging()\n",
    "swa_op = swa.apply(var_list=model_vars)\n",
    "with tf.variable_scope('BackupVariables'):\n",
    "    # force tensorflow to keep theese new variables on the CPU ! \n",
    "    backup_vars = [tf.get_variable(var.op.name, dtype=var.value().dtype, trainable=False,\n",
    "                                   initializer=var.initialized_value())\n",
    "                   for var in model_vars]\n",
    "\n",
    "# operation to assign SWA weights to model\n",
    "swa_to_weights = tf.group(*(tf.assign(var, swa.average(var).read_value()) for var in model_vars))\n",
    "# operation to store model into backup variables\n",
    "save_weight_backups = tf.group(*(tf.assign(bck, var.read_value()) for var, bck in zip(model_vars, backup_vars)))\n",
    "# operation to get back values from backup variables to model\n",
    "restore_weight_backups = tf.group(*(tf.assign(var, bck.read_value()) for var, bck in zip(model_vars, backup_vars)))\n",
    "\n",
    "initialize_uninitialized(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "*  Load in CSV data from Collect Earth\n",
    "*  Reconstruct the X, Y grid for the Y data per sample\n",
    "*  Calculate remote sensing indices\n",
    "*  Stack X, Y, length data\n",
    "*  Apply median filter to DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-459e26595f6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mbatchx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'development/{batch}_x.hkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mbatchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'development/{batch}_y.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/hickle/hickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file_obj, path, safe)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;31m# Load file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mpy_container\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyContainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0mpy_container\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_container\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_root_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/hickle/hickle.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(py_container, h_group)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# must be a dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0msubdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0mpy_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/hickle/hickle.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(h_node)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0mload_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;31m# If data is not py_type yet, convert to it (unless it is pickle)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/hickle/loaders/load_numpy.py\u001b[0m in \u001b[0;36mload_ndarray_dataset\u001b[0;34m(h_node)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_ndarray_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_type_and_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'np_dtype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/hickle/helpers.py\u001b[0m in \u001b[0;36mget_type_and_data\u001b[0;34m(h_node)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;34m\"\"\" Helper function to return the py_type and data block for an HDF node\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mpy_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpy_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mfspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m# Patch up the output for NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import hickle as hkl\n",
    "\n",
    "train_x = np.zeros((12+12+8+8+8+8+5 + 4 + 7 + 6 + 6 + 6 + 4 + 5 + 6 + 6 + 6, 12, 172, 172, 17), dtype = np.float32)\n",
    "train_y = np.zeros((12+12+8+8+8+8+5 + 4 + 7 + 6 + 6 + 6 + 4 + 5 + 6 + 6 + 6, 158, 158), dtype = np.float32)\n",
    "batches = [ '369X1279Y', '445X1241Y', '1638X1088Y',\n",
    "           '448X1239Y', '2283X714Y', '2288X763Y', \n",
    "           '1637X1088Y', '449X1239Y', '371X1277Y',\n",
    "          '412X1198Y', '465X1246Y', '1668X1076Y', \n",
    "           '359X1281Y', '2382X944Y', '1642X1140Y', '2309X679Y', '1669X1125Y']#'1650X1092Y (4)'\n",
    "start = 0\n",
    "for batch in batches:\n",
    "    batchx = hkl.load(f'development/{batch}_x.hkl')\n",
    "    batchy = np.load(f'development/{batch}_y.npy')\n",
    "    \n",
    "    _length = batchx.shape[0]\n",
    "    print(_length)\n",
    "    if batchy.shape[-1] == 14:\n",
    "        batchy = np.ones((_length, 158, 158))\n",
    "    train_x[start:start+_length] = batchx\n",
    "    train_y[start:start+_length] = batchy\n",
    "    start += _length\n",
    "    \n",
    "med = np.median(train_x, axis = 1)\n",
    "med = med[:, np.newaxis, :, :, :]\n",
    "train_x = np.concatenate([train_x, med], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_all = [0.006576638437476157, 0.0162050812542916, 0.010040436408026246, \n",
    "           0.013351644159609368, 0.01965362020294499, 0.014229037918669413, \n",
    "           0.015289539940489814, 0.011993591210803388, 0.008239871824216068, \n",
    "           0.006546120393682765, 0.0, 0.0, 0.0, -0.1409399364817101, \n",
    "           -0.4973397113668104, -0.09731556326714398, -0.7193834232943873]\n",
    "\n",
    "max_all = [0.2691233691920348, 0.3740291447318227, 0.5171435111009385, 0.6027466239414053,\n",
    "           0.5650263218127718, 0.5747005416952773, 0.5933928435187305, 0.6034943160143434, \n",
    "           0.7472037842374304, 0.4, 0.509269855802243, 0.948334642387533, \n",
    "           0.6729257769285485, 0.8177635298774327, 0.35768999002433816, 0.7545951919107605, \n",
    "           0.7602693339366691]\n",
    "\n",
    "# Min all, and max all are the 0.1 and 99.9 percentiles of each band\n",
    "for band in tnrange(0, train_x.shape[-1]):\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    train_x[..., band] = np.clip(train_x[..., band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (train_x[..., band] - midrange) / (rng / 2)\n",
    "    train_x[..., band] = standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score_at_tolerance(true, pred, tolerance = 1):\n",
    "    \"\"\"Because of coregistration errors, we evaluate the model\n",
    "    where false positives/negatives must be >1px away from a true positive\n",
    "    \"\"\"\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    tp = np.zeros_like(true)\n",
    "    fp = np.zeros_like(true)\n",
    "    fn = np.zeros_like(true)\n",
    "    \n",
    "    for x in range(true.shape[0]):\n",
    "        for y in range(true.shape[1]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([true.shape[0], y+2])\n",
    "            max_x = np.min([true.shape[0], x+2])\n",
    "            if true[x, y] == 1:\n",
    "                if np.sum(pred[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    tp[x, y] = 1\n",
    "                else:\n",
    "                    fn[x, y] = 1\n",
    "            if pred[x, y] == 1:\n",
    "                if np.sum(true[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    if true[x, y] == 1:\n",
    "                        tp[x, y] = 1\n",
    "                else:\n",
    "                    fp[x, y] = 1                \n",
    "                \n",
    "    return np.sum(tp), np.sum(fp), np.sum(fn)\n",
    "\n",
    "def calculate_metrics(al = 0.4, canopy_thresh = 100):\n",
    "    '''Calculates the following metrics\n",
    "       \n",
    "         - Loss\n",
    "         - F1\n",
    "         - Precision\n",
    "         - Recall\n",
    "         - Dice\n",
    "         - Mean surface distance\n",
    "         - Average error\n",
    "    \n",
    "         Parameters:\n",
    "          al (float):\n",
    "          canopy_thresh (int)\n",
    "          \n",
    "         Returns:\n",
    "          val_loss (float):\n",
    "          best_dice (float):\n",
    "          error (float):\n",
    "    '''\n",
    "    start_idx = 0\n",
    "    stop_idx = len(test_x)\n",
    "    best_f1, best_thresh, relaxed_f1 = 0, 0, 0\n",
    "    preds, trues, vls = [], [], []\n",
    "\n",
    "    test_ids = [x for x in range(len(test_x))]\n",
    "    for test_sample in test_ids[start_idx:stop_idx]:\n",
    "        if np.sum(test_y[test_sample]) < ((canopy_thresh/100) * 197):\n",
    "            x_input = test_x[test_sample].reshape(1, 13, 28, 28, n_bands)\n",
    "            x_median_input = calc_median_input(x_input)\n",
    "            y, vl = sess.run([fm, test_loss], feed_dict={inp: x_input,\n",
    "                                                          length: np.full((1,), 12),\n",
    "                                                          is_training: False,\n",
    "                                                          labels: test_y[test_sample].reshape(1, OUT, OUT),\n",
    "                                                          loss_weight: 0.1,\n",
    "                                                          alpha: 0.33,\n",
    "                                                          })\n",
    "            preds.append(y.reshape((OUT, OUT)))\n",
    "            vls.append(vl)\n",
    "            trues.append(test_y[test_sample].reshape((OUT, OUT)))\n",
    "            \n",
    "    # These threshes are just for ROC\n",
    "    for thresh in range(7, 9):\n",
    "        tps_relaxed = np.empty((len(preds), ))\n",
    "        fps_relaxed = np.empty((len(preds), ))\n",
    "        fns_relaxed = np.empty((len(preds), ))\n",
    "        abs_error = np.empty((len(preds), ))\n",
    "        \n",
    "        for sample in range(len(preds)):\n",
    "            pred = np.copy(preds[sample])\n",
    "            true = trues[sample]\n",
    "        \n",
    "            pred[np.where(pred >= thresh*0.05)] = 1\n",
    "            pred[np.where(pred < thresh*0.05)] = 0\n",
    "            \n",
    "            true_s = np.sum(true[1:-1])\n",
    "            pred_s = np.sum(pred[1:-1])\n",
    "            abs_error[sample] = abs(true_s - pred_s)\n",
    "            tp_relaxed, fp_relaxed, fn_relaxed = compute_f1_score_at_tolerance(true, pred)\n",
    "            tps_relaxed[sample] = tp_relaxed\n",
    "            fps_relaxed[sample] = fp_relaxed\n",
    "            fns_relaxed[sample] = fn_relaxed                   \n",
    "            \n",
    "        oa_error = np.mean(abs_error)\n",
    "        precision_r = np.sum(tps_relaxed) / (np.sum(tps_relaxed) + np.sum(fps_relaxed))\n",
    "        recall_r = np.sum(tps_relaxed) / (np.sum(tps_relaxed) + np.sum(fns_relaxed))\n",
    "        f1_r = 2*((precision_r* recall_r) / (precision_r + recall_r))\n",
    "        \n",
    "        if f1_r > best_f1:\n",
    "            best_f1 = f1_r\n",
    "            p = precision_r\n",
    "            r = recall_r\n",
    "            error = oa_error\n",
    "            best_thresh = thresh*0.05\n",
    "\n",
    "    print(f\"Val loss: {np.around(np.mean(vls), 3)}\"\n",
    "          f\" Thresh: {np.around(best_thresh, 2)}\"\n",
    "          f\" F1: {np.around(best_f1, 3)} R: {np.around(p, 3)} P: {np.around(r, 3)}\"\n",
    "          f\" Error: {np.around(error, 3)}\")\n",
    "    return np.mean(vls), best_f1, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code block implements cut mix where random samples are spliced together where the output labels have similar tree cover distributions (within the same kmeans cluster). Not super necessary but does give a small performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN = 4\n",
    "def augment_batch(batch_ids, batch_size):\n",
    "    '''Performs random flips and rotations of the X and Y\n",
    "       data for a total of 4 x augmentation\n",
    "    \n",
    "         Parameters:\n",
    "          batch_ids (list):\n",
    "          batch_size (int):\n",
    "          \n",
    "         Returns:\n",
    "          x_batch (arr):\n",
    "          y_batch (arr):\n",
    "    '''\n",
    "    x = np.copy(train_x[batch_ids])\n",
    "    samples_to_median = np.random.randint(0, 12, size=(batch_size, 12)) #[32, 6]\n",
    "    samples_to_select = np.zeros((batch_size, 4))\n",
    "    samples_to_select[:, 0] = np.random.randint(0, 4, size=(batch_size))\n",
    "    samples_to_select[:, 1] = np.random.randint(3, 7, size=(batch_size))\n",
    "    samples_to_select[:, 2] = np.random.randint(6, 10, size=(batch_size))\n",
    "    samples_to_select[:, 3] = np.random.randint(8, 12, size=(batch_size))\n",
    "    #samples_to_select = np.sort(samples_to_select, axis = 1)\n",
    "    samples_to_select = samples_to_select.astype(np.int)\n",
    "    #samples_to_median = np.sort(samples_to_median, axis = 1)\n",
    "    n_samples = np.random.randint(2, 5, size=(batch_size)) \n",
    "    \n",
    "    x_batch = np.zeros((x.shape[0], LEN + 1, 172, 172, 17))\n",
    "    for samp in range(batch_size):\n",
    "        samps = samples_to_median[samp, :]#:np.random.randint(6, 12)]\n",
    "        lower_samp = np.min(samps)\n",
    "        upper_samp = np.max(samps)\n",
    "        #print(np.unique(samps))\n",
    "        x_samp = x[samp]\n",
    "        samps = np.unique(samps)\n",
    "        med_samp = np.median(x_samp[samps], axis = 0)\n",
    "        #med_samp = np.median(x[samp, np.unique(samps)], axis = 0)\n",
    "\n",
    "       \n",
    "        if x_batch.shape[1] == 5:\n",
    "            #print(samples_to_select[samp])\n",
    "            x_batch[samp, :-1, ...] = x[samp, samples_to_select[samp]]\n",
    "        else:\n",
    "            x[samp, :lower_samp] = x[samp, lower_samp]\n",
    "            x[samp, upper_samp:] = x[samp, upper_samp]\n",
    "            x_batch[samp, :-1] = x[samp]\n",
    "        x_batch[samp, -1, ...] = med_samp\n",
    "        \n",
    "    x = x_batch\n",
    "    \n",
    "    #xmed = x[:, -1]\n",
    "\n",
    "    #x = np.concatenate([x, xmed[:, np.newaxis]], axis = 1)\n",
    "        \n",
    "    y = train_y[batch_ids]\n",
    "\n",
    "    \n",
    "    y_batch = np.zeros_like(y)\n",
    "    #chmy_batch = np.zeros_like(y)\n",
    "    \n",
    "    flips = np.random.choice(np.array([0, 1, 2, 3]), batch_size, replace = True)\n",
    "    for i in range(x.shape[0]):\n",
    "        current_flip = flips[i]\n",
    "        if current_flip == 0:\n",
    "            x_batch[i] = x[i]\n",
    "            y_batch[i] = y[i]\n",
    "            #chmy_batch[i] = chmy[i]\n",
    "            \n",
    "        if current_flip == 1:\n",
    "            x_batch[i] = np.flip(x[i], 1)\n",
    "            y_batch[i] = np.flip(y[i], 0)\n",
    "            #chmy_batch[i] = np.flip(chmy[i], 0)\n",
    "        if current_flip == 2:\n",
    "            x_batch[i] = np.flip(x[i], [2, 1])\n",
    "            y_batch[i] = np.flip(y[i], [1, 0])\n",
    "           # chmy_batch[i] = np.flip(chmy[i], [1, 0])\n",
    "        if current_flip == 3:\n",
    "            x_batch[i] = np.flip(x[i], 2)\n",
    "            y_batch[i] = np.flip(y[i], 1)\n",
    "           # chmy_batch[i] = np.flip(chmy[i], 1)\n",
    "\n",
    "    y_batch = y_batch.reshape((batch_size, 158, 158))\n",
    "\n",
    "    #x_batch, y_batch = cut_mix(x_batch, y_batch, batch_ids, 0.5, batch_size)\n",
    "    #x_batch = np.delete(x_batch, [11, 12], axis = -1)\n",
    "    return x_batch, y_batch \n",
    "\n",
    "#x_batch_test, y_batch_test= augment_batch([x for x in range(32)], 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = 75\n",
    "end = 125\n",
    "f, ((c1r1, c1r2), (c2r1, c2r2)) = plt.subplots(2, 2, sharey=False)\n",
    "f.set_size_inches(15, 12)\n",
    "\n",
    "c1r1.set_title(f\"Train loss - {model_path}\")\n",
    "l1 = sns.scatterplot(y = metrics[0, start:end], x = np.arange(start, end), ax = c1r1)\n",
    "l1.set(ylim=(0.30, .40))\n",
    "\n",
    "c1r2.set_title(\"F1 score\")\n",
    "f =sns.scatterplot(y = metrics[5, start:end], x = np.arange(start, end), ax = c1r2)\n",
    "f.set(ylim=(0.84, .91))\n",
    "\n",
    "c2r1.set_title(\"Test loss\")\n",
    "l = sns.scatterplot(y = metrics[1, start:end], x = np.arange(start, end), ax = c2r1)\n",
    "l.set(ylim=(0.140, .165)) \n",
    "\n",
    "c2r2.set_title(\"Absolute % error\")\n",
    "e = sns.scatterplot(y = metrics[2, start:end] / 2, x = np.arange(start, end), ax = c2r2)\n",
    "e.set(ylim=(2.2, 3.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "best_val = 0.72\n",
    "fine_tune = False\n",
    "ft_epochs = 0\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# loss2 125-160 is 0.4 alpha, >0.6 surface, 0.33 loss weight\n",
    "# loss45 is 0.4 alpha, >0.45 surface, 0.4 loss weight\n",
    "# loss45 250 - 300 is 0.4 alpha, >0.45 surface, 0.4 loss weight with minimum surface loss\n",
    "nepochs = 2\n",
    "for i in range(351, 360):\n",
    "    if i >= 120:\n",
    "        SWA = False# set to true to start SWA\n",
    "    else:\n",
    "        SWA = False\n",
    "    al = 0.35\n",
    "    \n",
    "    ft_learning_rate = 5e-3\n",
    "    if nepochs < 5:\n",
    "        ft_learning_rate *= (0.2 * nepochs)\n",
    "    nepochs += 1\n",
    "    be = 0.0\n",
    "    test_al = al\n",
    "    op = train_op# if fine_tune else train_op\n",
    "        \n",
    "    train_ids = [x for x in range(len(train_y))]\n",
    "    np.random.shuffle(train_ids)\n",
    "    randomize = train_ids\n",
    "    #randomize = equibatch(train_ids, 0)\n",
    "    print(f\"starting epoch {i}, \" \n",
    "          f\"alpha: {al}, beta: {be}, \"\n",
    "          f\"drop: {np.max(((1. - (i * 0.005)), 0.6))} \"\n",
    "          f\"Learning rate: {ft_learning_rate}\"\n",
    "         )\n",
    "    \n",
    "    loss = train_loss\n",
    "    #test_ids = [x for x in range(0, len(test_x))]\n",
    "    losses = []\n",
    "    \n",
    "    for k in tqdm.notebook.tnrange(int(len(randomize) // BATCH_SIZE)):\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        x_batch, y_batch = augment_batch(batch_ids, BATCH_SIZE)\n",
    "        #x_batch = augment_batch(batch_ids, BATCH_SIZE)\n",
    "        opt, tr = sess.run([op, loss],\n",
    "                          feed_dict={inp: x_batch,\n",
    "                                     length: np.full((BATCH_SIZE,), 4),\n",
    "                                     labels: y_batch,\n",
    "                                     mask: np.ones_like(y_batch),\n",
    "                                     is_training: True,\n",
    "                                     loss_weight: 0.4,\n",
    "                                     keep_rate: 0.5,#np.max(((1. - (i * 0.01)), MAX_DROPBLOCK)),\n",
    "                                     alpha: al,\n",
    "                                     beta_: be,\n",
    "                                     ft_lr: ft_learning_rate,\n",
    "                                     })\n",
    "        losses.append(tr)\n",
    "    \n",
    "    print(f\"Epoch {i}: Loss {np.around(np.mean(losses[:-1]), 3)}\")\n",
    "    #os.mkdir(f\"../models/epoch15{str(i)}\")\n",
    "    saver.save(sess, f\"../models/epoch30-{str(i)}/model\")\n",
    "    output_node_names = ['conv2d_5/Sigmoid']\n",
    "    frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "        sess,\n",
    "        sess.graph_def,\n",
    "        output_node_names)\n",
    "\n",
    "\n",
    "    # Save the frozen graph\n",
    "    with open(f'../models/epoch19/predict_graph-{str(i)}.pb', 'wb') as f:\n",
    "        f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(swa_to_weights)\n",
    "saver = tf.train.Saver(max_to_keep = 150)\n",
    "#os.mkdir(f\"../models/loss2/\")\n",
    "save_path = saver.save(sess, f\"../models/loss2/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_node_names = ['conv2d_5/Sigmoid']\n",
    "frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess,\n",
    "    sess.graph_def,\n",
    "    output_node_names)\n",
    "\n",
    "\n",
    "# Save the frozen graph\n",
    "with open('../models/loss3/predict_graph.pb', 'wb') as f:\n",
    "    f.write(frozen_graph_def.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
