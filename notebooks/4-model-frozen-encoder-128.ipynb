{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree segmentation with multitemporal Sentinel 1/2 imagery\n",
    "\n",
    "## John Brandt\n",
    "## December 2023\n",
    "\n",
    "## This notebook finetunes the TTC decoder for a new task\n",
    "\n",
    "## Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_29038/3754813325.py:4: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2023-12-22 12:39:56.310226: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-22 12:39:56.310907: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_29038/3754813325.py:16: The name tf.initializers.lecun_normal is deprecated. Please use tf.compat.v1.initializers.lecun_normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jbrandt.terminal/Documents/GitHub/sentinel-tree-cover/src/layers/zoneout.py:8: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ../src/layers/zoneout.py\n",
    "%run ../src/layers/losses.py\n",
    "%run ../src/layers/adabound.py\n",
    "%run ../src/layers/convgru.py\n",
    "%run ../src/layers/dropblock.py\n",
    "%run ../src/layers/extra_layers.py\n",
    "%run ../src/layers/stochastic_weight_averaging.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/preprocessing/slope.py\n",
    "#%run ../src/utils/metrics.py\n",
    "#%run ../src/utils/lovasz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.90\n",
    "ACTIVATION_FUNCTION = 'swish'\n",
    "\n",
    "INITIAL_LR = 1e-3\n",
    "DROPBLOCK_MAXSIZE = 5\n",
    "\n",
    "N_CONV_BLOCKS = 1\n",
    "FINAL_ALPHA = 0.33\n",
    "LABEL_SMOOTHING = 0.03\n",
    "\n",
    "L2_REG = 0.\n",
    "BATCH_SIZE = 32\n",
    "MAX_DROPBLOCK = 0.6\n",
    "\n",
    "FRESH_START = True\n",
    "best_val = 0.2\n",
    "\n",
    "START_EPOCH = 1\n",
    "END_EPOCH = 100\n",
    "\n",
    "n_bands = 17\n",
    "initial_flt = 32\n",
    "mid_flt = 32 * 2\n",
    "high_flt = 32 * 2 * 2\n",
    "\n",
    "temporal_model = True\n",
    "input_size = 124\n",
    "output_size = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layer definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv GRU Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_block(inp, length, size, flt, scope, train, normalize = True):\n",
    "    '''Bidirectional convolutional GRU block with \n",
    "       zoneout and CSSE blocks in each time step\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): (B, T, H, W, C) layer\n",
    "          length (tf.Variable): (B, T) layer denoting number of\n",
    "                                steps per sample\n",
    "          size (int): kernel size of convolution\n",
    "          flt (int): number of convolution filters\n",
    "          scope (str): tensorflow variable scope\n",
    "          train (tf.Bool): flag to differentiate between train/test ops\n",
    "          normalize (bool): whether to compute layer normalization\n",
    "\n",
    "         Returns:\n",
    "          gru (tf.Variable): (B, H, W, flt*2) bi-gru output\n",
    "          steps (tf.Variable): (B, T, H, W, flt*2) output of each step\n",
    "    '''\n",
    "    with tf.variable_scope(scope):\n",
    "        print(f\"GRU input shape {inp.shape}, zoneout: {ZONE_OUT_PROB}\")\n",
    "        \n",
    "        # normalize is internal group normalization within the reset gate\n",
    "        # sse is internal SSE block within the state cell\n",
    "\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', \n",
    "                           normalize = normalize, sse = True)\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID',\n",
    "                           normalize = normalize, sse = True)\n",
    "        \n",
    "        cell_fw = ZoneoutWrapper(\n",
    "           cell_fw, zoneout_drop_prob = 0.75, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = 0.75, is_training = train)\n",
    "        steps, out = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        gru = tf.concat(out, axis = -1)\n",
    "        steps = tf.concat(steps, axis = -1)\n",
    "        print(f\"GRU block output shape {gru.shape}\")\n",
    "    return gru, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_29038/2487390168.py:7: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_29038/2487390168.py:8: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg = tf.contrib.layers.l2_regularizer(0.)\n",
    "temporal_model = True\n",
    "n_bands = 17\n",
    "output_size = input_size - 14\n",
    "\n",
    "if temporal_model:\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, 5, input_size, input_size, n_bands))\n",
    "    length = tf.placeholder_with_default(np.full((1,), 4), shape = (None,))\n",
    "else:\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, input_size, input_size, n_bands))\n",
    "    \n",
    "labels = tf.placeholder(tf.float32, shape=(None, output_size, output_size))#, 1))\n",
    "mask = tf.placeholder(tf.float32, shape = (None, output_size, output_size))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training') # For DropBlock\n",
    "alpha = tf.placeholder(tf.float32, shape = ()) # For loss scheduling\n",
    "ft_lr = tf.placeholder_with_default(0.001, shape = ()) # For loss scheduling\n",
    "loss_weight = tf.placeholder_with_default(1.0, shape = ())\n",
    "beta_ = tf.placeholder_with_default(0.0, shape = ()) # For loss scheduling, not currently implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_29038/2476193274.py:19: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "GRU input shape (?, 4, 124, 124, 17), zoneout: 0.9\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "(3, 3, 49, 64)\n",
      "(3, 3, 49, 64)\n",
      "GRU block output shape (?, 124, 124, 64)\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/Documents/GitHub/sentinel-tree-cover/src/layers/dropblock.py:154: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "conv_median 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv_median_conv/conv_median/x/mul:0\", shape=(?, 124, 124, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32:0\", shape=(?, 124, 124, 64), dtype=float32)\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/Documents/GitHub/sentinel-tree-cover/src/layers/extra_layers.py:309: The name tf.keras.initializers.he_normal is deprecated. Please use tf.compat.v1.keras.initializers.he_normal instead.\n",
      "\n",
      "Median conv: (?, 124, 124, 64)\n",
      "conv_concat 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv_concat_conv/conv_concat/x/mul:0\", shape=(?, 124, 124, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_1:0\", shape=(?, 124, 124, 64), dtype=float32)\n",
      "Concat: (?, 124, 124, 64)\n",
      "conv1 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv1_conv/conv1/ws_conv2d_2/Conv2D:0\", shape=(?, 60, 60, 128), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_2:0\", shape=(?, 60, 60, 128), dtype=float32)\n",
      "Conv1: (?, 60, 60, 128)\n",
      "conv2 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"conv2_conv/conv2/ws_conv2d_3/Conv2D:0\", shape=(?, 28, 28, 256), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_3:0\", shape=(?, 28, 28, 256), dtype=float32)\n",
      "Encoded (?, 28, 28, 256)\n",
      "up2 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"up2_conv/up2/x/mul:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_4:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "(?, 56, 56, 128)\n",
      "up2_out 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"up2_out_conv/up2_out/x/mul:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_5:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "up3 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "The non normalized feats are Tensor(\"up3_conv/up3/x/mul:0\", shape=(?, 112, 112, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_6:0\", shape=(?, 112, 112, 64), dtype=float32)\n",
      "(?, 112, 112, 64)\n",
      "(?, 112, 112, 64)\n",
      "out 3 Conv 2D Group Norm RELU CSSE NoBias NoDrop\n",
      "The non normalized feats are Tensor(\"out_conv/out/ws_conv2d_7/Conv2D:0\", shape=(?, 110, 110, 64), dtype=float32)\n",
      "The non normalized feats are Tensor(\"swish_f32_7:0\", shape=(?, 110, 110, 64), dtype=float32)\n",
      "The output is (?, 56, 56, 128), with a receptive field of 1\n",
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_29038/3645527012.py:89: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "Tensor(\"conv2d/Sigmoid:0\", shape=(?, 110, 110, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# master modmel is 32, 64, 96, 230k paramms\n",
    "initial_flt = 64\n",
    "mid_flt = initial_flt * 2\n",
    "high_flt = initial_flt * 2 * 2\n",
    "\n",
    "gru_input = inp[:, :-1, ...]\n",
    "gru, steps = gru_block(inp = gru_input, length = length,\n",
    "                            size = [124, 124, ], # + 2 here for refleclt pad\n",
    "                            flt = initial_flt // 2,\n",
    "                            scope = 'down_16',\n",
    "                            train = is_training)\n",
    "with tf.variable_scope(\"gru_drop\"):\n",
    "    drop_block = DropBlock2D(keep_prob=keep_rate, block_size=4)\n",
    "    gru = drop_block(gru, is_training)\n",
    "    \n",
    "# Median conv\n",
    "median_input = inp[:, -1, ...]\n",
    "median_conv = conv_swish_gn(inp = median_input, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_median', filters = initial_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None,\n",
    "                            window_size = 104)\n",
    "print(f\"Median conv: {median_conv.shape}\")\n",
    "\n",
    "concat1 = tf.concat([gru, median_conv], axis = -1)\n",
    "\n",
    "\n",
    "concat = conv_swish_gn(inp = concat1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_concat', filters = initial_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, padding = \"SAME\",\n",
    "                       window_size = 104)\n",
    "print(f\"Concat: {concat.shape}\")\n",
    "\n",
    "# MaxPool-conv-swish-GroupNorm-csse\n",
    "pool1 = MaxPool2D()(concat)\n",
    "conv1 = conv_swish_gn(inp = pool1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv1', filters = mid_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True, padding = \"VALID\",\n",
    "            csse = True, dropblock = True, weight_decay = None)\n",
    "print(f\"Conv1: {conv1.shape}\")\n",
    "\n",
    "# MaxPool-conv-swish-csse-DropBlock\n",
    "pool2 = MaxPool2D()(conv1)\n",
    "conv2 = conv_swish_gn(inp = pool2, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv2', filters = high_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, block_size = 4, padding = \"VALID\",\n",
    "                     window_size = 24)\n",
    "print(\"Encoded\", conv2.shape)\n",
    "\n",
    "# Decoder 4 - 8, upsample-conv-swish-csse-concat-conv-swish\n",
    "up2 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(conv2)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2', filters = mid_flt, \n",
    "                    keep_rate = keep_rate, activation = True,use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "conv1_crop = Cropping2D(2)(conv1)\n",
    "print(conv1_crop.shape)\n",
    "up2 = tf.concat([up2, conv1_crop], -1)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2_out', filters = mid_flt, \n",
    "                    keep_rate =  keep_rate, activation = True,use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "\n",
    "# Decoder 8 - 14 upsample-conv-swish-csse-concat-conv-swish\n",
    "up3 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(up2)\n",
    "#up3 = ReflectionPadding2D((1, 1,))(up3)\n",
    "up3 = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up3', filters = initial_flt, \n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None, \n",
    "                    window_size = 104)\n",
    "gru_crop = Cropping2D(6)(concat)\n",
    "print(up3.shape)\n",
    "print(gru_crop.shape)\n",
    "up3 = tf.concat([up3, gru_crop], -1)\n",
    "\n",
    "up3out = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'out', filters = initial_flt, \n",
    "                    keep_rate  = keep_rate, activation = True,use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = False, weight_decay = None, padding = \"VALID\",\n",
    "                       window_size = 104)\n",
    "\n",
    "init = tf.constant_initializer([-np.log(0.7/0.3)]) # For focal loss\n",
    "print(f\"The output is {up2.shape}, with a receptive field of {1}\")\n",
    "fm = tf.layers.Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1),\n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init, name = 'conv2d')(up3out)#,\n",
    "print(fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_5\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up3\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up3_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up3\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_out_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up2_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up2\")# + \\\n",
    "\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"csse_conv2\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv1\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"csse_conv1\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv1\")\n",
    "finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"csse_conv1\")\n",
    "#finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv_concat\")\n",
    "#finetune_vars = finetune_vars + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"csse_conv_concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_29038/3460921231.py:26: The name tf.losses.get_regularization_loss is deprecated. Please use tf.compat.v1.losses.get_regularization_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_29038/3460921231.py:27: The name tf.losses.get_regularization_losses is deprecated. Please use tf.compat.v1.losses.get_regularization_losses instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 12:40:02.930471: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_29038/3460921231.py:60: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def grad_norm(gradients):\n",
    "    norm = tf.compat.v1.norm(\n",
    "        tf.stack([\n",
    "            tf.compat.v1.norm(grad) for grad in gradients if grad is not None\n",
    "        ])\n",
    "    )\n",
    "    return norm\n",
    "\n",
    "FRESH_START = True\n",
    "#print(f\"Starting model with: \\n {ZONE_OUT_PROB} zone out \\n {L2_REG} l2 \\n\"\n",
    " #     f\"{INITIAL_LR} initial LR \\n {total_parameters} parameters\")  \n",
    "\n",
    "OUT = input_size - 14\n",
    "if FRESH_START:\n",
    "    # We use the Adabound optimizer\n",
    "    optimizer = AdaBoundOptimizer(2e-4, 2e-2)\n",
    "    #train_loss1 = logcosh(tf.reshape(labels, (-1, 14, 14, 1)), output) \n",
    "    \n",
    "    train_loss2 = bce_surface_loss(tf.reshape(labels, (-1, OUT, OUT, 1)), fm,\n",
    "                                  weight = loss_weight, \n",
    "                             alpha = alpha, beta = beta_, mask = mask)\n",
    "\n",
    "    train_loss = train_loss2# + train_loss2\n",
    "    \n",
    "    # If there is any L2 regularization, add it. Current model does not use\n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    if len(tf.losses.get_regularization_losses()) > 0:\n",
    "        train_loss = train_loss + l2_loss\n",
    "        \n",
    "    test_loss = bce_surface_loss(tf.reshape(labels, (-1, OUT, OUT, 1)),\n",
    "                            fm, weight = loss_weight, \n",
    "                            alpha = alpha, beta = beta_, mask = mask)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(train_loss, var_list = finetune_vars)   \n",
    "        #ft_op = ft_optimizer.minimize(train_loss)\n",
    "    \n",
    "    # The following code blocks are for sharpness aware minimization\n",
    "    # Adapted from https://github.com/sayakpaul/Sharpness-Aware-Minimization-TensorFlow\n",
    "    # For tensorflow 1.15\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "    gradient_norm = grad_norm(gradients)\n",
    "    scale = 0.05 / (gradient_norm + 1e-12)\n",
    "    e_ws = []\n",
    "    for (grad, param) in gradients:\n",
    "        e_w = grad * scale\n",
    "        param.assign_add(e_w)\n",
    "        e_ws.append(e_w)\n",
    "\n",
    "    sam_gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "    for (param, e_w) in zip(trainable_params, e_ws):\n",
    "        param.assign_sub(e_w)\n",
    "    train_step = optimizer.apply_gradients(sam_gradients)\n",
    "    \n",
    "    # Create a saver to save the model each epoch\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 150)#, var_list = all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_saver_varlist(path):\n",
    "\n",
    "    current_items = []\n",
    "    vars_dict = {}\n",
    "    for var_current in tf.global_variables():\n",
    "        current_items.append(var_current) \n",
    "    names = [x.op.name for x in current_items]\n",
    "    names = np.argsort(names)\n",
    "    current_items = [current_items[x] for x in names]\n",
    "    \n",
    "    ckpt_items = []\n",
    "    for var_ckpt in tf.train.list_variables(path):\n",
    "        if 'BackupVariables' not in var_ckpt[0]:\n",
    "            if 'StochasticWeightAveraging' not in var_ckpt[0]:\n",
    "                if 'global_step' not in var_ckpt[0]:\n",
    "                    if 'is_training' not in var_ckpt[0]:\n",
    "                        if 'n_models' not in var_ckpt[0]:\n",
    "                            ckpt_items.append(var_ckpt[0])\n",
    "    \n",
    "    ckptdict = {}\n",
    "    for y, x in zip(ckpt_items, current_items):\n",
    "        ckptdict[y] = x\n",
    "    return ckptdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckptdict = make_saver_varlist('../models/172-ttc-dec2023-3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting anew\n",
      "INFO:tensorflow:Restoring parameters from ../models/172-ttc-dec2023-3/-0\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(ckptdict)\n",
    "model_path  = \"../models/172-ttc-dec2023-3/\"\n",
    "FRESH_START = False\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")\n",
    "else:\n",
    "    print(\"Starting anew\")\n",
    "    metrics = np.zeros((6, 300))\n",
    "\n",
    "if not FRESH_START:\n",
    "    path = model_path\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver.save(sess, '../models/loss-avg-tf2/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "*  Load in CSV data from Collect Earth\n",
    "*  Reconstruct the X, Y grid for the Y data per sample\n",
    "*  Calculate remote sensing indices\n",
    "*  Stack X, Y, length data\n",
    "*  Apply median filter to DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "\n",
    "def normalize_subtile(subtile):\n",
    "    for band in range(0, subtile.shape[-1]):\n",
    "        mins = min_all[band]\n",
    "        maxs = max_all[band]\n",
    "        subtile[..., band] = np.clip(subtile[..., band], mins, maxs)\n",
    "        midrange = (maxs + mins) / 2\n",
    "        rng = maxs - mins\n",
    "        standardized = (subtile[..., band] - midrange) / (rng / 2)\n",
    "        subtile[..., band] = standardized\n",
    "    return subtile\n",
    " \n",
    "def make_and_smooth_indices(arr):\n",
    "    \"\"\"Calculates remote sensing indices\n",
    "    (evi, bi, msavi2, grndvi) and smooths them\n",
    "    with the Whittaker smoother\n",
    "    \"\"\"\n",
    "    def _make_indices(arr):\n",
    "        indices = np.zeros(\n",
    "            (arr.shape[0], arr.shape[1], arr.shape[2], 4), dtype = np.float32\n",
    "        )\n",
    "        indices[:, ..., 0] = evi(arr)\n",
    "        indices[:, ...,  1] = bi(arr)\n",
    "        indices[:, ...,  2] = msavi2(arr)\n",
    "        indices[:, ...,  3] = grndvi(arr)\n",
    "        return indices\n",
    "\n",
    "    sm_indices = Smoother(lmbd = 50, \n",
    "                          size = 12, \n",
    "                          nbands = 4, \n",
    "                          dimx = arr.shape[1],\n",
    "                          dimy = arr.shape[2], \n",
    "                          outsize = 12)\n",
    "\n",
    "    indices = _make_indices(arr)\n",
    "    indices = sm_indices.interpolate_array(indices)\n",
    "    return indices\n",
    "\n",
    "def load_individual_sample(fpath, ypath, f):\n",
    "    ishkl = os.path.exists(fpath + f + '.hkl')\n",
    "    if ishkl:\n",
    "        x = hkl.load(fpath + f + '.hkl') / 65535\n",
    "    else:\n",
    "        x = np.load(fpath + f + \".npy\") / 65535\n",
    "\n",
    "    if x.shape[-1] == 13:\n",
    "        i = make_and_smooth_indices(x)\n",
    "        out = np.zeros((x.shape[0], x.shape[1], x.shape[2], 17), dtype = np.float32)\n",
    "        out[..., :13] = x \n",
    "        out[..., 13:] = i\n",
    "    else:\n",
    "        out = x\n",
    "        out[..., -1] *= 2\n",
    "        out[..., -1] -= 0.7193834232943873\n",
    "        \n",
    "        #out[-1] -= 0.7193834232943873\n",
    "        out[..., -2] -= 0.09731556326714398\n",
    "        out[..., -3] -= 0.4973397113668104,\n",
    "        out[..., -4] -= 0.1409399364817101\n",
    "        #out[]\n",
    "    #median = np.median(out, axis = 0)\n",
    "    #out = np.reshape(out, (4, 3, out.shape[1], out.shape[2], out.shape[3]))\n",
    "    #out = np.median(out, axis = 1, overwrite_input = True)\n",
    "    #out = np.concatenate([out, median[np.newaxis]], axis = 0)\n",
    "    return normalize_subtile(out[:, 2:-2, 2:-2, :]), rs.open(ypath + f + \".tif\").read(1) / 255\n",
    "\n",
    "def augment_single_sample(x, y):\n",
    "    '''Performs random flips and rotations of the X and Y\n",
    "       data for a total of 4 x augmentation\n",
    "    \n",
    "         Parameters:\n",
    "          batch_ids (list):\n",
    "          batch_size (int):\n",
    "          \n",
    "         Returns:\n",
    "          x_batch (arr):\n",
    "          y_batch (arr):\n",
    "    '''\n",
    "    samples_to_median = np.random.randint(0, 12, size=(12,)) #[32, 6]\n",
    "    samples_to_select = np.zeros((4))\n",
    "    samples_to_select[0] = np.random.randint(0, 3, size=(1))\n",
    "    samples_to_select[1] = np.random.randint(3, 6, size=(1))\n",
    "    samples_to_select[2] = np.random.randint(6, 9, size=(1))\n",
    "    samples_to_select[3] = np.random.randint(9, 12, size=(1))\n",
    "    samples_to_select = samples_to_select.astype(np.int)\n",
    "    n_samples = np.random.randint(2, 10) \n",
    "    \n",
    "    x_batch = np.zeros((5, 124, 124, 17))\n",
    "    x_batch[0] = x[samples_to_select[0]]\n",
    "    x_batch[1] = x[samples_to_select[1]]\n",
    "    x_batch[2] = x[samples_to_select[2]]\n",
    "    x_batch[3] = x[samples_to_select[3]]\n",
    "    x_batch[4] = np.median(x[samples_to_median[:n_samples]], axis = 0)\n",
    "    x = x_batch\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_all = [0.006576638437476157, 0.0162050812542916, 0.010040436408026246, \n",
    "           0.013351644159609368, 0.01965362020294499, 0.014229037918669413, \n",
    "           0.015289539940489814, 0.011993591210803388, 0.008239871824216068, \n",
    "           0.006546120393682765, 0.0, 0.0, 0.0, -0.1409399364817101, \n",
    "           -0.4973397113668104, -0.09731556326714398, -0.7193834232943873]\n",
    "\n",
    "max_all = [0.2691233691920348, 0.3740291447318227, 0.5171435111009385, 0.6027466239414053,\n",
    "           0.5650263218127718, 0.5747005416952773, 0.5933928435187305, 0.6034943160143434, \n",
    "           0.7472037842374304, 0.4, 0.509269855802243, 0.948334642387533, \n",
    "           0.6729257769285485, 0.8177635298774327, 0.35768999002433816, 0.7545951919107605, \n",
    "           0.7602693339366691]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score_at_tolerance(true, pred, tolerance = 1):\n",
    "    \"\"\"Because of coregistration errors, we evaluate the model\n",
    "    where false positives/negatives must be >1px away from a true positive\n",
    "    \"\"\"\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    tp = np.zeros_like(true)\n",
    "    fp = np.zeros_like(true)\n",
    "    fn = np.zeros_like(true)\n",
    "    \n",
    "    for x in range(true.shape[0]):\n",
    "        for y in range(true.shape[1]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([true.shape[0], y+2])\n",
    "            max_x = np.min([true.shape[0], x+2])\n",
    "            if true[x, y] == 1:\n",
    "                if np.sum(pred[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    tp[x, y] = 1\n",
    "                else:\n",
    "                    fn[x, y] = 1\n",
    "            if pred[x, y] == 1:\n",
    "                if np.sum(true[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    if true[x, y] == 1:\n",
    "                        tp[x, y] = 1\n",
    "                else:\n",
    "                    fp[x, y] = 1                \n",
    "                \n",
    "    return np.sum(tp), np.sum(fp), np.sum(fn)\n",
    "\n",
    "def calculate_metrics(al = 0.4, canopy_thresh = 100):\n",
    "    '''Calculates the following metrics\n",
    "       \n",
    "         - Loss\n",
    "         - F1\n",
    "         - Precision\n",
    "         - Recall\n",
    "         - Dice\n",
    "         - Mean surface distance\n",
    "         - Average error\n",
    "    \n",
    "         Parameters:\n",
    "          al (float):\n",
    "          canopy_thresh (int)\n",
    "          \n",
    "         Returns:\n",
    "          val_loss (float):\n",
    "          best_dice (float):\n",
    "          error (float):\n",
    "    '''\n",
    "    start_idx = 0\n",
    "    stop_idx = len(test_x)\n",
    "    best_f1, best_thresh, relaxed_f1 = 0, 0, 0\n",
    "    preds, trues, vls = [], [], []\n",
    "\n",
    "    test_ids = [x for x in range(len(test_x))]\n",
    "    for test_sample in test_ids[start_idx:stop_idx]:\n",
    "        if np.sum(test_y[test_sample]) < ((canopy_thresh/100) * 197):\n",
    "            x_input = test_x[test_sample].reshape(1, 13, 28, 28, n_bands)\n",
    "            x_median_input = calc_median_input(x_input)\n",
    "            y, vl = sess.run([fm, test_loss], feed_dict={inp: x_input,\n",
    "                                                          length: np.full((1,), 12),\n",
    "                                                          is_training: False,\n",
    "                                                          labels: test_y[test_sample].reshape(1, OUT, OUT),\n",
    "                                                          loss_weight: 0.1,\n",
    "                                                          alpha: 0.33,\n",
    "                                                          })\n",
    "            preds.append(y.reshape((OUT, OUT)))\n",
    "            vls.append(vl)\n",
    "            trues.append(test_y[test_sample].reshape((OUT, OUT)))\n",
    "            \n",
    "    # These threshes are just for ROC\n",
    "    for thresh in range(7, 9):\n",
    "        tps_relaxed = np.empty((len(preds), ))\n",
    "        fps_relaxed = np.empty((len(preds), ))\n",
    "        fns_relaxed = np.empty((len(preds), ))\n",
    "        abs_error = np.empty((len(preds), ))\n",
    "        \n",
    "        for sample in range(len(preds)):\n",
    "            pred = np.copy(preds[sample])\n",
    "            true = trues[sample]\n",
    "        \n",
    "            pred[np.where(pred >= thresh*0.05)] = 1\n",
    "            pred[np.where(pred < thresh*0.05)] = 0\n",
    "            \n",
    "            true_s = np.sum(true[1:-1])\n",
    "            pred_s = np.sum(pred[1:-1])\n",
    "            abs_error[sample] = abs(true_s - pred_s)\n",
    "            tp_relaxed, fp_relaxed, fn_relaxed = compute_f1_score_at_tolerance(true, pred)\n",
    "            tps_relaxed[sample] = tp_relaxed\n",
    "            fps_relaxed[sample] = fp_relaxed\n",
    "            fns_relaxed[sample] = fn_relaxed                   \n",
    "            \n",
    "        oa_error = np.mean(abs_error)\n",
    "        precision_r = np.sum(tps_relaxed) / (np.sum(tps_relaxed) + np.sum(fps_relaxed))\n",
    "        recall_r = np.sum(tps_relaxed) / (np.sum(tps_relaxed) + np.sum(fns_relaxed))\n",
    "        f1_r = 2*((precision_r* recall_r) / (precision_r + recall_r))\n",
    "        \n",
    "        if f1_r > best_f1:\n",
    "            best_f1 = f1_r\n",
    "            p = precision_r\n",
    "            r = recall_r\n",
    "            error = oa_error\n",
    "            best_thresh = thresh*0.05\n",
    "\n",
    "    print(f\"Val loss: {np.around(np.mean(vls), 3)}\"\n",
    "          f\" Thresh: {np.around(best_thresh, 2)}\"\n",
    "          f\" F1: {np.around(best_f1, 3)} R: {np.around(p, 3)} P: {np.around(r, 3)}\"\n",
    "          f\" Error: {np.around(error, 3)}\")\n",
    "    return np.mean(vls), best_f1, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code block implements cut mix where random samples are spliced together where the output labels have similar tree cover distributions (within the same kmeans cluster). Not super necessary but does give a small performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2294\n",
      "2043\n"
     ]
    }
   ],
   "source": [
    "import rasterio as rs\n",
    "LEN = 4\n",
    "train_xs= [x[:-4] for x in os.listdir('/Volumes/John/data/train-17k-may2023/figs/') if '.png' in x]\n",
    "print(len(train_xs))\n",
    "train_bad = [x[:-4] for x in os.listdir('/Volumes/John/data/train-17k-may2023/bad/')]\n",
    "train_xs = [x for x in train_xs if x not in train_bad]\n",
    "print(len(train_xs))\n",
    "x_path = '/Volumes/John/train-ard-128/'\n",
    "y_path = '/Volumes/John/data/train-17k-may2023/train-y/'\n",
    "\n",
    "def load_and_augment_xy(x, y, f):\n",
    "    x, y = load_individual_sample(x, y, f)\n",
    "    x, y = augment_single_sample(x, y)\n",
    "    return x, y\n",
    "\n",
    "percs = np.zeros((len(train_xs)))\n",
    "for i in range(len(train_xs)):\n",
    "    percs[i] = np.mean(rs.open(y_path + train_xs[i] + '.tif').read(1)) / 2.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343 272 221 275 189 181 172 390\n"
     ]
    }
   ],
   "source": [
    "zeros = np.argwhere(percs == 0).flatten()\n",
    "fives = np.argwhere(np.logical_and(percs > 0, percs <= 5)).flatten()\n",
    "tens = np.argwhere(np.logical_and(percs > 5, percs <= 10)).flatten()\n",
    "twenties = np.argwhere(np.logical_and(percs > 10, percs <= 20)).flatten()\n",
    "thirties = np.argwhere(np.logical_and(percs > 20, percs <= 35)).flatten()\n",
    "forties = np.argwhere(np.logical_and(percs > 35, percs <= 60)).flatten()\n",
    "fifties = np.argwhere(np.logical_and(percs > 60, percs <= 90)).flatten()\n",
    "seventies = np.argwhere(np.logical_and(percs > 90, percs <= 100)).flatten()\n",
    "print(len(zeros), len(fives), len(tens), len(twenties), len(thirties), len(forties), len(fifties), len(seventies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1, alpha: 0.3, beta: 0.0, drop: 0.995 Learning rate: 0.02\n",
      "3.3333333333333335e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/ipykernel_launcher.py:86: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141370751 0.11 0.0\n",
      "200058 0.07 0.02\n",
      "135787414 0.02 0.09\n",
      "135732397 0.09 0.17\n",
      "140860628 0.24 0.32\n",
      "136318424 0.54 0.57\n",
      "3117060010 0.57 0.69\n",
      "20048 0.95 1.0\n",
      "6.666666666666667e-05\n",
      "2200114 0.01 0.0\n",
      "139379537 0.02 0.01\n",
      "139379145 0.1 0.07\n",
      "139341808 0.08 0.12\n",
      "135702863 0.38 0.31\n",
      "138872341 0.46 0.41\n",
      "139027214 0.52 0.71\n",
      "135703926 0.94 1.0\n",
      "0.0001\n",
      "139291647 0.03 0.0\n",
      "139077430 0.02 0.03\n",
      "135804029 0.05 0.05\n",
      "135224895 0.1 0.16\n",
      "139207917 0.13 0.24\n",
      "135703898 0.51 0.38\n",
      "138872498 0.89 0.88\n",
      "135703851 0.88 0.96\n",
      "0.00013333333333333334\n",
      "141370751 0.11 0.0\n",
      "139379801 0.13 0.03\n",
      "138173606 0.04 0.08\n",
      "135702947 0.1 0.15\n",
      "135224843 0.22 0.31\n",
      "138872600 0.43 0.43\n",
      "5232050030 0.53 0.69\n",
      "135787639 0.91 0.9\n",
      "0.00016666666666666666\n",
      "139397648 0.01 0.0\n",
      "137517365 0.05 0.04\n",
      "139379145 0.1 0.07\n",
      "135804089 0.06 0.18\n",
      "139025822 0.19 0.23\n",
      "135703846 0.62 0.49\n",
      "137532551 0.62 0.87\n",
      "1234277 0.88 0.96\n",
      "0.0002\n",
      "139208166 0.05 0.0\n",
      "139379445 0.01 0.04\n",
      "135724967 0.09 0.08\n",
      "11527110010 0.11 0.19\n",
      "135505956 0.05 0.2\n",
      "135505842 0.47 0.51\n",
      "136089094 0.75 0.83\n",
      "139077886 0.95 1.0\n",
      "0.00023333333333333336\n",
      "139025801 0.01 0.0\n",
      "135732344 0.09 0.05\n",
      "135702958 0.05 0.1\n",
      "135681019 0.07 0.18\n",
      "138872634 0.23 0.34\n",
      "136318407 0.3 0.36\n",
      "139046361 0.38 0.62\n",
      "135732291 0.46 1.0\n",
      "0.0002666666666666667\n",
      "200072 0.02 0.0\n",
      "139048361 0.07 0.05\n",
      "135846836 0.1 0.05\n",
      "136434425 0.1 0.14\n",
      "135542470 0.13 0.28\n",
      "139077582 0.25 0.36\n",
      "3760050040 0.6 0.75\n",
      "8847060010 0.88 0.95\n",
      "0.0003\n",
      "8732110010 0.05 0.0\n",
      "139077430 0.02 0.03\n",
      "135840931 0.13 0.08\n",
      "139058217 0.08 0.1\n",
      "139291428 0.25 0.25\n",
      "13777050040 0.56 0.57\n",
      "138872495 0.55 0.66\n",
      "139146687 0.95 1.0\n",
      "0.0003333333333333333\n",
      "139291959 0.01 0.0\n",
      "200143 0.02 0.04\n",
      "135698200 0.1 0.09\n",
      "800112 0.14 0.1\n",
      "135345856 0.47 0.28\n",
      "135505842 0.47 0.51\n",
      "139077723 0.75 0.68\n",
      "9466050040 0.92 0.97\n",
      "0.00036666666666666667\n",
      "2000122 0.11 0.0\n",
      "12411080020 0.02 0.02\n",
      "135704022 0.04 0.06\n",
      "135703862 0.12 0.13\n",
      "136134474 0.33 0.29\n",
      "135841001 0.22 0.35\n",
      "6116110010 0.82 0.85\n",
      "137517297 0.89 0.94\n",
      "0.0004\n",
      "4210080010 0.01 0.0\n",
      "80011 0.04 0.04\n",
      "135702889 0.11 0.09\n",
      "139077739 0.42 0.15\n",
      "12145110030 0.13 0.33\n",
      "6350060040 0.72 0.55\n",
      "136411025 0.64 0.71\n",
      "9993060020 0.97 1.0\n",
      "0.00043333333333333337\n",
      "139027135 0.04 0.0\n",
      "135732406 0.05 0.02\n",
      "135724972 0.1 0.06\n",
      "139048295 0.11 0.12\n",
      "7031080010 0.45 0.33\n",
      "135680828 0.33 0.4\n",
      "1234141 0.48 0.89\n",
      "310034 0.84 1.0\n",
      "0.0004666666666666667\n",
      "24005 0.21 0.0\n",
      "139070801 0.07 0.02\n",
      "600119 0.07 0.08\n",
      "140750841 0.1 0.12\n",
      "136410947 0.25 0.3\n",
      "139320368 0.25 0.4\n",
      "136410972 0.77 0.82\n",
      "137517064 0.86 0.94\n",
      "0.0005\n",
      "139025658 0.01 0.0\n",
      "139064533 0.05 0.04\n",
      "139077916 0.03 0.05\n",
      "136410875 0.13 0.15\n",
      "135344058 0.4 0.33\n",
      "138901244 0.24 0.45\n",
      "136077525 0.61 0.83\n",
      "137587702 0.96 1.0\n",
      "0.0005333333333333334\n",
      "139319943 0.03 0.0\n",
      "600107 0.09 0.04\n",
      "139077858 0.28 0.08\n",
      "139025742 0.22 0.12\n",
      "135787385 0.23 0.3\n",
      "136318407 0.35 0.36\n",
      "136077525 0.57 0.83\n",
      "200241 0.9 1.0\n",
      "0.0005666666666666666\n",
      "250049 0.08 0.0\n",
      "138948677 0.05 0.05\n",
      "135344100 0.1 0.09\n",
      "139025798 0.13 0.19\n",
      "135654714 0.06 0.25\n",
      "3896060010 0.42 0.35\n",
      "138872620 0.52 0.6\n",
      "310011 0.41 1.0\n",
      "0.0006\n",
      "250073 0.02 0.0\n",
      "139379656 0.05 0.01\n",
      "141018703 0.03 0.05\n",
      "135807683 0.07 0.1\n",
      "139077588 0.2 0.21\n",
      "139077779 0.33 0.49\n",
      "136410944 0.55 0.65\n",
      "1234104 0.9 1.0\n",
      "0.0006333333333333334\n",
      "2500241 0.03 0.0\n",
      "2600221 0.02 0.01\n",
      "9353050030 0.06 0.06\n",
      "139025561 0.35 0.19\n",
      "135787283 0.14 0.22\n",
      "4531050030 0.52 0.38\n",
      "135703235 0.85 0.87\n",
      "2231060010 0.63 0.95\n",
      "0.0006666666666666666\n",
      "1234169 0.02 0.0\n",
      "139027276 0.06 0.01\n",
      "135703264 0.33 0.05\n",
      "139179612 0.14 0.12\n",
      "135703027 0.21 0.21\n",
      "135703823 0.34 0.41\n",
      "4736050030 0.73 0.81\n",
      "20048 0.96 1.0\n",
      "0.0007000000000000001\n",
      "135732300 0.05 0.0\n",
      "135191150 0.01 0.03\n",
      "136134530 0.18 0.06\n",
      "135809774 0.14 0.19\n",
      "800147 0.15 0.22\n",
      "135704101 0.48 0.41\n",
      "139146780 0.35 0.66\n",
      "1234161 0.84 1.0\n",
      "0.0007333333333333333\n",
      "135787353 0.16 0.0\n",
      "139025636 0.02 0.04\n",
      "13402110020 0.09 0.09\n",
      "135680214 0.24 0.11\n",
      "139025822 0.21 0.23\n",
      "4591060040 0.31 0.42\n",
      "700168 0.8 0.86\n",
      "290068 0.97 1.0\n",
      "0.0007666666666666666\n",
      "2600214 0.02 0.0\n",
      "600118 0.01 0.02\n",
      "136410663 0.11 0.08\n",
      "135224777 0.13 0.15\n",
      "135732320 0.2 0.23\n",
      "136077612 0.43 0.47\n",
      "240028 0.46 0.62\n",
      "290079 0.83 1.0\n",
      "0.0008\n",
      "8687050010 0.02 0.0\n",
      "139146664 0.02 0.01\n",
      "135787359 0.17 0.09\n",
      "139025821 0.42 0.13\n",
      "135787283 0.2 0.22\n",
      "137532546 0.67 0.56\n",
      "139052279 0.73 0.69\n",
      "139146808 0.73 1.0\n",
      "0.0008333333333333333\n",
      "200077 0.1 0.0\n",
      "139365814 0.04 0.02\n",
      "137532577 0.23 0.09\n",
      "135680980 0.33 0.19\n",
      "139160506 0.07 0.23\n",
      "139179727 0.36 0.45\n",
      "135703903 0.89 0.86\n",
      "139089899 0.97 1.0\n",
      "0.0008666666666666667\n",
      "2600206 0.01 0.0\n",
      "135787321 0.05 0.02\n",
      "139027234 0.16 0.1\n",
      "4438050040 0.16 0.13\n",
      "139160506 0.07 0.23\n",
      "136029695 0.38 0.37\n",
      "135703988 0.64 0.88\n",
      "7125110030 0.95 0.93\n",
      "0.0009\n",
      "48 0.06 0.0\n",
      "135787058 0.06 0.03\n",
      "139397780 0.14 0.07\n",
      "139397782 0.07 0.11\n",
      "135654714 0.07 0.25\n",
      "136410955 0.4 0.35\n",
      "136134609 0.46 0.8\n",
      "10361110040 0.95 0.99\n",
      "0.0009333333333333334\n",
      "139277652 0.01 0.0\n",
      "140750839 0.09 0.02\n",
      "139413342 0.2 0.09\n",
      "135787110 0.27 0.1\n",
      "139048875 0.04 0.23\n",
      "135751791 0.34 0.37\n",
      "33 0.85 0.85\n",
      "9860080011 0.89 0.98\n",
      "0.0009666666666666667\n",
      "200015 0.06 0.0\n",
      "135804096 0.04 0.05\n",
      "139077706 0.2 0.09\n",
      "141018076 0.15 0.1\n",
      "135702863 0.48 0.31\n",
      "136134466 0.66 0.37\n",
      "138872362 0.74 0.8\n",
      "6361080030 0.97 1.0\n",
      "0.001\n",
      "141370751 0.13 0.0\n",
      "800106 0.07 0.02\n",
      "7867110010 0.2 0.09\n",
      "137517095 0.23 0.11\n",
      "135809900 0.39 0.27\n",
      "139077779 0.35 0.49\n",
      "136752977 0.85 0.84\n",
      "8999110010 0.56 0.94\n",
      "0.0010333333333333334\n",
      "139277652 0.02 0.0\n",
      "139089875 0.05 0.01\n",
      "138173606 0.05 0.08\n",
      "139207989 0.17 0.12\n",
      "7031080010 0.49 0.33\n",
      "139077964 0.38 0.56\n",
      "141019171 0.75 0.84\n",
      "9229080041 0.96 0.99\n",
      "0.0010666666666666667\n",
      "135738194 0.01 0.0\n",
      "139146664 0.02 0.01\n",
      "139077858 0.33 0.08\n",
      "139025561 0.5 0.19\n",
      "139025816 0.16 0.24\n",
      "136410955 0.4 0.35\n",
      "5637060040 0.67 0.72\n",
      "7870110010 0.95 0.96\n",
      "0.0011\n",
      "2600249 0.01 0.0\n",
      "139319981 0.03 0.05\n",
      "135702369 0.15 0.09\n",
      "135732495 0.39 0.18\n",
      "139319944 0.15 0.23\n",
      "135702865 0.48 0.45\n",
      "135787643 0.69 0.83\n",
      "135787422 0.94 0.99\n",
      "0.0011333333333333332\n",
      "200079 0.1 0.0\n",
      "135724951 0.04 0.01\n",
      "138152160 0.09 0.09\n",
      "600208 0.25 0.12\n",
      "135787385 0.25 0.3\n",
      "136029590 0.6 0.53\n",
      "300021 0.87 0.88\n",
      "290071 0.98 1.0\n",
      "0.0011666666666666668\n",
      "139320425 0.08 0.0\n",
      "139178485 0.04 0.04\n",
      "139413229 0.48 0.09\n",
      "135803882 0.17 0.14\n",
      "139077536 0.2 0.29\n",
      "135751791 0.35 0.37\n",
      "135787653 0.88 0.83\n",
      "31006 0.81 1.0\n",
      "0.0012\n",
      "13589110020 0.02 0.0\n",
      "139365837 0.07 0.04\n",
      "139323498 0.06 0.09\n",
      "8769080020 0.51 0.19\n",
      "135344057 0.31 0.2\n",
      "135703915 0.51 0.54\n",
      "66 0.19 0.66\n",
      "8268050041 0.97 1.0\n",
      "0.0012333333333333335\n",
      "139207903 0.05 0.0\n",
      "139051750 0.1 0.03\n",
      "139179780 0.01 0.06\n",
      "14405060010 0.09 0.2\n",
      "137891146 0.32 0.28\n",
      "9807110010 0.39 0.4\n",
      "139052279 0.73 0.69\n",
      "9519050010 0.96 1.0\n",
      "0.0012666666666666668\n",
      "2000124 0.12 0.0\n",
      "135697619 0.1 0.01\n",
      "139025826 0.03 0.05\n",
      "8769080020 0.47 0.19\n",
      "400231 0.38 0.21\n",
      "135841001 0.25 0.35\n",
      "135703903 0.87 0.86\n",
      "135732368 0.85 1.0\n",
      "0.0013000000000000002\n",
      "139089926 0.03 0.0\n",
      "800196 0.03 0.01\n",
      "135732259 0.11 0.07\n",
      "135732285 0.13 0.14\n",
      "139077410 0.24 0.21\n",
      "139027211 0.38 0.43\n",
      "139179730 0.86 0.67\n",
      "8205080040 0.96 1.0\n",
      "0.0013333333333333333\n",
      "2600239 0.02 0.0\n",
      "135732264 0.06 0.03\n",
      "135505869 0.14 0.07\n",
      "135724939 0.19 0.18\n",
      "135787283 0.15 0.22\n",
      "139077811 0.54 0.57\n",
      "700168 0.82 0.86\n",
      "140789991 0.74 0.97\n",
      "0.0013666666666666666\n",
      "137535073 0.01 0.0\n",
      "139027175 0.07 0.04\n",
      "139161827 0.08 0.07\n",
      "13944060040 0.22 0.12\n",
      "7340110031 0.33 0.34\n",
      "135702861 0.45 0.54\n",
      "135703787 0.75 0.69\n",
      "137532629 0.97 1.0\n",
      "0.0014000000000000002\n",
      "200161 0.03 0.0\n",
      "139292058 0.1 0.05\n",
      "138872774 0.06 0.06\n",
      "14405060010 0.07 0.2\n",
      "139379913 0.17 0.29\n",
      "135546362 0.63 0.39\n",
      "800199 0.84 0.84\n",
      "310010 0.07 1.0\n",
      "0.0014333333333333333\n",
      "200236 0.01 0.0\n",
      "136446310 0.16 0.04\n",
      "135732356 0.19 0.06\n",
      "139025749 0.2 0.13\n",
      "135732348 0.15 0.2\n",
      "10828060020 0.62 0.4\n",
      "800161 0.75 0.66\n",
      "290049 0.97 1.0\n",
      "0.0014666666666666667\n",
      "139025625 0.02 0.0\n",
      "800189 0.03 0.02\n",
      "139077706 0.18 0.09\n",
      "139025742 0.19 0.12\n",
      "139320449 0.15 0.21\n",
      "136134601 0.6 0.54\n",
      "135703864 0.71 0.69\n",
      "8455050040 0.95 0.93\n",
      "0.0015\n",
      "2000126 0.12 0.0\n",
      "135732261 0.07 0.04\n",
      "137862276 0.07 0.06\n",
      "700193 0.19 0.2\n",
      "135703845 0.26 0.28\n",
      "135703915 0.49 0.54\n",
      "136411025 0.8 0.71\n",
      "8363060030 0.96 0.97\n",
      "0.0015333333333333332\n",
      "2600211 0.04 0.0\n",
      "800189 0.03 0.02\n",
      "135803812 0.07 0.09\n",
      "8976080010 0.17 0.17\n",
      "138152120 0.22 0.35\n",
      "138872681 0.34 0.38\n",
      "5232050030 0.57 0.69\n",
      "1234276 0.4 0.9\n",
      "0.0015666666666666667\n",
      "136410827 0.1 0.0\n",
      "11046110030 0.04 0.02\n",
      "138004142 0.17 0.09\n",
      "280099 0.09 0.16\n",
      "137966738 0.26 0.24\n",
      "136318424 0.58 0.57\n",
      "139379934 0.65 0.7\n",
      "400143 0.97 1.0\n",
      "0.0016\n",
      "135654723 0.01 0.0\n",
      "136752985 0.01 0.02\n",
      "2000160 0.03 0.06\n",
      "135732371 0.16 0.14\n",
      "135191374 0.26 0.31\n",
      "135703898 0.59 0.38\n",
      "200189 0.88 0.85\n",
      "310017 0.75 1.0\n",
      "0.0016333333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24005 0.23 0.0\n",
      "135787448 0.06 0.04\n",
      "31005 0.13 0.07\n",
      "135224686 0.16 0.18\n",
      "139077754 0.45 0.28\n",
      "9636050010 0.6 0.39\n",
      "135703328 0.54 0.65\n",
      "9017080020 0.96 0.99\n",
      "0.0016666666666666666\n",
      "200072 0.02 0.0\n",
      "135704174 0.26 0.03\n",
      "139379145 0.11 0.07\n",
      "137862315 0.15 0.17\n",
      "135703890 0.4 0.34\n",
      "139365986 0.56 0.41\n",
      "200159 0.71 0.65\n",
      "8691060010 0.95 1.0\n",
      "0.0017000000000000001\n",
      "135787400 0.01 0.0\n",
      "135732396 0.1 0.03\n",
      "7867110010 0.21 0.09\n",
      "135840880 0.11 0.1\n",
      "139365859 0.22 0.23\n",
      "10288060040 0.51 0.5\n",
      "137532551 0.67 0.87\n",
      "9835060010 0.96 0.99\n",
      "0.0017333333333333335\n",
      "2500232 0.03 0.0\n",
      "2600248 0.03 0.02\n",
      "139323565 0.06 0.06\n",
      "139160350 0.21 0.12\n",
      "20089 0.4 0.22\n",
      "5101050010 0.5 0.4\n",
      "240028 0.4 0.62\n",
      "135680051 0.96 1.0\n",
      "0.0017666666666666666\n",
      "135672785 0.03 0.0\n",
      "137535092 0.02 0.02\n",
      "135654602 0.04 0.08\n",
      "139420305 0.25 0.13\n",
      "140786039 0.51 0.28\n",
      "135704062 0.38 0.49\n",
      "80052 0.78 0.88\n",
      "138872365 0.96 1.0\n",
      "0.0018\n",
      "2100178 0.02 0.0\n",
      "135703106 0.1 0.04\n",
      "138173606 0.04 0.08\n",
      "13826110040 0.23 0.13\n",
      "135191161 0.35 0.33\n",
      "135787440 0.68 0.51\n",
      "139686336 0.78 0.81\n",
      "2231060010 0.69 0.95\n",
      "0.0018333333333333333\n",
      "135787407 0.03 0.0\n",
      "137862312 0.05 0.05\n",
      "139397780 0.05 0.07\n",
      "135787423 0.36 0.17\n",
      "138872597 0.2 0.33\n",
      "135787295 0.24 0.47\n",
      "135787344 0.68 0.78\n",
      "80014 0.93 1.0\n",
      "0.0018666666666666669\n",
      "2100191 0.01 0.0\n",
      "135804096 0.03 0.05\n",
      "139027235 0.05 0.06\n",
      "135732327 0.22 0.15\n",
      "135702863 0.34 0.31\n",
      "138901244 0.24 0.45\n",
      "136077525 0.59 0.83\n",
      "600234 0.96 1.0\n",
      "0.0019\n",
      "12973080040 0.01 0.0\n",
      "139027219 0.05 0.04\n",
      "135787074 0.07 0.05\n",
      "136134573 0.29 0.2\n",
      "2175080020 0.46 0.25\n",
      "137587781 0.59 0.57\n",
      "13826060040 0.55 0.74\n",
      "300051 0.95 1.0\n",
      "0.0019333333333333333\n",
      "24005 0.17 0.0\n",
      "600113 0.04 0.01\n",
      "136134530 0.18 0.06\n",
      "139027213 0.13 0.17\n",
      "137517110 0.37 0.29\n",
      "139178080 0.62 0.57\n",
      "1234194 0.6 0.76\n",
      "139077871 0.96 1.0\n",
      "0.0019666666666666665\n",
      "135546184 0.04 0.0\n",
      "139077434 0.01 0.01\n",
      "139027235 0.04 0.06\n",
      "135703142 0.07 0.11\n",
      "139025791 0.19 0.26\n",
      "136318443 0.51 0.56\n",
      "135703811 0.87 0.87\n",
      "137517091 0.92 0.99\n",
      "0.002\n",
      "270022 0.02 0.0\n",
      "138872425 0.02 0.02\n",
      "135724919 0.1 0.1\n",
      "137532507 0.07 0.14\n",
      "139379913 0.17 0.29\n",
      "135505843 0.48 0.44\n",
      "139365764 0.58 0.66\n",
      "138872339 0.96 1.0\n",
      "0.0020333333333333336\n",
      "2000162 0.04 0.0\n",
      "11048050031 0.04 0.01\n",
      "135191289 0.14 0.07\n",
      "139025821 0.46 0.13\n",
      "135191161 0.4 0.33\n",
      "135703819 0.56 0.47\n",
      "200148 0.42 0.65\n",
      "8722050010 0.96 1.0\n",
      "0.0020666666666666667\n",
      "2600226 0.06 0.0\n",
      "2000130 0.14 0.04\n",
      "136456616 0.14 0.08\n",
      "135787430 0.24 0.13\n",
      "12019060030 0.63 0.26\n",
      "135703885 0.31 0.42\n",
      "1234188 0.73 0.86\n",
      "6804060010 0.96 0.99\n",
      "0.0021\n",
      "2000174 0.21 0.0\n",
      "135724951 0.03 0.01\n",
      "139077444 0.04 0.06\n",
      "80085 0.07 0.11\n",
      "8610050010 0.25 0.25\n",
      "136134571 0.43 0.41\n",
      "135703864 0.68 0.69\n",
      "290074 0.95 1.0\n",
      "0.0021333333333333334\n",
      "139146789 0.01 0.0\n",
      "135787325 0.02 0.05\n",
      "135505869 0.12 0.07\n",
      "135224777 0.09 0.15\n",
      "135724983 0.16 0.23\n",
      "136029590 0.63 0.53\n",
      "139146780 0.27 0.66\n",
      "139146809 0.85 0.93\n",
      "0.002166666666666667\n",
      "2000113 0.02 0.0\n",
      "139160528 0.03 0.04\n",
      "139379521 0.09 0.06\n",
      "135505920 0.08 0.12\n",
      "138872338 0.25 0.26\n",
      "135703898 0.54 0.38\n",
      "14252110010 0.6 0.92\n",
      "10395110041 0.95 0.98\n",
      "0.0022\n",
      "2000165 0.03 0.0\n",
      "2600229 0.04 0.01\n",
      "139397780 0.09 0.07\n",
      "139089900 0.13 0.16\n",
      "139048937 0.1 0.21\n",
      "9974080020 0.38 0.36\n",
      "33 0.79 0.85\n",
      "137891119 0.95 1.0\n",
      "0.0022333333333333333\n",
      "200087 0.03 0.0\n",
      "200066 0.04 0.01\n",
      "75 0.04 0.06\n",
      "136075836 0.23 0.17\n",
      "135787358 0.13 0.31\n",
      "138948353 0.34 0.37\n",
      "137532551 0.66 0.87\n",
      "310033 0.89 1.0\n",
      "0.0022666666666666664\n",
      "20036 0.02 0.0\n",
      "2000130 0.14 0.04\n",
      "139160447 0.05 0.06\n",
      "139413145 0.17 0.1\n",
      "135787875 0.09 0.23\n",
      "31 0.34 0.56\n",
      "80090 0.78 0.77\n",
      "300054 0.98 1.0\n",
      "0.0023\n",
      "135345998 0.01 0.0\n",
      "135732396 0.13 0.03\n",
      "2000160 0.02 0.06\n",
      "135732340 0.12 0.15\n",
      "139190066 0.26 0.33\n",
      "135680828 0.37 0.4\n",
      "135787613 0.58 0.84\n",
      "14542110010 0.97 1.0\n",
      "0.0023333333333333335\n",
      "139397646 0.02 0.0\n",
      "2000136 0.02 0.02\n",
      "136089146 0.13 0.06\n",
      "136410822 0.14 0.14\n",
      "135344121 0.27 0.25\n",
      "139669334 0.27 0.45\n",
      "5637060040 0.68 0.72\n",
      "137891124 0.97 1.0\n",
      "0.0023666666666666667\n",
      "2000170 0.06 0.0\n",
      "11048050031 0.04 0.01\n",
      "139025795 0.09 0.08\n",
      "136075854 0.19 0.19\n",
      "135724983 0.3 0.23\n",
      "137517081 0.49 0.41\n",
      "6116110010 0.81 0.85\n",
      "141018529 0.72 0.97\n",
      "0.0024\n",
      "600195 0.02 0.0\n",
      "139379242 0.05 0.05\n",
      "12521110020 0.25 0.07\n",
      "300146 0.16 0.19\n",
      "2175080020 0.49 0.25\n",
      "5263080030 0.28 0.4\n",
      "136410944 0.73 0.65\n",
      "300126 0.97 1.0\n",
      "0.0024333333333333334\n",
      "2600242 0.02 0.0\n",
      "139365814 0.03 0.02\n",
      "135344146 0.12 0.06\n",
      "135703980 0.13 0.13\n",
      "135697523 0.4 0.23\n",
      "3394110030 0.28 0.41\n",
      "139046361 0.39 0.62\n",
      "200145 0.96 1.0\n",
      "0.002466666666666667\n",
      "9045110040 0.01 0.0\n",
      "139379656 0.03 0.01\n",
      "139291487 0.07 0.05\n",
      "135788009 0.08 0.12\n",
      "135680067 0.23 0.32\n",
      "135654684 0.24 0.36\n",
      "80016 0.8 0.81\n",
      "139319918 0.85 1.0\n",
      "0.0025\n",
      "2600222 0.02 0.0\n",
      "5220060030 0.05 0.05\n",
      "2000105 0.1 0.09\n",
      "135732495 0.38 0.18\n",
      "12953080030 0.24 0.3\n",
      "139178453 0.58 0.48\n",
      "6832110020 0.57 0.88\n",
      "290057 0.97 1.0\n",
      "0.0025333333333333336\n",
      "20009 0.05 0.0\n",
      "139077923 0.06 0.02\n",
      "135680180 0.07 0.08\n",
      "135680771 0.06 0.11\n",
      "140786039 0.51 0.28\n",
      "31 0.44 0.56\n",
      "4736050030 0.8 0.81\n",
      "6904060020 0.95 1.0\n",
      "0.0025666666666666667\n",
      "2000132 0.03 0.0\n",
      "600100 0.03 0.02\n",
      "139413342 0.1 0.09\n",
      "139187481 0.21 0.11\n",
      "135542470 0.15 0.28\n",
      "136029622 0.48 0.43\n",
      "135787654 0.56 0.67\n",
      "49 0.96 1.0\n",
      "0.0026000000000000003\n",
      "139146798 0.01 0.0\n",
      "135732264 0.06 0.03\n",
      "135703886 0.09 0.06\n",
      "139162003 0.17 0.15\n",
      "13746050040 0.07 0.21\n",
      "139046457 0.28 0.37\n",
      "8008 0.83 0.77\n",
      "137532557 0.96 1.0\n",
      "0.002633333333333333\n",
      "4210080010 0.01 0.0\n",
      "135191150 0.01 0.03\n",
      "2808110030 0.08 0.09\n",
      "9190110041 0.07 0.11\n",
      "137547568 0.22 0.31\n",
      "140474083 0.4 0.43\n",
      "138872495 0.62 0.66\n",
      "135704479 0.87 0.98\n",
      "0.0026666666666666666\n",
      "200020 0.01 0.0\n",
      "137532541 0.04 0.03\n",
      "139027273 0.03 0.09\n",
      "135703486 0.05 0.11\n",
      "135702863 0.32 0.31\n",
      "136134571 0.45 0.41\n",
      "139077788 0.66 0.68\n",
      "139077951 0.96 1.0\n",
      "0.0027\n",
      "135703322 0.02 0.0\n",
      "139027212 0.03 0.05\n",
      "135724926 0.08 0.05\n",
      "135703989 0.21 0.17\n",
      "135191374 0.23 0.31\n",
      "6892110020 0.64 0.36\n",
      "135787344 0.7 0.78\n",
      "200237 0.96 1.0\n",
      "0.0027333333333333333\n",
      "139191624 0.03 0.0\n",
      "600100 0.04 0.02\n",
      "139341797 0.1 0.1\n",
      "135787294 0.13 0.13\n",
      "138872597 0.18 0.33\n",
      "136077612 0.46 0.47\n",
      "14849060040 0.64 0.62\n",
      "139077671 0.62 0.9\n",
      "0.002766666666666667\n",
      "10147080040 0.03 0.0\n",
      "135732346 0.22 0.05\n",
      "135702930 0.05 0.1\n",
      "135703091 0.05 0.16\n",
      "135224677 0.19 0.32\n",
      "139277311 0.41 0.35\n",
      "137891978 0.63 0.71\n",
      "139201884 0.97 1.0\n",
      "0.0028000000000000004\n",
      "135546184 0.02 0.0\n",
      "2000115 0.08 0.01\n",
      "135703911 0.47 0.05\n",
      "139077833 0.03 0.16\n",
      "139365788 0.16 0.24\n",
      "12004060030 0.64 0.59\n",
      "7657050010 0.62 0.62\n",
      "31004 0.62 1.0\n",
      "0.002833333333333333\n",
      "2500224 0.02 0.0\n",
      "139420304 0.16 0.01\n",
      "135680754 0.33 0.07\n",
      "139378743 0.63 0.18\n",
      "135724983 0.14 0.23\n",
      "135804072 0.62 0.58\n",
      "138872718 0.37 0.62\n",
      "1234279 0.94 0.91\n",
      "0.0028666666666666667\n",
      "2500233 0.03 0.0\n",
      "135724951 0.03 0.01\n",
      "139077543 0.06 0.05\n",
      "700193 0.2 0.2\n",
      "136029732 0.12 0.26\n",
      "139077582 0.31 0.36\n",
      "12689060010 0.78 0.79\n",
      "1234175 0.77 1.0\n",
      "0.0029\n",
      "135732300 0.03 0.0\n",
      "135732418 0.06 0.04\n",
      "135224897 0.44 0.09\n",
      "138173604 0.06 0.13\n",
      "135804308 0.33 0.31\n",
      "139378507 0.31 0.45\n",
      "1234190 0.43 0.71\n",
      "135703907 0.96 0.99\n",
      "0.0029333333333333334\n",
      "2000150 0.04 0.0\n",
      "135787360 0.04 0.02\n",
      "138173606 0.03 0.08\n",
      "8969050010 0.15 0.2\n",
      "135680963 0.13 0.2\n",
      "137517143 0.37 0.44\n",
      "135704286 0.73 0.65\n",
      "137517117 0.91 1.0\n",
      "0.002966666666666667\n",
      "135732439 0.05 0.0\n",
      "135724951 0.02 0.01\n",
      "139025803 0.04 0.05\n",
      "138173604 0.05 0.13\n",
      "135787283 0.14 0.22\n",
      "400247 0.27 0.36\n",
      "135703863 0.77 0.64\n",
      "600176 0.95 1.0\n",
      "0.003\n",
      "139070805 0.02 0.0\n",
      "135804032 0.04 0.05\n",
      "135787162 0.09 0.1\n",
      "137532568 0.16 0.19\n",
      "9744110021 0.44 0.28\n",
      "137517143 0.32 0.44\n",
      "12069060030 0.9 0.73\n",
      "139058213 0.93 1.0\n",
      "0.0030333333333333336\n",
      "139208284 0.01 0.0\n",
      "13857050040 0.05 0.01\n",
      "12609060020 0.04 0.06\n",
      "400249 0.05 0.14\n",
      "135542615 0.23 0.26\n",
      "136077612 0.45 0.47\n",
      "10679060020 0.74 0.72\n",
      "1234179 0.96 1.0\n",
      "0.0030666666666666663\n",
      "2000139 0.04 0.0\n",
      "200089 0.04 0.01\n",
      "2000160 0.03 0.06\n",
      "139025510 0.23 0.11\n",
      "135787875 0.13 0.23\n",
      "138900677 0.51 0.43\n",
      "135703235 0.85 0.87\n",
      "136410930 0.96 0.99\n",
      "0.0031\n",
      "10147080040 0.03 0.0\n",
      "139046352 0.02 0.03\n",
      "139048878 0.18 0.06\n",
      "139291416 0.38 0.2\n",
      "135725005 0.16 0.29\n",
      "139179709 0.44 0.37\n",
      "3342060020 0.76 0.74\n",
      "1234017 0.98 1.0\n",
      "0.0031333333333333335\n",
      "2600215 0.02 0.0\n",
      "27007 0.05 0.04\n",
      "139027253 0.1 0.07\n",
      "139160350 0.2 0.12\n",
      "139420426 0.48 0.27\n",
      "139178384 0.44 0.52\n",
      "135703990 0.61 0.78\n",
      "138872342 0.97 1.0\n",
      "0.0031666666666666666\n",
      "2000167 0.01 0.0\n",
      "135738185 0.16 0.03\n",
      "139077462 0.23 0.06\n",
      "2022080010 0.25 0.19\n",
      "139025760 0.21 0.22\n",
      "80099 0.44 0.57\n",
      "5232050030 0.57 0.69\n",
      "600175 0.96 1.0\n",
      "0.0032\n",
      "139077894 0.02 0.0\n",
      "135787438 0.04 0.05\n",
      "135724967 0.07 0.08\n",
      "135732458 0.1 0.12\n",
      "139077906 0.6 0.29\n",
      "135505843 0.46 0.44\n",
      "1234159 0.36 0.86\n",
      "31006 0.83 1.0\n",
      "0.0032333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139025733 0.02 0.0\n",
      "135787272 0.03 0.04\n",
      "139323565 0.06 0.06\n",
      "800155 0.05 0.11\n",
      "135803878 0.47 0.23\n",
      "135751791 0.31 0.37\n",
      "80016 0.8 0.81\n",
      "2178080010 0.32 0.97\n",
      "0.003266666666666667\n",
      "139291657 0.07 0.0\n",
      "27007 0.04 0.04\n",
      "8725110010 0.03 0.06\n",
      "40073 0.19 0.16\n",
      "135344159 0.16 0.2\n",
      "137517062 0.58 0.58\n",
      "8008 0.82 0.77\n",
      "138872365 0.97 1.0\n",
      "0.0033000000000000004\n",
      "139077432 0.01 0.0\n",
      "135841016 0.03 0.03\n",
      "139025529 0.23 0.1\n",
      "135680980 0.31 0.19\n",
      "139048986 0.19 0.32\n",
      "139669334 0.3 0.45\n",
      "300021 0.92 0.88\n",
      "1234274 0.89 0.93\n",
      "0.003333333333333333\n",
      "139419876 0.01 0.0\n",
      "135787438 0.04 0.05\n",
      "139379571 0.16 0.07\n",
      "135702611 0.07 0.1\n",
      "139207917 0.14 0.24\n",
      "136029544 0.26 0.37\n",
      "9394050040 0.71 0.91\n",
      "5530050040 0.97 1.0\n",
      "0.0033666666666666667\n",
      "900182 0.03 0.0\n",
      "135732487 0.04 0.02\n",
      "135672824 0.09 0.09\n",
      "135681029 0.24 0.15\n",
      "137517146 0.21 0.2\n",
      "137527070 0.51 0.46\n",
      "3117060010 0.72 0.69\n",
      "139186714 0.96 1.0\n",
      "0.0034000000000000002\n",
      "2500241 0.02 0.0\n",
      "11976050030 0.03 0.02\n",
      "139413342 0.12 0.09\n",
      "2100165 0.18 0.15\n",
      "135703839 0.2 0.22\n",
      "135703898 0.57 0.38\n",
      "80090 0.81 0.77\n",
      "3790110040 0.97 1.0\n",
      "0.0034333333333333334\n",
      "139189005 0.08 0.0\n",
      "135191349 0.06 0.01\n",
      "135804029 0.05 0.05\n",
      "136075836 0.25 0.17\n",
      "140860628 0.27 0.32\n",
      "135787036 0.67 0.58\n",
      "135787613 0.58 0.84\n",
      "20048 0.97 1.0\n",
      "0.003466666666666667\n",
      "136410827 0.07 0.0\n",
      "135697619 0.05 0.01\n",
      "135732356 0.17 0.06\n",
      "2100165 0.2 0.15\n",
      "139025769 0.19 0.24\n",
      "135787036 0.68 0.58\n",
      "141019171 0.79 0.84\n",
      "8847060010 0.91 0.95\n",
      "0.0034999999999999996\n",
      "2000127 0.09 0.0\n",
      "139064533 0.03 0.04\n",
      "139025826 0.02 0.05\n",
      "135732364 0.25 0.18\n",
      "13746050040 0.06 0.21\n",
      "138900783 0.48 0.42\n",
      "12149050030 0.35 0.79\n",
      "12302080040 0.97 0.95\n",
      "0.003533333333333333\n",
      "139208284 0.01 0.0\n",
      "135732288 0.03 0.03\n",
      "135732488 0.13 0.08\n",
      "140860597 0.04 0.16\n",
      "137517146 0.2 0.2\n",
      "139186879 0.37 0.41\n",
      "139077788 0.66 0.68\n",
      "14127060040 0.98 1.0\n",
      "0.0035666666666666668\n",
      "250013 0.03 0.0\n",
      "9049110030 0.02 0.04\n",
      "135724972 0.11 0.06\n",
      "600102 0.31 0.15\n",
      "135344121 0.27 0.25\n",
      "135703819 0.56 0.47\n",
      "135724910 0.72 0.7\n",
      "1234185 0.94 1.0\n",
      "0.0036\n",
      "200120 0.02 0.0\n",
      "139185734 0.01 0.01\n",
      "138947960 0.05 0.06\n",
      "136134535 0.19 0.2\n",
      "137547568 0.25 0.31\n",
      "135702861 0.49 0.54\n",
      "136089149 0.72 0.71\n",
      "139160388 0.93 1.0\n",
      "0.0036333333333333335\n",
      "10147080040 0.03 0.0\n",
      "139319948 0.05 0.03\n",
      "137532577 0.15 0.09\n",
      "600191 0.13 0.17\n",
      "136134474 0.29 0.29\n",
      "139179727 0.33 0.45\n",
      "3855060010 0.6 0.68\n",
      "1234276 0.39 0.9\n",
      "0.0036666666666666666\n",
      "139089891 0.01 0.0\n",
      "9213110040 0.06 0.03\n",
      "135787307 0.05 0.07\n",
      "139077441 0.19 0.18\n",
      "135787269 0.26 0.31\n",
      "135724952 0.67 0.59\n",
      "11200060030 0.41 0.73\n",
      "6537060040 0.95 0.95\n",
      "0.0037\n",
      "135345904 0.01 0.0\n",
      "139146772 0.02 0.04\n",
      "139319278 0.1 0.05\n",
      "135703142 0.07 0.11\n",
      "135697523 0.2 0.23\n",
      "136029590 0.62 0.53\n",
      "6346050031 0.74 0.66\n",
      "5480050030 0.96 0.98\n",
      "0.0037333333333333337\n",
      "14696060040 0.01 0.0\n",
      "135787291 0.03 0.01\n",
      "136446390 0.14 0.05\n",
      "135680829 0.29 0.18\n",
      "7031080010 0.48 0.33\n",
      "138948353 0.36 0.37\n",
      "139077808 0.79 0.8\n",
      "135703991 0.96 1.0\n",
      "0.0037666666666666664\n",
      "200010 0.07 0.0\n",
      "139048883 0.07 0.04\n",
      "135787376 0.04 0.08\n",
      "140474070 0.1 0.11\n",
      "138152144 0.18 0.21\n",
      "136089240 0.45 0.54\n",
      "12485080010 0.74 0.66\n",
      "135703957 0.87 0.95\n",
      "0.0038\n",
      "27005 0.01 0.0\n",
      "135787345 0.13 0.04\n",
      "139291481 0.09 0.08\n",
      "139027266 0.12 0.16\n",
      "9530110010 0.04 0.27\n",
      "139291389 0.22 0.37\n",
      "1234141 0.48 0.89\n",
      "137517219 0.97 1.0\n",
      "0.0038333333333333336\n",
      "2000165 0.03 0.0\n",
      "139320343 0.04 0.01\n",
      "139027159 0.03 0.08\n",
      "135345953 0.13 0.16\n",
      "135344121 0.26 0.25\n",
      "135703899 0.39 0.47\n",
      "7657050010 0.67 0.62\n",
      "139077780 0.92 0.95\n",
      "0.0038666666666666667\n",
      "139189005 0.09 0.0\n",
      "2600220 0.01 0.03\n",
      "139025755 0.05 0.08\n",
      "135809884 0.24 0.16\n",
      "135680963 0.1 0.2\n",
      "8393080040 0.45 0.4\n",
      "800161 0.75 0.66\n",
      "4953050010 0.97 0.99\n",
      "0.0039000000000000003\n",
      "139077853 0.08 0.0\n",
      "60099 0.03 0.02\n",
      "136410663 0.07 0.08\n",
      "5002110020 0.06 0.14\n",
      "139027893 0.35 0.33\n",
      "138900837 0.45 0.47\n",
      "139046361 0.42 0.62\n",
      "12701050010 0.96 1.0\n",
      "0.003933333333333333\n",
      "139397662 0.01 0.0\n",
      "139070801 0.05 0.02\n",
      "135680937 0.07 0.05\n",
      "140474070 0.11 0.11\n",
      "135724983 0.1 0.23\n",
      "12004060030 0.66 0.59\n",
      "139077509 0.87 0.86\n",
      "140789991 0.81 0.97\n",
      "0.003966666666666667\n",
      "135703481 0.02 0.0\n",
      "139025748 0.05 0.02\n",
      "139419924 0.06 0.07\n",
      "6350050030 0.35 0.12\n",
      "138872634 0.3 0.34\n",
      "139186879 0.38 0.41\n",
      "13826060040 0.57 0.74\n",
      "1234182 0.95 1.0\n",
      "0.004\n",
      "2100123 0.03 0.0\n",
      "135787058 0.04 0.03\n",
      "139027259 0.06 0.09\n",
      "13289060040 0.36 0.15\n",
      "135344058 0.4 0.33\n",
      "135787141 0.31 0.36\n",
      "3805050040 0.93 0.9\n",
      "47 0.93 1.0\n",
      "0.004033333333333333\n",
      "800170 0.05 0.0\n",
      "200089 0.05 0.01\n",
      "139379521 0.11 0.06\n",
      "4155110010 0.16 0.15\n",
      "135732419 0.21 0.24\n",
      "6300110040 0.54 0.54\n",
      "139379934 0.66 0.7\n",
      "139186669 0.97 1.0\n",
      "0.004066666666666667\n",
      "2600227 0.05 0.0\n",
      "139420304 0.1 0.01\n",
      "139189017 0.18 0.09\n",
      "13826110040 0.21 0.13\n",
      "139077754 0.47 0.28\n",
      "135702865 0.39 0.45\n",
      "1234190 0.5 0.71\n",
      "1234172 0.58 0.96\n",
      "0.0040999999999999995\n",
      "2600215 0.03 0.0\n",
      "139027903 0.04 0.03\n",
      "135804029 0.06 0.05\n",
      "137532568 0.17 0.19\n",
      "135732329 0.16 0.21\n",
      "6350060040 0.78 0.55\n",
      "139179723 0.6 0.66\n",
      "310024 0.97 1.0\n",
      "0.0041333333333333335\n",
      "2000158 0.01 0.0\n",
      "139146771 0.03 0.01\n",
      "139161946 0.12 0.09\n",
      "135787431 0.06 0.19\n",
      "135344057 0.24 0.2\n",
      "135703846 0.62 0.49\n",
      "139686336 0.85 0.81\n",
      "80014 0.95 1.0\n",
      "0.004166666666666667\n",
      "600131 0.01 0.0\n",
      "139051747 0.03 0.03\n",
      "137517276 0.08 0.08\n",
      "139077739 0.47 0.15\n",
      "139379913 0.17 0.29\n",
      "135703794 0.51 0.44\n",
      "139146780 0.38 0.66\n",
      "30017 0.96 1.0\n",
      "0.0042\n",
      "270012 0.03 0.0\n",
      "900168 0.08 0.04\n",
      "139025759 0.21 0.06\n",
      "135703980 0.11 0.13\n",
      "10205050030 0.13 0.27\n",
      "8797060010 0.34 0.42\n",
      "136089094 0.81 0.83\n",
      "14740060030 0.68 0.92\n",
      "0.004233333333333334\n",
      "5373050040 0.03 0.0\n",
      "139025825 0.01 0.01\n",
      "139420264 0.16 0.1\n",
      "9092050030 0.07 0.1\n",
      "137587859 0.21 0.27\n",
      "139669334 0.37 0.45\n",
      "135703853 0.76 0.84\n",
      "8569060010 0.63 0.95\n",
      "0.004266666666666667\n",
      "2000139 0.04 0.0\n",
      "135732261 0.04 0.04\n",
      "135787406 0.14 0.08\n",
      "139046276 0.13 0.18\n",
      "2175080020 0.51 0.25\n",
      "135703870 0.59 0.53\n",
      "290059 0.95 0.85\n",
      "300060 0.98 1.0\n",
      "0.0043\n",
      "14313080010 0.07 0.0\n",
      "900168 0.08 0.04\n",
      "135787414 0.03 0.09\n",
      "11527110010 0.13 0.19\n",
      "139025769 0.24 0.24\n",
      "3896060010 0.46 0.35\n",
      "7509060040 0.71 0.74\n",
      "9860080011 0.94 0.98\n",
      "0.004333333333333334\n",
      "600104 0.07 0.0\n",
      "135787058 0.04 0.03\n",
      "137532577 0.17 0.09\n",
      "139027266 0.14 0.16\n",
      "200243 0.15 0.23\n",
      "136029695 0.34 0.37\n",
      "7387110040 0.54 0.73\n",
      "3189080020 0.97 1.0\n",
      "0.004366666666666666\n",
      "2000125 0.1 0.0\n",
      "135804096 0.03 0.05\n",
      "800165 0.03 0.06\n",
      "135787423 0.3 0.17\n",
      "139365594 0.18 0.26\n",
      "11526050020 0.34 0.48\n",
      "4637080040 0.51 0.69\n",
      "139201884 0.97 1.0\n",
      "0.0044\n",
      "141370751 0.1 0.0\n",
      "139077769 0.03 0.03\n",
      "135787447 0.02 0.07\n",
      "136134573 0.27 0.2\n",
      "13421110010 0.38 0.22\n",
      "139179709 0.45 0.37\n",
      "135703903 0.89 0.86\n",
      "300060 0.98 1.0\n",
      "0.004433333333333333\n",
      "280097 0.02 0.0\n",
      "139077434 0.01 0.01\n",
      "135787376 0.04 0.08\n",
      "135840971 0.18 0.17\n",
      "138948044 0.23 0.33\n",
      "135780069 0.37 0.42\n",
      "136077525 0.6 0.83\n",
      "1234187 0.96 1.0\n",
      "0.0044666666666666665\n",
      "139027135 0.05 0.0\n",
      "2000136 0.02 0.02\n",
      "135505838 0.03 0.06\n",
      "135732451 0.18 0.15\n",
      "139048986 0.24 0.32\n",
      "9852050020 0.29 0.41\n",
      "14252110010 0.7 0.92\n",
      "6102080010 0.97 0.99\n",
      "0.0045000000000000005\n",
      "24005 0.26 0.0\n",
      "139027169 0.04 0.04\n",
      "135224687 0.02 0.07\n",
      "2022080010 0.25 0.19\n",
      "140786039 0.53 0.28\n",
      "138152110 0.39 0.43\n",
      "400225 0.71 0.8\n",
      "3340060010 0.97 1.0\n",
      "0.004533333333333333\n",
      "135724897 0.09 0.0\n",
      "139025825 0.01 0.01\n",
      "7577110010 0.04 0.08\n",
      "139160342 0.07 0.14\n",
      "139379486 0.6 0.3\n",
      "9807110010 0.38 0.4\n",
      "135704015 0.89 0.89\n",
      "310011 0.56 1.0\n",
      "0.004566666666666667\n",
      "139077431 0.01 0.0\n",
      "139146771 0.03 0.01\n",
      "139319278 0.09 0.05\n",
      "135345955 0.11 0.14\n",
      "135344121 0.28 0.25\n",
      "135841001 0.26 0.35\n",
      "139178449 0.84 0.67\n",
      "139186718 0.97 1.0\n",
      "0.0046\n",
      "2600211 0.02 0.0\n",
      "2000136 0.02 0.02\n",
      "135344027 0.04 0.08\n",
      "139179656 0.09 0.19\n",
      "135344196 0.4 0.22\n",
      "136752983 0.44 0.43\n",
      "1234176 0.81 0.86\n",
      "1234179 0.96 1.0\n",
      "0.004633333333333333\n",
      "2500224 0.01 0.0\n",
      "135787303 0.03 0.01\n",
      "135702889 0.1 0.09\n",
      "139027177 0.1 0.12\n",
      "13746050040 0.08 0.21\n",
      "11526050020 0.41 0.48\n",
      "135704422 0.6 0.8\n",
      "139077787 0.93 1.0\n",
      "0.004666666666666667\n",
      "139146789 0.01 0.0\n",
      "2000136 0.02 0.02\n",
      "135703058 0.09 0.07\n",
      "139397790 0.4 0.11\n",
      "135344057 0.27 0.2\n",
      "5522050031 0.6 0.53\n",
      "135787643 0.76 0.83\n",
      "7870110010 0.95 0.96\n",
      "0.0047\n",
      "200019 0.02 0.0\n",
      "135787452 0.01 0.01\n",
      "138948588 0.07 0.1\n",
      "139291416 0.4 0.2\n",
      "13421110010 0.4 0.22\n",
      "13777050040 0.67 0.57\n",
      "139046472 0.58 0.64\n",
      "5232080030 0.96 1.0\n",
      "0.004733333333333333\n",
      "139291965 0.01 0.0\n",
      "139397647 0.06 0.02\n",
      "135787307 0.05 0.07\n",
      "2262110010 0.38 0.19\n",
      "10985060030 0.6 0.31\n",
      "135787350 0.55 0.37\n",
      "135703235 0.85 0.87\n",
      "200168 0.97 1.0\n",
      "0.004766666666666667\n",
      "270046 0.01 0.0\n",
      "139051750 0.08 0.03\n",
      "12609060020 0.06 0.06\n",
      "139027266 0.16 0.16\n",
      "135697523 0.25 0.23\n",
      "138872681 0.34 0.38\n",
      "1234188 0.82 0.86\n",
      "13079050040 0.98 1.0\n",
      "0.0048\n",
      "139025625 0.01 0.0\n",
      "135732459 0.02 0.02\n",
      "139025755 0.04 0.08\n",
      "139420305 0.29 0.13\n",
      "137891146 0.28 0.28\n",
      "135703846 0.71 0.49\n",
      "139077723 0.8 0.68\n",
      "310050 0.97 1.0\n",
      "0.004833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000150 0.04 0.0\n",
      "139292058 0.07 0.05\n",
      "139178381 0.03 0.05\n",
      "135732431 0.11 0.13\n",
      "138948227 0.24 0.22\n",
      "139027689 0.32 0.48\n",
      "12689060010 0.78 0.79\n",
      "139027999 0.98 1.0\n",
      "0.004866666666666667\n",
      "135542446 0.01 0.0\n",
      "135702943 0.35 0.04\n",
      "137532577 0.18 0.09\n",
      "139177929 0.41 0.2\n",
      "2175080020 0.73 0.25\n",
      "20079 0.33 0.48\n",
      "136134609 0.54 0.8\n",
      "139178376 0.98 1.0\n",
      "0.0049\n",
      "2500223 0.03 0.0\n",
      "2000130 0.15 0.04\n",
      "139025598 0.03 0.07\n",
      "139160350 0.2 0.12\n",
      "139077536 0.15 0.29\n",
      "136134714 0.24 0.37\n",
      "80051 0.58 0.89\n",
      "9519050010 0.97 1.0\n",
      "0.004933333333333334\n",
      "139379771 0.02 0.0\n",
      "139188995 0.04 0.03\n",
      "2190110020 0.02 0.08\n",
      "4763080040 0.36 0.16\n",
      "139277261 0.09 0.25\n",
      "9636050010 0.65 0.39\n",
      "12485080010 0.76 0.66\n",
      "8999110010 0.68 0.94\n",
      "0.004966666666666667\n",
      "200072 0.01 0.0\n",
      "139027219 0.03 0.04\n",
      "137587691 0.06 0.08\n",
      "135840880 0.07 0.1\n",
      "139048875 0.02 0.23\n",
      "139146774 0.12 0.4\n",
      "135703903 0.91 0.86\n",
      "135703912 0.94 1.0\n",
      "0.005\n",
      "280023 0.01 0.0\n",
      "135787360 0.03 0.02\n",
      "139025529 0.26 0.1\n",
      "135703980 0.1 0.13\n",
      "135703931 0.23 0.28\n",
      "139178384 0.45 0.52\n",
      "136134609 0.54 0.8\n",
      "137532557 0.97 1.0\n",
      "0.005033333333333333\n",
      "2800100 0.01 0.0\n",
      "135787360 0.03 0.02\n",
      "139077858 0.27 0.08\n",
      "139025439 0.06 0.13\n",
      "300190 0.17 0.34\n",
      "135680828 0.49 0.4\n",
      "139077738 0.84 0.77\n",
      "139064502 0.92 1.0\n",
      "0.005066666666666667\n",
      "139208284 0.01 0.0\n",
      "139064533 0.03 0.04\n",
      "135809765 0.11 0.08\n",
      "600191 0.16 0.17\n",
      "139380117 0.2 0.23\n",
      "136134601 0.59 0.54\n",
      "9394050040 0.82 0.91\n",
      "140789991 0.83 0.97\n",
      "0.0051\n",
      "139027142 0.03 0.0\n",
      "600129 0.02 0.01\n",
      "139046269 0.04 0.06\n",
      "139323529 0.24 0.13\n",
      "135546173 0.1 0.25\n",
      "800107 0.12 0.41\n",
      "136134671 0.73 0.85\n",
      "80066 0.96 1.0\n",
      "0.0051333333333333335\n",
      "200236 0.01 0.0\n",
      "2000100 0.06 0.05\n",
      "2808110030 0.09 0.09\n",
      "135804329 0.23 0.2\n",
      "139365788 0.16 0.24\n",
      "136134445 0.48 0.45\n",
      "3855060010 0.58 0.68\n",
      "290049 0.97 1.0\n",
      "0.0051666666666666675\n",
      "2000154 0.1 0.0\n",
      "600192 0.08 0.04\n",
      "135680180 0.07 0.08\n",
      "139027848 0.15 0.16\n",
      "135803878 0.47 0.23\n",
      "137517122 0.56 0.37\n",
      "135672796 0.91 0.88\n",
      "1234179 0.96 1.0\n",
      "0.005200000000000001\n",
      "139397646 0.01 0.0\n",
      "135654664 0.01 0.05\n",
      "138152160 0.05 0.09\n",
      "14405060010 0.04 0.2\n",
      "10205050030 0.14 0.27\n",
      "139178453 0.57 0.48\n",
      "4736050030 0.8 0.81\n",
      "1234007 0.89 0.96\n",
      "0.005233333333333333\n",
      "139397653 0.01 0.0\n",
      "200092 0.02 0.02\n",
      "139378967 0.09 0.06\n",
      "135787431 0.06 0.19\n",
      "135787273 0.07 0.22\n",
      "136077612 0.47 0.47\n",
      "9350060040 0.81 0.74\n",
      "8489080040 0.92 1.0\n",
      "0.005266666666666666\n",
      "2500233 0.02 0.0\n",
      "600100 0.03 0.02\n",
      "135732326 0.12 0.07\n",
      "135732327 0.23 0.15\n",
      "135787385 0.28 0.3\n",
      "139669334 0.32 0.45\n",
      "13826060040 0.54 0.74\n",
      "110048 0.99 1.0\n",
      "0.0053\n",
      "2500233 0.02 0.0\n",
      "200098 0.03 0.03\n",
      "135672824 0.07 0.09\n",
      "139146786 0.14 0.16\n",
      "600205 0.37 0.34\n",
      "138152110 0.38 0.43\n",
      "5178050010 0.82 0.76\n",
      "1234276 0.35 0.9\n",
      "0.005333333333333333\n",
      "2600247 0.01 0.0\n",
      "135703950 0.06 0.02\n",
      "600119 0.04 0.08\n",
      "136410875 0.1 0.15\n",
      "13216110041 0.55 0.32\n",
      "136029544 0.22 0.37\n",
      "135787651 0.72 0.65\n",
      "20027 0.97 0.99\n",
      "0.005366666666666666\n",
      "200010 0.05 0.0\n",
      "900168 0.07 0.04\n",
      "139025635 0.08 0.07\n",
      "135809929 0.25 0.19\n",
      "135703805 0.42 0.31\n",
      "136134601 0.59 0.54\n",
      "80016 0.78 0.81\n",
      "138872621 0.97 1.0\n",
      "0.0054\n",
      "2000162 0.02 0.0\n",
      "135704028 0.11 0.03\n",
      "139025660 0.11 0.08\n",
      "10055050020 0.3 0.11\n",
      "135841059 0.32 0.21\n",
      "139179709 0.44 0.37\n",
      "139077808 0.76 0.8\n",
      "11882060030 0.97 1.0\n",
      "0.005433333333333333\n",
      "600195 0.01 0.0\n",
      "270034 0.02 0.02\n",
      "139064528 0.05 0.06\n",
      "141018076 0.1 0.1\n",
      "400235 0.53 0.31\n",
      "136134601 0.57 0.54\n",
      "11072060040 0.8 0.85\n",
      "9587080020 0.97 0.98\n",
      "0.0054666666666666665\n",
      "2500222 0.02 0.0\n",
      "139025748 0.05 0.02\n",
      "136446390 0.14 0.05\n",
      "8686110011 0.04 0.19\n",
      "136029732 0.09 0.26\n",
      "139077964 0.39 0.56\n",
      "5232050030 0.59 0.69\n",
      "139027573 0.96 1.0\n",
      "0.0055000000000000005\n",
      "139025763 0.01 0.0\n",
      "2600248 0.03 0.02\n",
      "137532577 0.18 0.09\n",
      "135804329 0.22 0.2\n",
      "8625110010 0.37 0.25\n",
      "700161 0.38 0.38\n",
      "80051 0.61 0.89\n",
      "139319912 0.88 0.93\n",
      "0.005533333333333334\n",
      "139291965 0.01 0.0\n",
      "139201895 0.09 0.04\n",
      "135680099 0.2 0.09\n",
      "140750841 0.08 0.12\n",
      "135787427 0.2 0.28\n",
      "6300110040 0.56 0.54\n",
      "139046361 0.42 0.62\n",
      "139077886 0.97 1.0\n",
      "0.005566666666666667\n",
      "139025831 0.01 0.0\n",
      "135787452 0.01 0.01\n",
      "135703058 0.09 0.07\n",
      "137547482 0.25 0.15\n",
      "135191374 0.23 0.31\n",
      "136029622 0.45 0.43\n",
      "136134499 0.92 0.89\n",
      "137517080 0.96 1.0\n",
      "0.005600000000000001\n",
      "2600210 0.01 0.0\n",
      "200179 0.07 0.01\n",
      "139189017 0.2 0.09\n",
      "135804089 0.05 0.18\n",
      "10406060030 0.67 0.26\n",
      "800107 0.17 0.41\n",
      "80090 0.78 0.77\n",
      "139046502 0.94 1.0\n",
      "0.005633333333333334\n",
      "139291959 0.01 0.0\n",
      "139051754 0.05 0.03\n",
      "2190110020 0.01 0.08\n",
      "800112 0.12 0.1\n",
      "138152144 0.14 0.21\n",
      "135787415 0.2 0.38\n",
      "135787613 0.59 0.84\n",
      "137587724 0.97 1.0\n",
      "0.005666666666666666\n",
      "3264080010 0.01 0.0\n",
      "135787383 0.05 0.04\n",
      "139379571 0.13 0.07\n",
      "139146631 0.15 0.11\n",
      "139420426 0.47 0.27\n",
      "138872357 0.35 0.46\n",
      "135804395 0.66 0.7\n",
      "200237 0.96 1.0\n",
      "0.005699999999999999\n",
      "3813080010 0.02 0.0\n",
      "139341834 0.08 0.04\n",
      "139379613 0.08 0.08\n",
      "139277654 0.1 0.12\n",
      "136134474 0.28 0.29\n",
      "10288060040 0.51 0.5\n",
      "600115 0.59 0.69\n",
      "3202080010 0.97 0.99\n",
      "0.005733333333333333\n",
      "137862280 0.01 0.0\n",
      "139077442 0.05 0.02\n",
      "7867110010 0.2 0.09\n",
      "200178 0.34 0.15\n",
      "300190 0.18 0.34\n",
      "137517143 0.32 0.44\n",
      "80051 0.6 0.89\n",
      "110022 0.98 1.0\n",
      "0.0057666666666666665\n",
      "140750849 0.29 0.0\n",
      "13857050040 0.05 0.01\n",
      "40077 0.04 0.06\n",
      "4745050030 0.31 0.17\n",
      "139319944 0.15 0.23\n",
      "135780069 0.43 0.42\n",
      "135704286 0.75 0.65\n",
      "135703851 0.93 0.96\n",
      "0.0058\n",
      "2100191 0.01 0.0\n",
      "135787279 0.01 0.01\n",
      "135505838 0.03 0.06\n",
      "4819050010 0.25 0.17\n",
      "135732372 0.19 0.2\n",
      "139048454 0.5 0.41\n",
      "1234190 0.49 0.71\n",
      "137517065 0.95 1.0\n",
      "0.005833333333333334\n",
      "139397654 0.01 0.0\n",
      "137532541 0.05 0.03\n",
      "139413342 0.12 0.09\n",
      "139048435 0.25 0.2\n",
      "10406060030 0.7 0.26\n",
      "135702865 0.37 0.45\n",
      "6832110020 0.57 0.88\n",
      "4256060020 0.97 1.0\n",
      "0.005866666666666667\n",
      "139277191 0.01 0.0\n",
      "139027970 0.05 0.01\n",
      "139161946 0.12 0.09\n",
      "137547482 0.27 0.15\n",
      "2221080011 0.44 0.28\n",
      "139365986 0.61 0.41\n",
      "135703853 0.78 0.84\n",
      "300031 0.97 1.0\n",
      "0.0059\n",
      "200060 0.03 0.0\n",
      "139178463 0.41 0.04\n",
      "139379560 0.54 0.06\n",
      "137532565 0.27 0.15\n",
      "139319944 0.14 0.23\n",
      "137517062 0.66 0.58\n",
      "8587050010 0.82 0.77\n",
      "8473110040 0.97 0.98\n",
      "0.005933333333333334\n",
      "135787400 0.01 0.0\n",
      "2000136 0.02 0.02\n",
      "40077 0.03 0.06\n",
      "135732333 0.08 0.11\n",
      "139027878 0.26 0.33\n",
      "6892110020 0.68 0.36\n",
      "136077603 0.83 0.81\n",
      "137517065 0.95 1.0\n",
      "0.005966666666666667\n",
      "139027233 0.02 0.0\n",
      "139146770 0.01 0.01\n",
      "137587691 0.06 0.08\n",
      "135732440 0.04 0.13\n",
      "137891146 0.29 0.28\n",
      "31 0.49 0.56\n",
      "136410952 0.8 0.87\n",
      "9942080010 0.89 0.96\n",
      "0.006\n",
      "2100123 0.03 0.0\n",
      "135787345 0.11 0.04\n",
      "138173606 0.02 0.08\n",
      "135224662 0.04 0.17\n",
      "138152120 0.22 0.35\n",
      "80099 0.43 0.57\n",
      "136411025 0.8 0.71\n",
      "7870110010 0.94 0.96\n",
      "0.006033333333333334\n",
      "4210080010 0.01 0.0\n",
      "2000131 0.05 0.02\n",
      "140785966 0.09 0.06\n",
      "135840971 0.16 0.17\n",
      "139025769 0.18 0.24\n",
      "12004060030 0.68 0.59\n",
      "240028 0.36 0.62\n",
      "600231 0.97 1.0\n",
      "0.006066666666666667\n",
      "270016 0.01 0.0\n",
      "200023 0.05 0.01\n",
      "136434357 0.01 0.09\n",
      "135732489 0.17 0.19\n",
      "135191374 0.2 0.31\n",
      "139186879 0.37 0.41\n",
      "139027214 0.63 0.71\n",
      "138872621 0.96 1.0\n",
      "0.0061\n",
      "2000122 0.09 0.0\n",
      "135732415 0.02 0.03\n",
      "135732356 0.13 0.06\n",
      "135546353 0.25 0.2\n",
      "6273050030 0.35 0.32\n",
      "135787141 0.33 0.36\n",
      "136411025 0.84 0.71\n",
      "31006 0.86 1.0\n",
      "0.006133333333333333\n",
      "280052 0.01 0.0\n",
      "135787452 0.01 0.01\n",
      "139161946 0.13 0.09\n",
      "139025749 0.14 0.13\n",
      "135697523 0.27 0.23\n",
      "135704023 0.49 0.37\n",
      "9559080010 0.71 0.65\n",
      "1234185 0.88 1.0\n",
      "0.0061666666666666675\n",
      "140750829 0.31 0.0\n",
      "2600220 0.01 0.03\n",
      "135703817 0.25 0.1\n",
      "140474070 0.09 0.11\n",
      "135680960 0.1 0.34\n",
      "138872341 0.48 0.41\n",
      "80090 0.84 0.77\n",
      "6102080010 0.97 0.99\n",
      "0.0062\n",
      "139146798 0.01 0.0\n",
      "135732406 0.04 0.02\n",
      "138152160 0.04 0.09\n",
      "135787294 0.08 0.13\n",
      "135704086 0.27 0.27\n",
      "13777050040 0.61 0.57\n",
      "135724910 0.74 0.7\n",
      "50013 0.96 1.0\n",
      "0.006233333333333333\n",
      "2000162 0.03 0.0\n",
      "141019153 0.01 0.01\n",
      "135732259 0.09 0.07\n",
      "139319937 0.1 0.11\n",
      "138948044 0.2 0.33\n",
      "139046457 0.28 0.37\n",
      "139379934 0.65 0.7\n",
      "138872621 0.97 1.0\n",
      "0.006266666666666667\n",
      "2600226 0.06 0.0\n",
      "139420304 0.13 0.01\n",
      "139319278 0.07 0.05\n",
      "2100165 0.16 0.15\n",
      "139025760 0.22 0.22\n",
      "136077609 0.46 0.52\n",
      "11200060030 0.46 0.73\n",
      "6904060020 0.96 1.0\n",
      "0.0063\n",
      "139430756 0.01 0.0\n",
      "139027175 0.07 0.04\n",
      "135702958 0.04 0.1\n",
      "139207919 0.04 0.12\n",
      "139077831 0.19 0.24\n",
      "300020 0.42 0.52\n",
      "9559080010 0.72 0.65\n",
      "135703991 0.96 1.0\n",
      "0.006333333333333333\n",
      "135703945 0.01 0.0\n",
      "139320395 0.05 0.04\n",
      "600116 0.02 0.06\n",
      "139146785 0.07 0.12\n",
      "139277261 0.09 0.25\n",
      "80099 0.39 0.57\n",
      "9350060040 0.8 0.74\n",
      "136134656 0.97 1.0\n",
      "0.006366666666666667\n",
      "139178427 0.01 0.0\n",
      "136752976 0.01 0.03\n",
      "135702958 0.05 0.1\n",
      "140750841 0.08 0.12\n",
      "4267110010 0.21 0.34\n",
      "80099 0.45 0.57\n",
      "138872353 0.51 0.73\n",
      "1234084 0.97 1.0\n",
      "0.0064\n",
      "139146665 0.04 0.0\n",
      "135732346 0.11 0.05\n",
      "136134530 0.15 0.06\n",
      "135703018 0.25 0.13\n",
      "135344196 0.36 0.22\n",
      "139077494 0.33 0.53\n",
      "5668050030 0.81 0.62\n",
      "1234181 0.95 1.0\n",
      "0.0064333333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500225 0.02 0.0\n",
      "135787403 0.02 0.04\n",
      "136456566 0.2 0.08\n",
      "138900715 0.03 0.15\n",
      "400235 0.56 0.31\n",
      "139277255 0.24 0.36\n",
      "135703864 0.66 0.69\n",
      "138872365 0.97 1.0\n",
      "0.006466666666666667\n",
      "139146789 0.01 0.0\n",
      "135787345 0.1 0.04\n",
      "135724926 0.07 0.05\n",
      "137532493 0.1 0.11\n",
      "200243 0.15 0.23\n",
      "135704000 0.48 0.42\n",
      "139686336 0.76 0.81\n",
      "1234175 0.87 1.0\n",
      "0.006500000000000001\n",
      "139178427 0.01 0.0\n",
      "135732396 0.07 0.03\n",
      "139291487 0.05 0.05\n",
      "139027210 0.03 0.1\n",
      "135224677 0.16 0.32\n",
      "12004060030 0.67 0.59\n",
      "135703811 0.86 0.87\n",
      "30041 0.96 1.0\n",
      "0.006533333333333334\n",
      "140750829 0.13 0.0\n",
      "270041 0.04 0.01\n",
      "800174 0.25 0.09\n",
      "136075854 0.13 0.19\n",
      "10985060030 0.54 0.31\n",
      "14550060010 0.41 0.57\n",
      "135703988 0.65 0.88\n",
      "200121 0.89 1.0\n",
      "0.006566666666666666\n",
      "139208290 0.03 0.0\n",
      "139146778 0.01 0.02\n",
      "135703058 0.12 0.07\n",
      "135680829 0.3 0.18\n",
      "400231 0.35 0.21\n",
      "135703870 0.56 0.53\n",
      "500224 0.89 0.68\n",
      "137517124 0.94 1.0\n",
      "0.006600000000000001\n",
      "2000150 0.04 0.0\n",
      "136446310 0.11 0.04\n",
      "138872508 0.06 0.07\n",
      "135702611 0.05 0.1\n",
      "4267110010 0.21 0.34\n",
      "136077612 0.46 0.47\n",
      "80020 0.44 0.65\n",
      "139077804 0.94 1.0\n",
      "0.006633333333333333\n",
      "135345862 0.04 0.0\n",
      "139146667 0.09 0.02\n",
      "2808110030 0.1 0.09\n",
      "139027139 0.06 0.1\n",
      "30035 0.32 0.26\n",
      "139338673 0.41 0.55\n",
      "139077899 0.92 0.88\n",
      "139046502 0.96 1.0\n",
      "0.006666666666666666\n",
      "200057 0.03 0.0\n",
      "9213110040 0.05 0.03\n",
      "135702958 0.05 0.1\n",
      "135680943 0.11 0.18\n",
      "139379486 0.55 0.3\n",
      "135703846 0.66 0.49\n",
      "8840110010 0.59 0.74\n",
      "6293050040 0.97 0.98\n",
      "0.0067\n",
      "139025625 0.02 0.0\n",
      "135787384 0.04 0.03\n",
      "135787447 0.02 0.07\n",
      "400249 0.07 0.14\n",
      "135191374 0.26 0.31\n",
      "139027211 0.34 0.43\n",
      "136318453 0.58 0.64\n",
      "80056 0.96 1.0\n",
      "0.006733333333333333\n",
      "138948139 0.02 0.0\n",
      "200088 0.02 0.01\n",
      "135732488 0.11 0.08\n",
      "137547482 0.23 0.15\n",
      "135542470 0.13 0.28\n",
      "5006080020 0.43 0.41\n",
      "5178050010 0.82 0.76\n",
      "135704077 0.96 0.93\n",
      "0.0067666666666666665\n",
      "139185924 0.01 0.0\n",
      "135732459 0.02 0.02\n",
      "139025660 0.09 0.08\n",
      "139397769 0.02 0.15\n",
      "139365594 0.14 0.26\n",
      "139365986 0.6 0.41\n",
      "135787281 0.73 0.73\n",
      "400243 0.56 0.96\n",
      "0.0068000000000000005\n",
      "4343080020 0.01 0.0\n",
      "135841016 0.04 0.03\n",
      "139027147 0.04 0.06\n",
      "200234 0.07 0.12\n",
      "135224843 0.26 0.31\n",
      "136134571 0.47 0.41\n",
      "80051 0.61 0.89\n",
      "139179724 0.83 0.92\n",
      "0.006833333333333334\n",
      "600195 0.01 0.0\n",
      "200092 0.01 0.02\n",
      "139161946 0.11 0.09\n",
      "600191 0.14 0.17\n",
      "135787039 0.28 0.27\n",
      "136134571 0.46 0.41\n",
      "1234159 0.44 0.86\n",
      "8893110010 0.97 1.0\n",
      "0.006866666666666667\n",
      "2100187 0.01 0.0\n",
      "136410874 0.05 0.02\n",
      "4793110041 0.06 0.08\n",
      "139146785 0.08 0.12\n",
      "135787401 0.25 0.27\n",
      "135787141 0.29 0.36\n",
      "135703988 0.67 0.88\n",
      "310051 0.85 1.0\n",
      "0.0069\n",
      "2500229 0.02 0.0\n",
      "135191349 0.03 0.01\n",
      "136446390 0.11 0.05\n",
      "139379805 0.29 0.17\n",
      "135703890 0.36 0.34\n",
      "3896060010 0.42 0.35\n",
      "135732253 0.19 0.85\n",
      "137532629 0.98 1.0\n",
      "0.006933333333333334\n",
      "139277191 0.01 0.0\n",
      "139146667 0.09 0.02\n",
      "138173606 0.03 0.08\n",
      "136410875 0.11 0.15\n",
      "136029604 0.28 0.3\n",
      "135703801 0.69 0.47\n",
      "138872499 0.67 0.67\n",
      "310039 0.49 1.0\n",
      "0.006966666666666667\n",
      "20008 0.04 0.0\n",
      "20086 0.03 0.01\n",
      "135840903 0.04 0.06\n",
      "600208 0.21 0.12\n",
      "139291428 0.25 0.25\n",
      "135703891 0.5 0.36\n",
      "135703988 0.68 0.88\n",
      "1234278 0.95 0.91\n",
      "0.006999999999999999\n",
      "2500224 0.03 0.0\n",
      "11048050031 0.04 0.01\n",
      "135546389 0.09 0.09\n",
      "135224777 0.08 0.15\n",
      "135804308 0.33 0.31\n",
      "5128050020 0.75 0.57\n",
      "139179723 0.58 0.66\n",
      "1234279 0.96 0.91\n",
      "0.007033333333333334\n",
      "2000125 0.11 0.0\n",
      "135732459 0.02 0.02\n",
      "135698200 0.13 0.09\n",
      "140750841 0.08 0.12\n",
      "138872338 0.3 0.26\n",
      "136318473 0.41 0.48\n",
      "136410972 0.87 0.82\n",
      "310040 0.97 1.0\n",
      "0.007066666666666666\n",
      "280049 0.02 0.0\n",
      "600100 0.03 0.02\n",
      "139365855 0.1 0.09\n",
      "135787294 0.1 0.13\n",
      "6726060010 0.34 0.33\n",
      "135703891 0.54 0.36\n",
      "5018110010 0.64 0.64\n",
      "139058213 0.94 1.0\n",
      "0.0070999999999999995\n",
      "2500242 0.09 0.0\n",
      "135804051 0.02 0.01\n",
      "140860627 0.02 0.07\n",
      "139379805 0.31 0.17\n",
      "135224677 0.21 0.32\n",
      "12004060030 0.67 0.59\n",
      "136411025 0.84 0.71\n",
      "300053 0.97 1.0\n",
      "0.0071333333333333335\n",
      "2500236 0.01 0.0\n",
      "139048883 0.07 0.04\n",
      "200127 0.13 0.09\n",
      "139146786 0.14 0.16\n",
      "135698246 0.31 0.25\n",
      "3196110020 0.36 0.45\n",
      "135787643 0.73 0.83\n",
      "139077961 0.96 1.0\n",
      "0.007166666666666667\n",
      "13741060020 0.03 0.0\n",
      "135787345 0.12 0.04\n",
      "135732317 0.05 0.08\n",
      "139413150 0.13 0.14\n",
      "135702863 0.48 0.31\n",
      "139146774 0.15 0.4\n",
      "139046472 0.56 0.64\n",
      "4240110010 0.97 1.0\n",
      "0.0072\n",
      "210016 0.01 0.0\n",
      "135697619 0.06 0.01\n",
      "135787352 0.05 0.08\n",
      "139027210 0.04 0.1\n",
      "6726060010 0.33 0.33\n",
      "137517081 0.51 0.41\n",
      "135704015 0.87 0.89\n",
      "1234184 0.98 1.0\n",
      "0.007233333333333334\n",
      "139397754 0.06 0.0\n",
      "139185734 0.01 0.01\n",
      "139077928 0.04 0.06\n",
      "139186793 0.22 0.11\n",
      "135680963 0.15 0.2\n",
      "135703834 0.58 0.53\n",
      "9394050040 0.77 0.91\n",
      "137587724 0.98 1.0\n",
      "0.007266666666666667\n",
      "135542446 0.01 0.0\n",
      "137517218 0.01 0.01\n",
      "139077891 0.24 0.07\n",
      "136410875 0.15 0.15\n",
      "135680963 0.15 0.2\n",
      "139277255 0.24 0.36\n",
      "138872495 0.61 0.66\n",
      "2115050020 0.96 0.99\n",
      "0.0073\n",
      "137532564 0.04 0.0\n",
      "139027175 0.07 0.04\n",
      "139378967 0.11 0.06\n",
      "135680214 0.25 0.11\n",
      "139025805 0.33 0.33\n",
      "135787337 0.69 0.55\n",
      "135703787 0.76 0.69\n",
      "14535110010 0.97 1.0\n",
      "0.007333333333333333\n",
      "137862280 0.01 0.0\n",
      "139292058 0.09 0.05\n",
      "73 0.14 0.08\n",
      "200126 0.14 0.11\n",
      "135732320 0.27 0.23\n",
      "136029622 0.5 0.43\n",
      "1234188 0.77 0.86\n",
      "6102080010 0.97 0.99\n",
      "0.007366666666666667\n",
      "139077433 0.01 0.0\n",
      "135732415 0.02 0.03\n",
      "135703058 0.08 0.07\n",
      "135697753 0.07 0.1\n",
      "7340110031 0.26 0.34\n",
      "13574080010 0.46 0.4\n",
      "135703235 0.87 0.87\n",
      "139186862 0.97 1.0\n",
      "0.0074\n",
      "2500126 0.01 0.0\n",
      "136786537 0.09 0.02\n",
      "135787296 0.07 0.06\n",
      "136075836 0.24 0.17\n",
      "6726060010 0.38 0.33\n",
      "8393080040 0.51 0.4\n",
      "135732253 0.63 0.85\n",
      "6361080030 0.97 1.0\n",
      "0.0074333333333333335\n",
      "2000117 0.06 0.0\n",
      "6219080020 0.01 0.01\n",
      "135803869 0.06 0.05\n",
      "140474120 0.15 0.16\n",
      "8610050010 0.27 0.25\n",
      "138152110 0.41 0.43\n",
      "5018110010 0.64 0.64\n",
      "135703826 0.88 0.93\n",
      "0.0074666666666666675\n",
      "2100176 0.01 0.0\n",
      "135703551 0.08 0.02\n",
      "139025803 0.04 0.05\n",
      "900133 0.2 0.1\n",
      "139027878 0.24 0.33\n",
      "139027689 0.36 0.48\n",
      "136410952 0.85 0.87\n",
      "310011 0.7 1.0\n",
      "0.0075\n",
      "139025658 0.01 0.0\n",
      "135787267 0.11 0.01\n",
      "200153 0.04 0.05\n",
      "13826060041 0.19 0.18\n",
      "135344159 0.25 0.2\n",
      "7973080020 0.46 0.47\n",
      "5668050030 0.87 0.62\n",
      "2569060030 0.97 1.0\n",
      "0.007533333333333333\n",
      "2800100 0.01 0.0\n",
      "139146778 0.01 0.02\n",
      "139027269 0.06 0.06\n",
      "135702611 0.05 0.1\n",
      "135680925 0.32 0.29\n",
      "7973080020 0.48 0.47\n",
      "80016 0.85 0.81\n",
      "8268050041 0.97 1.0\n",
      "0.007566666666666667\n",
      "2500233 0.02 0.0\n",
      "135704174 0.25 0.03\n",
      "135344076 0.06 0.07\n",
      "139027139 0.06 0.1\n",
      "137517110 0.32 0.29\n",
      "136318473 0.42 0.48\n",
      "14252110010 0.75 0.92\n",
      "136134476 0.94 0.96\n",
      "0.0076\n",
      "2000108 0.03 0.0\n",
      "137862312 0.04 0.05\n",
      "139027269 0.06 0.06\n",
      "135943377 0.17 0.16\n",
      "135787427 0.23 0.28\n",
      "10288060040 0.52 0.5\n",
      "7387110040 0.78 0.73\n",
      "290049 0.98 1.0\n",
      "0.007633333333333333\n",
      "139178362 0.01 0.0\n",
      "139027276 0.06 0.01\n",
      "8725110010 0.03 0.06\n",
      "139058222 0.16 0.15\n",
      "13421110010 0.37 0.22\n",
      "8847110020 0.54 0.58\n",
      "135732253 0.53 0.85\n",
      "300126 0.98 1.0\n",
      "0.007666666666666667\n",
      "139291965 0.0 0.0\n",
      "139048361 0.09 0.05\n",
      "139413348 0.24 0.07\n",
      "9400110041 0.13 0.11\n",
      "200243 0.15 0.23\n",
      "135704101 0.51 0.41\n",
      "139046361 0.42 0.62\n",
      "1234193 0.73 0.94\n",
      "0.0077\n",
      "2500126 0.01 0.0\n",
      "135787272 0.01 0.04\n",
      "139025803 0.05 0.05\n",
      "135804098 0.01 0.13\n",
      "135680929 0.15 0.21\n",
      "136077609 0.56 0.52\n",
      "4614110040 0.89 0.78\n",
      "135787422 0.95 0.99\n",
      "0.007733333333333333\n",
      "139077962 0.01 0.0\n",
      "200092 0.03 0.02\n",
      "138872774 0.02 0.06\n",
      "139025510 0.26 0.11\n",
      "135542470 0.18 0.28\n",
      "139077582 0.42 0.36\n",
      "80016 0.85 0.81\n",
      "9466050040 0.96 0.97\n",
      "0.0077666666666666665\n",
      "2600247 0.01 0.0\n",
      "11048050031 0.03 0.01\n",
      "139379613 0.11 0.08\n",
      "4819050010 0.34 0.17\n",
      "2175080020 0.75 0.25\n",
      "700161 0.45 0.38\n",
      "139077738 0.85 0.77\n",
      "1234113 0.94 1.0\n",
      "0.0078000000000000005\n",
      "2000151 0.03 0.0\n",
      "138872425 0.01 0.02\n",
      "136089066 0.06 0.07\n",
      "80085 0.08 0.11\n",
      "135804045 0.18 0.26\n",
      "136029590 0.67 0.53\n",
      "139146797 0.62 0.66\n",
      "9419060040 0.97 0.99\n",
      "0.007833333333333333\n",
      "139027152 0.01 0.0\n",
      "139077430 0.01 0.03\n",
      "135702930 0.06 0.1\n",
      "135787263 0.21 0.19\n",
      "135703845 0.25 0.28\n",
      "135787350 0.57 0.37\n",
      "800161 0.79 0.66\n",
      "137517076 0.92 1.0\n",
      "0.007866666666666666\n",
      "139341922 0.01 0.0\n",
      "139178485 0.03 0.04\n",
      "40066 0.04 0.05\n",
      "137532565 0.26 0.15\n",
      "135732348 0.19 0.2\n",
      "139208220 0.54 0.59\n",
      "5668050030 0.86 0.62\n",
      "9515110010 0.94 1.0\n",
      "0.0079\n",
      "139320425 0.06 0.0\n",
      "139048883 0.06 0.04\n",
      "139397750 0.09 0.07\n",
      "13944060040 0.24 0.12\n",
      "135732492 0.1 0.22\n",
      "136318407 0.29 0.36\n",
      "135703978 0.75 0.76\n",
      "11276110040 0.97 1.0\n",
      "0.007933333333333334\n",
      "135703481 0.03 0.0\n",
      "11976050030 0.02 0.02\n",
      "139686311 0.1 0.09\n",
      "600191 0.14 0.17\n",
      "136434400 0.37 0.24\n",
      "135680275 0.41 0.36\n",
      "11200060030 0.53 0.73\n",
      "135787308 0.98 1.0\n",
      "0.007966666666666667\n",
      "27005 0.01 0.0\n",
      "139320395 0.08 0.04\n",
      "139379145 0.11 0.07\n",
      "135680771 0.07 0.11\n",
      "10985060030 0.57 0.31\n",
      "8847110020 0.53 0.58\n",
      "10679060020 0.77 0.72\n",
      "135703829 0.89 0.94\n",
      "0.008\n",
      "141370751 0.12 0.0\n",
      "135732487 0.03 0.02\n",
      "135787379 0.06 0.09\n",
      "139319937 0.13 0.11\n",
      "8610050010 0.3 0.25\n",
      "135703885 0.37 0.42\n",
      "136411003 0.85 0.72\n",
      "9466050040 0.95 0.97\n",
      "Epoch 7: Loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/jbrandt.terminal/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 65 variables.\n",
      "INFO:tensorflow:Converted 65 variables to const ops.\n",
      "starting epoch 2, alpha: 0.3, beta: 0.0, drop: 0.99 Learning rate: 0.02\n",
      "0.008033333333333333\n",
      "2500221 0.04 0.0\n",
      "200088 0.01 0.01\n",
      "139160447 0.05 0.06\n",
      "140860608 0.06 0.11\n",
      "2847060010 0.23 0.34\n",
      "5006080020 0.58 0.41\n",
      "14252110010 0.66 0.92\n",
      "8205080040 0.96 1.0\n",
      "0.008066666666666666\n",
      "2600214 0.01 0.0\n",
      "135732418 0.02 0.04\n",
      "139077462 0.36 0.06\n",
      "139319999 0.19 0.11\n",
      "12953080030 0.2 0.3\n",
      "140474083 0.42 0.43\n",
      "400229 0.87 0.82\n",
      "1234193 0.69 0.94\n",
      "0.008100000000000001\n",
      "2600227 0.03 0.0\n",
      "136434333 0.01 0.04\n",
      "135724919 0.09 0.1\n",
      "135681019 0.06 0.18\n",
      "139025822 0.17 0.23\n",
      "8393080040 0.47 0.4\n",
      "135704456 0.86 0.75\n",
      "136410973 0.76 1.0\n",
      "0.008133333333333334\n",
      "1234219 0.02 0.0\n",
      "139160528 0.02 0.04\n",
      "135840931 0.11 0.08\n",
      "135809884 0.23 0.16\n",
      "135680808 0.22 0.22\n",
      "139179727 0.35 0.45\n",
      "5668050030 0.82 0.62\n",
      "9565110040 0.94 0.97\n",
      "0.008166666666666666\n",
      "12973080040 0.01 0.0\n",
      "600192 0.07 0.04\n",
      "135191289 0.16 0.07\n",
      "9190110041 0.06 0.11\n",
      "138948044 0.2 0.33\n",
      "135191261 0.31 0.43\n",
      "1234194 0.66 0.76\n",
      "135724996 0.95 1.0\n",
      "0.008199999999999999\n",
      "2000118 0.02 0.0\n",
      "200084 0.02 0.01\n",
      "135698200 0.13 0.09\n",
      "139027139 0.06 0.1\n",
      "400235 0.55 0.31\n",
      "4702110030 0.83 0.47\n",
      "5178050010 0.85 0.76\n",
      "9625080010 0.97 0.95\n",
      "0.008233333333333334\n",
      "137535101 0.01 0.0\n",
      "139025748 0.04 0.02\n",
      "31005 0.14 0.07\n",
      "139028045 0.13 0.12\n",
      "139342340 0.28 0.24\n",
      "139669334 0.38 0.45\n",
      "12689060010 0.78 0.79\n",
      "9587080020 0.97 0.98\n",
      "0.008266666666666667\n",
      "139319943 0.01 0.0\n",
      "139189633 0.1 0.04\n",
      "135724972 0.09 0.06\n",
      "700189 0.15 0.19\n",
      "139048986 0.19 0.32\n",
      "136077609 0.55 0.52\n",
      "136134527 0.92 0.81\n",
      "2003050020 0.96 0.97\n",
      "0.0083\n",
      "200236 0.01 0.0\n",
      "600100 0.03 0.02\n",
      "135787162 0.1 0.1\n",
      "8686110011 0.03 0.19\n",
      "135803878 0.44 0.23\n",
      "135804072 0.63 0.58\n",
      "138872402 0.75 0.75\n",
      "12767050010 0.96 0.99\n",
      "0.008333333333333333\n",
      "400242 0.01 0.0\n",
      "135787435 0.02 0.03\n",
      "135724946 0.14 0.1\n",
      "135680943 0.1 0.18\n",
      "139025822 0.19 0.23\n",
      "139669334 0.41 0.45\n",
      "136134527 0.92 0.81\n",
      "137532557 0.97 1.0\n",
      "0.008366666666666666\n",
      "139379564 0.01 0.0\n",
      "139025636 0.01 0.04\n",
      "139161827 0.06 0.07\n",
      "2100165 0.2 0.15\n",
      "135804038 0.48 0.22\n",
      "8847110020 0.53 0.58\n",
      "300021 0.95 0.88\n",
      "5092050020 0.97 1.0\n",
      "0.0084\n",
      "200080 0.02 0.0\n",
      "135732349 0.1 0.03\n",
      "135774796 0.08 0.09\n",
      "137862315 0.13 0.17\n",
      "139277261 0.07 0.25\n",
      "135703846 0.65 0.49\n",
      "240028 0.43 0.62\n",
      "200168 0.97 1.0\n",
      "0.008433333333333334\n",
      "20008 0.03 0.0\n",
      "139146778 0.01 0.02\n",
      "135505950 0.03 0.07\n",
      "135787426 0.14 0.14\n",
      "135704007 0.22 0.26\n",
      "135703899 0.38 0.47\n",
      "4194060020 0.91 0.9\n",
      "5141080010 0.97 0.99\n",
      "0.008466666666666667\n",
      "2000151 0.03 0.0\n",
      "135787293 0.06 0.05\n",
      "135702958 0.05 0.1\n",
      "140750841 0.07 0.12\n",
      "135732320 0.21 0.23\n",
      "137517101 0.18 0.39\n",
      "135703978 0.74 0.76\n",
      "12616060020 0.97 0.99\n",
      "0.0085\n",
      "270016 0.01 0.0\n",
      "135787339 0.02 0.02\n",
      "139064528 0.05 0.06\n",
      "8686110011 0.02 0.19\n",
      "139025816 0.1 0.24\n",
      "9974080020 0.4 0.36\n",
      "80090 0.76 0.77\n",
      "200167 0.96 1.0\n",
      "0.008533333333333334\n",
      "139277644 0.01 0.0\n",
      "2000164 0.06 0.02\n",
      "135803812 0.03 0.09\n",
      "135732397 0.06 0.17\n",
      "400235 0.55 0.31\n",
      "138900837 0.44 0.47\n",
      "135703864 0.69 0.69\n",
      "8691060010 0.96 1.0\n",
      "0.008566666666666667\n",
      "5373050040 0.02 0.0\n",
      "135804055 0.01 0.03\n",
      "4793110041 0.05 0.08\n",
      "135787336 0.07 0.15\n",
      "135345856 0.43 0.28\n",
      "138948353 0.36 0.37\n",
      "80090 0.79 0.77\n",
      "290075 0.97 1.0\n",
      "0.0086\n",
      "135787274 0.01 0.0\n",
      "135840873 0.03 0.03\n",
      "139027159 0.03 0.08\n",
      "139368654 0.16 0.18\n",
      "400235 0.56 0.31\n",
      "135680912 0.22 0.49\n",
      "3855060010 0.59 0.68\n",
      "137517139 0.93 1.0\n",
      "0.008633333333333333\n",
      "11409060010 0.01 0.0\n",
      "139365814 0.03 0.02\n",
      "139048878 0.13 0.06\n",
      "137532568 0.16 0.19\n",
      "135732329 0.16 0.21\n",
      "139291389 0.21 0.37\n",
      "138872620 0.53 0.6\n",
      "5232080030 0.96 1.0\n",
      "0.008666666666666668\n",
      "135654723 0.01 0.0\n",
      "200179 0.06 0.01\n",
      "135732466 0.12 0.07\n",
      "139025439 0.06 0.13\n",
      "9785060010 0.5 0.3\n",
      "136134601 0.58 0.54\n",
      "136410972 0.86 0.82\n",
      "136134795 0.97 1.0\n",
      "0.0087\n",
      "2500232 0.04 0.0\n",
      "135702943 0.16 0.04\n",
      "135654751 0.07 0.08\n",
      "139319937 0.11 0.11\n",
      "136029732 0.1 0.26\n",
      "4531050030 0.6 0.38\n",
      "135787651 0.72 0.65\n",
      "310050 0.97 1.0\n",
      "0.008733333333333333\n",
      "600131 0.01 0.0\n",
      "139025415 0.02 0.04\n",
      "135724926 0.08 0.05\n",
      "139420305 0.25 0.13\n",
      "137587859 0.17 0.27\n",
      "10828060020 0.63 0.4\n",
      "200159 0.72 0.65\n",
      "1234187 0.93 1.0\n",
      "0.008766666666666667\n",
      "2000113 0.01 0.0\n",
      "2000163 0.03 0.01\n",
      "135732445 0.06 0.06\n",
      "140474070 0.09 0.11\n",
      "135703983 0.31 0.24\n",
      "8393080040 0.47 0.4\n",
      "600115 0.6 0.69\n",
      "135703926 0.96 1.0\n",
      "0.0088\n",
      "200077 0.08 0.0\n",
      "139146664 0.01 0.01\n",
      "7577110010 0.04 0.08\n",
      "135191354 0.33 0.19\n",
      "136410737 0.3 0.2\n",
      "6922080020 0.76 0.37\n",
      "30091 0.64 0.71\n",
      "4011050010 0.86 0.91\n",
      "0.008833333333333334\n",
      "2500225 0.01 0.0\n",
      "139027169 0.03 0.04\n",
      "135703911 0.31 0.05\n",
      "135703289 0.12 0.14\n",
      "139027215 0.28 0.31\n",
      "138901244 0.22 0.45\n",
      "1234188 0.76 0.86\n",
      "135703829 0.85 0.94\n",
      "0.008866666666666667\n",
      "200070 0.02 0.0\n",
      "139025636 0.02 0.04\n",
      "135680099 0.26 0.09\n",
      "139025749 0.2 0.13\n",
      "30035 0.33 0.26\n",
      "135703941 0.46 0.45\n",
      "6346050031 0.74 0.66\n",
      "9688110020 0.96 0.99\n",
      "0.0089\n",
      "2000117 0.06 0.0\n",
      "140750835 0.2 0.04\n",
      "135787162 0.09 0.1\n",
      "14711050030 0.33 0.1\n",
      "136410570 0.08 0.23\n",
      "136089240 0.44 0.54\n",
      "800193 0.89 0.81\n",
      "139186648 0.96 1.0\n",
      "0.008933333333333333\n",
      "5373050040 0.02 0.0\n",
      "2000130 0.12 0.04\n",
      "137517276 0.06 0.08\n",
      "135809774 0.09 0.19\n",
      "139277261 0.08 0.25\n",
      "139027260 0.66 0.57\n",
      "136089149 0.73 0.71\n",
      "290077 0.97 1.0\n",
      "0.008966666666666666\n",
      "1234199 0.01 0.0\n",
      "137517090 0.06 0.02\n",
      "139027259 0.05 0.09\n",
      "135807683 0.12 0.1\n",
      "139052583 0.33 0.29\n",
      "135703809 0.57 0.57\n",
      "1234190 0.52 0.71\n",
      "700179 0.98 1.0\n",
      "0.009000000000000001\n",
      "200020 0.01 0.0\n",
      "139146778 0.01 0.02\n",
      "135809765 0.06 0.08\n",
      "135809884 0.21 0.16\n",
      "135703890 0.32 0.34\n",
      "138900699 0.37 0.54\n",
      "800111 0.45 0.64\n",
      "290079 0.91 1.0\n",
      "0.009033333333333334\n",
      "280097 0.02 0.0\n",
      "200088 0.02 0.01\n",
      "138872774 0.03 0.06\n",
      "139186793 0.22 0.11\n",
      "139379486 0.52 0.3\n",
      "3196110020 0.34 0.45\n",
      "11072060040 0.78 0.85\n",
      "8489080040 0.91 1.0\n",
      "0.009066666666666666\n",
      "139146665 0.03 0.0\n",
      "800189 0.02 0.02\n",
      "135732445 0.03 0.06\n",
      "139025747 0.1 0.16\n",
      "135724956 0.26 0.32\n",
      "135703899 0.36 0.47\n",
      "400225 0.82 0.8\n",
      "20048 0.97 1.0\n",
      "0.0091\n",
      "139146773 0.01 0.0\n",
      "139025830 0.01 0.02\n",
      "139077445 0.02 0.09\n",
      "600208 0.21 0.12\n",
      "9744110021 0.37 0.28\n",
      "138872341 0.49 0.41\n",
      "136134499 0.87 0.89\n",
      "135704155 0.95 1.0\n",
      "0.009133333333333334\n",
      "139077432 0.01 0.0\n",
      "139191620 0.04 0.04\n",
      "2000105 0.1 0.09\n",
      "135703091 0.05 0.16\n",
      "135654714 0.03 0.25\n",
      "138872600 0.46 0.43\n",
      "200148 0.35 0.65\n",
      "1234187 0.95 1.0\n",
      "0.009166666666666667\n",
      "270030 0.01 0.0\n",
      "27003 0.02 0.02\n",
      "139027253 0.08 0.07\n",
      "135702870 0.12 0.17\n",
      "9744110021 0.39 0.28\n",
      "139077811 0.51 0.57\n",
      "7657050010 0.66 0.62\n",
      "139186860 0.98 1.0\n",
      "0.0092\n",
      "140860697 0.01 0.0\n",
      "135787293 0.04 0.05\n",
      "135698200 0.09 0.09\n",
      "135344156 0.15 0.11\n",
      "139025822 0.15 0.23\n",
      "137517081 0.48 0.41\n",
      "210020 0.71 0.7\n",
      "139319912 0.88 0.93\n",
      "0.009233333333333333\n",
      "2000109 0.01 0.0\n",
      "139292058 0.07 0.05\n",
      "135703817 0.19 0.1\n",
      "139319937 0.13 0.11\n",
      "600205 0.31 0.34\n",
      "4702110030 0.82 0.47\n",
      "139686336 0.82 0.81\n",
      "600175 0.95 1.0\n",
      "0.009266666666666666\n",
      "2000132 0.02 0.0\n",
      "135787321 0.03 0.02\n",
      "135751783 0.02 0.07\n",
      "137532507 0.24 0.14\n",
      "7031080010 0.47 0.33\n",
      "135809858 0.28 0.38\n",
      "135724910 0.74 0.7\n",
      "137891119 0.95 1.0\n",
      "0.009300000000000001\n",
      "2600206 0.01 0.0\n",
      "6219080020 0.01 0.01\n",
      "139319278 0.07 0.05\n",
      "20049 0.4 0.1\n",
      "139025791 0.14 0.26\n",
      "6922080020 0.76 0.37\n",
      "8587050010 0.8 0.77\n",
      "80014 0.94 1.0\n",
      "0.009333333333333334\n",
      "139397654 0.01 0.0\n",
      "139368484 0.04 0.05\n",
      "135702930 0.03 0.1\n",
      "138948444 0.01 0.1\n",
      "135804045 0.17 0.26\n",
      "400247 0.25 0.36\n",
      "210020 0.72 0.7\n",
      "8847060010 0.91 0.95\n",
      "0.009366666666666667\n",
      "139397653 0.01 0.0\n",
      "135841034 0.04 0.01\n",
      "135787379 0.04 0.09\n",
      "140860597 0.04 0.16\n",
      "7340110031 0.29 0.34\n",
      "135703283 0.45 0.49\n",
      "12485080010 0.66 0.66\n",
      "1234161 0.8 1.0\n",
      "0.0094\n",
      "137535080 0.01 0.0\n",
      "139089875 0.02 0.01\n",
      "136434357 0.02 0.09\n",
      "2022080010 0.24 0.19\n",
      "135344196 0.35 0.22\n",
      "137517081 0.46 0.41\n",
      "7509060040 0.66 0.74\n",
      "300126 0.97 1.0\n",
      "0.009433333333333334\n",
      "139178362 0.01 0.0\n",
      "200092 0.02 0.02\n",
      "138004142 0.12 0.09\n",
      "135787426 0.11 0.14\n",
      "139025805 0.27 0.33\n",
      "9487110040 0.15 0.48\n",
      "135703864 0.68 0.69\n",
      "135804090 0.97 1.0\n",
      "0.009466666666666667\n",
      "270044 0.02 0.0\n",
      "135787339 0.02 0.02\n",
      "139379571 0.12 0.07\n",
      "135732489 0.2 0.19\n",
      "137517110 0.32 0.29\n",
      "4531050030 0.57 0.38\n",
      "135704006 0.77 0.71\n",
      "138872442 0.97 1.0\n",
      "0.0095\n",
      "4210080010 0.01 0.0\n",
      "139077439 0.06 0.04\n",
      "139027147 0.03 0.06\n",
      "4438050040 0.12 0.13\n",
      "10985060030 0.52 0.31\n",
      "135680828 0.38 0.4\n",
      "139365764 0.57 0.66\n",
      "139077932 0.77 0.91\n",
      "0.009533333333333335\n",
      "200022 0.02 0.0\n",
      "137517218 0.01 0.01\n",
      "135724972 0.08 0.06\n",
      "139189349 0.05 0.16\n",
      "12282050030 0.19 0.22\n",
      "137517062 0.63 0.58\n",
      "12689060010 0.77 0.79\n",
      "290073 0.96 1.0\n",
      "0.009566666666666666\n",
      "200072 0.02 0.0\n",
      "139027146 0.02 0.01\n",
      "138152160 0.04 0.09\n",
      "600128 0.36 0.12\n",
      "6726060010 0.27 0.33\n",
      "4702110030 0.76 0.47\n",
      "136089149 0.66 0.71\n",
      "11276110040 0.96 1.0\n",
      "0.0096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139189061 0.02 0.0\n",
      "2000163 0.04 0.01\n",
      "135809729 0.07 0.1\n",
      "139397782 0.06 0.11\n",
      "8625110010 0.31 0.25\n",
      "400247 0.23 0.36\n",
      "12069060030 0.9 0.73\n",
      "1234161 0.78 1.0\n",
      "0.009633333333333334\n",
      "250049 0.08 0.0\n",
      "139027186 0.04 0.03\n",
      "137587691 0.06 0.08\n",
      "135702870 0.13 0.17\n",
      "135703805 0.35 0.31\n",
      "135787415 0.21 0.38\n",
      "80052 0.76 0.88\n",
      "139027236 0.97 1.0\n",
      "0.009666666666666667\n",
      "200061 0.01 0.0\n",
      "27003 0.03 0.02\n",
      "135787376 0.02 0.08\n",
      "135702870 0.13 0.17\n",
      "135344159 0.21 0.2\n",
      "13574080010 0.41 0.4\n",
      "136411025 0.71 0.71\n",
      "9519050010 0.96 1.0\n",
      "0.0097\n",
      "280023 0.01 0.0\n",
      "139365837 0.06 0.04\n",
      "135698200 0.1 0.09\n",
      "135787319 0.06 0.11\n",
      "135787078 0.51 0.34\n",
      "137547458 0.16 0.4\n",
      "1234194 0.61 0.76\n",
      "139146791 0.88 0.95\n",
      "0.009733333333333333\n",
      "2500225 0.02 0.0\n",
      "2000136 0.02 0.02\n",
      "135787379 0.06 0.09\n",
      "135703529 0.14 0.17\n",
      "135841059 0.27 0.21\n",
      "300144 0.24 0.56\n",
      "139077723 0.75 0.68\n",
      "600178 0.93 1.0\n",
      "0.009766666666666667\n",
      "2000159 0.02 0.0\n",
      "139189633 0.1 0.04\n",
      "135751783 0.03 0.07\n",
      "138173604 0.06 0.13\n",
      "139365788 0.15 0.24\n",
      "136134445 0.43 0.45\n",
      "136410944 0.65 0.65\n",
      "4953050010 0.97 0.99\n",
      "0.0098\n",
      "27005 0.01 0.0\n",
      "139027882 0.02 0.03\n",
      "135702930 0.06 0.1\n",
      "135345955 0.1 0.14\n",
      "135703825 0.34 0.33\n",
      "3588050040 0.59 0.36\n",
      "135703970 0.79 0.82\n",
      "310040 0.97 1.0\n",
      "0.009833333333333333\n",
      "141018592 0.01 0.0\n",
      "200175 0.02 0.02\n",
      "135702889 0.12 0.09\n",
      "135840979 0.3 0.15\n",
      "139025835 0.15 0.21\n",
      "5757050010 0.59 0.6\n",
      "600193 0.82 0.76\n",
      "9565110040 0.92 0.97\n",
      "0.009866666666666668\n",
      "139379864 0.03 0.0\n",
      "139025830 0.01 0.02\n",
      "27002 0.09 0.05\n",
      "4334050030 0.13 0.11\n",
      "10406060030 0.59 0.26\n",
      "9974080020 0.34 0.36\n",
      "800199 0.8 0.84\n",
      "5530050040 0.97 1.0\n",
      "0.0099\n",
      "200011 0.07 0.0\n",
      "135224713 0.02 0.05\n",
      "8725110010 0.03 0.06\n",
      "139186793 0.22 0.11\n",
      "135702944 0.17 0.28\n",
      "139146774 0.13 0.4\n",
      "13826060040 0.55 0.74\n",
      "136134476 0.91 0.96\n",
      "0.009933333333333334\n",
      "139025658 0.01 0.0\n",
      "200066 0.04 0.01\n",
      "139077858 0.21 0.08\n",
      "200042 0.05 0.12\n",
      "138872473 0.21 0.32\n",
      "137517101 0.16 0.39\n",
      "138872495 0.59 0.66\n",
      "200121 0.86 1.0\n",
      "0.009966666666666667\n",
      "2000126 0.1 0.0\n",
      "11046110030 0.02 0.02\n",
      "138872774 0.03 0.06\n",
      "135680214 0.14 0.11\n",
      "300190 0.14 0.34\n",
      "135703870 0.52 0.53\n",
      "1234176 0.79 0.86\n",
      "139201858 0.7 1.0\n",
      "0.01\n",
      "5373050040 0.02 0.0\n",
      "139077439 0.06 0.04\n",
      "135702369 0.11 0.09\n",
      "139048435 0.23 0.2\n",
      "135702944 0.18 0.28\n",
      "9807110010 0.35 0.4\n",
      "4637080040 0.45 0.69\n",
      "8205080040 0.96 1.0\n",
      "0.010033333333333335\n",
      "2100176 0.01 0.0\n",
      "135780055 0.06 0.04\n",
      "135724972 0.09 0.06\n",
      "135840971 0.18 0.17\n",
      "2221080011 0.35 0.28\n",
      "135191261 0.3 0.43\n",
      "139179730 0.8 0.67\n",
      "3189050020 0.97 1.0\n",
      "0.010066666666666666\n",
      "139089926 0.04 0.0\n",
      "138872516 0.05 0.03\n",
      "800191 0.04 0.06\n",
      "135787294 0.1 0.13\n",
      "140786039 0.48 0.28\n",
      "136410955 0.41 0.35\n",
      "1234176 0.79 0.86\n",
      "7337060030 0.94 1.0\n",
      "0.0101\n",
      "1234168 0.01 0.0\n",
      "135703228 0.05 0.05\n",
      "139160447 0.04 0.06\n",
      "135680829 0.25 0.18\n",
      "135697523 0.22 0.23\n",
      "139179709 0.42 0.37\n",
      "135672796 0.89 0.88\n",
      "200167 0.96 1.0\n",
      "0.010133333333333334\n",
      "137535073 0.01 0.0\n",
      "135787435 0.04 0.03\n",
      "139077858 0.21 0.08\n",
      "139319379 0.16 0.1\n",
      "138302779 0.4 0.34\n",
      "135704101 0.42 0.41\n",
      "6832110020 0.6 0.88\n",
      "80066 0.95 1.0\n",
      "0.010166666666666666\n",
      "2347050010 0.03 0.0\n",
      "137535092 0.02 0.02\n",
      "135809729 0.07 0.1\n",
      "200234 0.07 0.12\n",
      "30035 0.33 0.26\n",
      "135787036 0.64 0.58\n",
      "135787654 0.57 0.67\n",
      "136134656 0.96 1.0\n",
      "0.0102\n",
      "139178427 0.01 0.0\n",
      "139201895 0.09 0.04\n",
      "139025759 0.18 0.06\n",
      "135703877 0.36 0.13\n",
      "135191374 0.26 0.31\n",
      "136029544 0.28 0.37\n",
      "135703988 0.65 0.88\n",
      "8455050040 0.95 0.93\n",
      "0.010233333333333334\n",
      "139089891 0.01 0.0\n",
      "270034 0.03 0.02\n",
      "136134530 0.18 0.06\n",
      "141018076 0.08 0.1\n",
      "136029732 0.13 0.26\n",
      "139046457 0.29 0.37\n",
      "139365764 0.58 0.66\n",
      "139046274 0.96 1.0\n",
      "0.010266666666666667\n",
      "1234212 0.08 0.0\n",
      "135787438 0.04 0.05\n",
      "139052582 0.08 0.06\n",
      "139089900 0.13 0.16\n",
      "139048986 0.22 0.32\n",
      "138152110 0.36 0.43\n",
      "135704456 0.82 0.75\n",
      "290051 0.98 1.0\n",
      "0.0103\n",
      "10147080040 0.02 0.0\n",
      "135703950 0.07 0.02\n",
      "135703184 0.12 0.07\n",
      "400249 0.06 0.14\n",
      "135505956 0.05 0.2\n",
      "8393080040 0.43 0.4\n",
      "135787653 0.84 0.83\n",
      "137587748 0.97 1.0\n",
      "0.010333333333333335\n",
      "135804074 0.01 0.0\n",
      "139027276 0.05 0.01\n",
      "135774796 0.1 0.09\n",
      "135787336 0.07 0.15\n",
      "13216110041 0.51 0.32\n",
      "139186879 0.37 0.41\n",
      "135804395 0.66 0.7\n",
      "8541050010 0.97 1.0\n",
      "0.010366666666666666\n",
      "139208166 0.03 0.0\n",
      "9213110040 0.05 0.03\n",
      "139161827 0.06 0.07\n",
      "135732327 0.21 0.15\n",
      "200243 0.16 0.23\n",
      "6350060040 0.77 0.55\n",
      "3760050040 0.64 0.75\n",
      "1234104 0.94 1.0\n",
      "0.010400000000000001\n",
      "2000139 0.09 0.0\n",
      "139292058 0.09 0.05\n",
      "137532577 0.13 0.09\n",
      "9190110041 0.06 0.11\n",
      "139077807 0.29 0.26\n",
      "80099 0.46 0.57\n",
      "138872402 0.73 0.75\n",
      "140786031 0.95 1.0\n",
      "0.010433333333333333\n",
      "2000114 0.03 0.0\n",
      "200092 0.02 0.02\n",
      "135732259 0.11 0.07\n",
      "138173604 0.05 0.13\n",
      "139207917 0.12 0.24\n",
      "135191261 0.31 0.43\n",
      "135703853 0.74 0.84\n",
      "6804060010 0.97 0.99\n",
      "0.010466666666666666\n",
      "2800100 0.01 0.0\n",
      "600103 0.05 0.05\n",
      "139379613 0.07 0.08\n",
      "139025741 0.08 0.13\n",
      "135704007 0.21 0.26\n",
      "7897110010 0.2 0.4\n",
      "8587050010 0.82 0.77\n",
      "14847080040 0.97 0.97\n",
      "0.0105\n",
      "2600201 0.01 0.0\n",
      "139077430 0.02 0.03\n",
      "135505869 0.11 0.07\n",
      "9190110041 0.08 0.11\n",
      "138900665 0.33 0.27\n",
      "139027211 0.35 0.43\n",
      "138872402 0.72 0.75\n",
      "139146685 0.98 1.0\n",
      "0.010533333333333332\n",
      "200019 0.03 0.0\n",
      "135732261 0.03 0.04\n",
      "135787447 0.02 0.07\n",
      "135681029 0.23 0.15\n",
      "135703805 0.4 0.31\n",
      "135787634 0.21 0.39\n",
      "141018059 0.63 0.68\n",
      "300126 0.98 1.0\n",
      "0.010566666666666667\n",
      "139291965 0.01 0.0\n",
      "139046352 0.03 0.03\n",
      "135809765 0.07 0.08\n",
      "138900715 0.03 0.15\n",
      "135704007 0.21 0.26\n",
      "138872357 0.41 0.46\n",
      "141019171 0.79 0.84\n",
      "8694060010 0.98 1.0\n",
      "0.0106\n",
      "135943388 0.01 0.0\n",
      "2000137 0.05 0.01\n",
      "135732488 0.16 0.08\n",
      "135703862 0.1 0.13\n",
      "9744110021 0.38 0.28\n",
      "135654684 0.3 0.36\n",
      "139379934 0.66 0.7\n",
      "1234017 0.98 1.0\n",
      "0.010633333333333333\n",
      "200082 0.08 0.0\n",
      "11021110040 0.04 0.01\n",
      "139047002 0.08 0.07\n",
      "135732440 0.07 0.13\n",
      "135787078 0.58 0.34\n",
      "5757050010 0.63 0.6\n",
      "1234112 0.36 0.86\n",
      "4953050010 0.97 0.99\n",
      "0.010666666666666666\n",
      "1234203 0.11 0.0\n",
      "135732406 0.05 0.02\n",
      "40066 0.06 0.05\n",
      "135702947 0.08 0.15\n",
      "135803813 0.15 0.24\n",
      "135751791 0.33 0.37\n",
      "8008 0.81 0.77\n",
      "139161869 0.91 0.93\n",
      "0.010700000000000001\n",
      "140750849 0.47 0.0\n",
      "139146795 0.03 0.03\n",
      "139291487 0.05 0.05\n",
      "139319937 0.11 0.11\n",
      "136029604 0.36 0.3\n",
      "138302777 0.53 0.52\n",
      "135724910 0.71 0.7\n",
      "135703912 0.93 1.0\n",
      "0.010733333333333333\n",
      "135943388 0.01 0.0\n",
      "135697619 0.04 0.01\n",
      "139025660 0.11 0.08\n",
      "135191354 0.36 0.19\n",
      "135787078 0.54 0.34\n",
      "136029544 0.29 0.37\n",
      "800193 0.85 0.81\n",
      "137532557 0.97 1.0\n",
      "0.010766666666666667\n",
      "139178362 0.02 0.0\n",
      "2000164 0.05 0.02\n",
      "139025755 0.04 0.08\n",
      "135703800 0.06 0.18\n",
      "137517146 0.21 0.2\n",
      "3394110030 0.27 0.41\n",
      "139077930 0.62 0.64\n",
      "135787639 0.95 0.9\n",
      "0.0108\n",
      "139320425 0.05 0.0\n",
      "139025823 0.02 0.03\n",
      "800174 0.26 0.09\n",
      "136134634 0.04 0.16\n",
      "300190 0.18 0.34\n",
      "135787082 0.4 0.42\n",
      "20085 0.85 0.88\n",
      "137517091 0.94 0.99\n",
      "0.010833333333333332\n",
      "139146665 0.05 0.0\n",
      "135840873 0.05 0.03\n",
      "138947960 0.05 0.06\n",
      "11107080030 0.06 0.12\n",
      "135703845 0.24 0.28\n",
      "135703801 0.69 0.47\n",
      "138872498 0.93 0.88\n",
      "135680051 0.96 1.0\n",
      "0.010866666666666667\n",
      "139291965 0.01 0.0\n",
      "600103 0.05 0.05\n",
      "139397802 0.03 0.08\n",
      "139342333 0.17 0.16\n",
      "139048875 0.02 0.23\n",
      "5757050010 0.64 0.6\n",
      "12689060010 0.78 0.79\n",
      "1234084 0.97 1.0\n",
      "0.010900000000000002\n",
      "280096 0.02 0.0\n",
      "135703950 0.08 0.02\n",
      "135724919 0.11 0.1\n",
      "139058217 0.08 0.1\n",
      "7898050010 0.43 0.3\n",
      "139077494 0.38 0.53\n",
      "9350060040 0.79 0.74\n",
      "12701050010 0.94 1.0\n",
      "0.010933333333333333\n",
      "139077797 0.01 0.0\n",
      "139077434 0.01 0.01\n",
      "7577110010 0.03 0.08\n",
      "135732333 0.13 0.11\n",
      "137532539 0.32 0.33\n",
      "135703801 0.72 0.47\n",
      "4614110040 0.89 0.78\n",
      "3340060010 0.96 1.0\n",
      "0.010966666666666666\n",
      "139319946 0.05 0.0\n",
      "135780055 0.06 0.04\n",
      "139323565 0.06 0.06\n",
      "11229080030 0.11 0.17\n",
      "141018130 0.3 0.33\n",
      "136134445 0.46 0.45\n",
      "4736050030 0.86 0.81\n",
      "300049 0.92 1.0\n",
      "0.011000000000000001\n",
      "139179869 0.03 0.0\n",
      "135738185 0.17 0.03\n",
      "13694110020 0.05 0.06\n",
      "139089892 0.05 0.14\n",
      "135704086 0.28 0.27\n",
      "139179727 0.4 0.45\n",
      "136411025 0.69 0.71\n",
      "9587080020 0.97 0.98\n",
      "0.011033333333333332\n",
      "8003 0.01 0.0\n",
      "136410874 0.02 0.02\n",
      "27002 0.12 0.05\n",
      "135703989 0.22 0.17\n",
      "135703983 0.34 0.24\n",
      "136318424 0.58 0.57\n",
      "136410929 0.56 0.64\n",
      "135732368 0.88 1.0\n",
      "0.011066666666666667\n",
      "141370751 0.11 0.0\n",
      "139319948 0.04 0.03\n",
      "135344146 0.12 0.06\n",
      "9190110041 0.05 0.11\n",
      "139027207 0.16 0.21\n",
      "136029622 0.4 0.43\n",
      "135732253 0.49 0.85\n",
      "290051 0.98 1.0\n",
      "0.0111\n",
      "2600214 0.01 0.0\n",
      "135787058 0.04 0.03\n",
      "135840882 0.09 0.05\n",
      "600208 0.24 0.12\n",
      "135680925 0.27 0.29\n",
      "10828060020 0.63 0.4\n",
      "12069060030 0.91 0.73\n",
      "136410958 0.82 0.91\n",
      "0.011133333333333334\n",
      "135804074 0.01 0.0\n",
      "140750835 0.12 0.04\n",
      "139025759 0.21 0.06\n",
      "139077952 0.18 0.12\n",
      "135803813 0.13 0.24\n",
      "8847110020 0.54 0.58\n",
      "8901080010 0.76 0.64\n",
      "137891119 0.95 1.0\n",
      "0.011166666666666667\n",
      "8687050010 0.02 0.0\n",
      "139146664 0.02 0.01\n",
      "800180 0.17 0.09\n",
      "135703091 0.04 0.16\n",
      "139027207 0.15 0.21\n",
      "136029544 0.27 0.37\n",
      "141018059 0.58 0.68\n",
      "1234186 0.91 1.0\n",
      "0.011200000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137535073 0.01 0.0\n",
      "135787435 0.04 0.03\n",
      "9353050030 0.03 0.06\n",
      "137517095 0.14 0.11\n",
      "139379486 0.54 0.3\n",
      "139178384 0.44 0.52\n",
      "135787281 0.78 0.73\n",
      "7574080041 0.97 1.0\n",
      "0.011233333333333333\n",
      "2000122 0.09 0.0\n",
      "139027173 0.03 0.03\n",
      "135680099 0.16 0.09\n",
      "13543110010 0.13 0.18\n",
      "139342340 0.25 0.24\n",
      "5101050010 0.47 0.4\n",
      "1234159 0.66 0.86\n",
      "310039 0.72 1.0\n",
      "0.011266666666666668\n",
      "2000109 0.01 0.0\n",
      "139379751 0.06 0.05\n",
      "139048878 0.17 0.06\n",
      "135787430 0.2 0.13\n",
      "138872338 0.26 0.26\n",
      "137517062 0.65 0.58\n",
      "137535114 0.81 0.84\n",
      "135703907 0.96 0.99\n",
      "0.0113\n",
      "139070804 0.28 0.0\n",
      "2000136 0.02 0.02\n",
      "139189017 0.18 0.09\n",
      "135224777 0.11 0.15\n",
      "135787358 0.18 0.31\n",
      "9807110010 0.38 0.4\n",
      "137517104 0.7 0.79\n",
      "137517117 0.93 1.0\n",
      "0.011333333333333332\n",
      "600104 0.08 0.0\n",
      "137532541 0.04 0.03\n",
      "135787352 0.05 0.08\n",
      "135703289 0.14 0.14\n",
      "139027215 0.31 0.31\n",
      "135703794 0.46 0.44\n",
      "135703990 0.62 0.78\n",
      "137517139 0.94 1.0\n",
      "0.011366666666666667\n",
      "139379564 0.01 0.0\n",
      "139025636 0.02 0.04\n",
      "139161946 0.12 0.09\n",
      "7524080040 0.25 0.16\n",
      "139052583 0.33 0.29\n",
      "135703283 0.46 0.49\n",
      "9394050040 0.71 0.91\n",
      "3189050020 0.97 1.0\n",
      "0.011399999999999999\n",
      "139046494 0.01 0.0\n",
      "210055 0.01 0.01\n",
      "135732445 0.11 0.06\n",
      "139341901 0.37 0.18\n",
      "135702944 0.22 0.28\n",
      "139027262 0.5 0.55\n",
      "80052 0.85 0.88\n",
      "140789991 0.85 0.97\n",
      "0.011433333333333334\n",
      "139070805 0.04 0.0\n",
      "139058219 0.06 0.01\n",
      "139052582 0.09 0.06\n",
      "135191288 0.21 0.17\n",
      "135809900 0.34 0.27\n",
      "2328110010 0.39 0.39\n",
      "139178449 0.81 0.67\n",
      "2569060030 0.97 1.0\n",
      "0.011466666666666667\n",
      "1234167 0.01 0.0\n",
      "135732343 0.05 0.03\n",
      "135809729 0.06 0.1\n",
      "60096 0.04 0.1\n",
      "136434400 0.29 0.24\n",
      "137517081 0.5 0.41\n",
      "139179723 0.52 0.66\n",
      "137517064 0.91 0.94\n",
      "0.0115\n",
      "600131 0.01 0.0\n",
      "135787448 0.05 0.04\n",
      "137517276 0.07 0.08\n",
      "4819050010 0.31 0.17\n",
      "136029604 0.35 0.3\n",
      "139027262 0.53 0.55\n",
      "9559080010 0.72 0.65\n",
      "139077886 0.97 1.0\n",
      "0.011533333333333333\n",
      "14696060040 0.01 0.0\n",
      "2600220 0.01 0.03\n",
      "139027273 0.06 0.09\n",
      "900133 0.17 0.1\n",
      "2175080020 0.51 0.25\n",
      "139077779 0.3 0.49\n",
      "136410952 0.8 0.87\n",
      "136134476 0.91 0.96\n",
      "0.011566666666666668\n",
      "139146798 0.01 0.0\n",
      "200058 0.03 0.02\n",
      "139189017 0.23 0.09\n",
      "4180060010 0.18 0.17\n",
      "135344121 0.3 0.25\n",
      "135724952 0.66 0.59\n",
      "135704055 0.79 0.89\n",
      "136318438 0.93 0.91\n",
      "0.0116\n",
      "139420302 0.23 0.0\n",
      "200163 0.02 0.03\n",
      "135732326 0.14 0.07\n",
      "135787426 0.12 0.14\n",
      "135505956 0.08 0.2\n",
      "139178453 0.56 0.48\n",
      "139077870 0.84 0.82\n",
      "9835060010 0.96 0.99\n",
      "0.011633333333333334\n",
      "139025811 0.01 0.0\n",
      "800152 0.01 0.05\n",
      "135732466 0.12 0.07\n",
      "139077833 0.04 0.16\n",
      "8614110010 0.27 0.34\n",
      "139178080 0.62 0.57\n",
      "4614110040 0.86 0.78\n",
      "139077963 0.97 1.0\n",
      "0.011666666666666667\n",
      "2000124 0.09 0.0\n",
      "135698161 0.04 0.05\n",
      "135732297 0.1 0.08\n",
      "135345953 0.16 0.16\n",
      "139077410 0.21 0.21\n",
      "2487110030 0.44 0.42\n",
      "135724910 0.75 0.7\n",
      "139077804 0.95 1.0\n",
      "0.0117\n",
      "139430757 0.02 0.0\n",
      "139027970 0.03 0.01\n",
      "136410864 0.07 0.09\n",
      "135703877 0.39 0.13\n",
      "139048986 0.18 0.32\n",
      "4702110030 0.83 0.47\n",
      "500224 0.89 0.68\n",
      "135787389 0.96 1.0\n",
      "0.011733333333333333\n",
      "135804074 0.01 0.0\n",
      "135787303 0.02 0.01\n",
      "135732433 0.11 0.09\n",
      "135703486 0.05 0.11\n",
      "7031080010 0.46 0.33\n",
      "135702865 0.34 0.45\n",
      "136752977 0.88 0.84\n",
      "2115050020 0.95 0.99\n",
      "0.011766666666666668\n",
      "139077797 0.01 0.0\n",
      "135732335 0.07 0.04\n",
      "12609060020 0.03 0.06\n",
      "3091080010 0.41 0.13\n",
      "135680808 0.19 0.22\n",
      "138948353 0.36 0.37\n",
      "135732253 0.36 0.85\n",
      "300052 0.98 1.0\n",
      "0.0118\n",
      "7138110030 0.02 0.0\n",
      "600113 0.02 0.01\n",
      "135344027 0.03 0.08\n",
      "139077748 0.09 0.15\n",
      "139379486 0.54 0.3\n",
      "135751791 0.27 0.37\n",
      "14849060040 0.57 0.62\n",
      "139077951 0.97 1.0\n",
      "0.011833333333333335\n",
      "40078 0.06 0.0\n",
      "135732288 0.02 0.03\n",
      "137862276 0.05 0.06\n",
      "135732330 0.14 0.11\n",
      "139025822 0.2 0.23\n",
      "139669334 0.35 0.45\n",
      "139077930 0.6 0.64\n",
      "300127 0.97 1.0\n",
      "0.011866666666666668\n",
      "140750832 0.02 0.0\n",
      "139319981 0.01 0.05\n",
      "135224897 0.36 0.09\n",
      "135681019 0.05 0.18\n",
      "11353110030 0.37 0.29\n",
      "139077564 0.49 0.45\n",
      "20085 0.86 0.88\n",
      "139178376 0.97 1.0\n",
      "0.011899999999999999\n",
      "270019 0.01 0.0\n",
      "2000136 0.02 0.02\n",
      "800175 0.03 0.05\n",
      "135804089 0.06 0.18\n",
      "135703805 0.42 0.31\n",
      "136318443 0.51 0.56\n",
      "136411025 0.8 0.71\n",
      "290070 0.98 1.0\n",
      "0.011933333333333334\n",
      "139208187 0.02 0.0\n",
      "200066 0.03 0.01\n",
      "2808110030 0.1 0.09\n",
      "135703800 0.08 0.18\n",
      "139319944 0.15 0.23\n",
      "135703915 0.46 0.54\n",
      "290059 0.95 0.85\n",
      "5074080020 0.91 0.97\n",
      "0.011966666666666667\n",
      "2500227 0.02 0.0\n",
      "140750835 0.06 0.04\n",
      "31005 0.08 0.07\n",
      "135702870 0.12 0.17\n",
      "138872634 0.2 0.34\n",
      "135804072 0.57 0.58\n",
      "6346050031 0.75 0.66\n",
      "30017 0.96 1.0\n",
      "0.012\n",
      "200061 0.01 0.0\n",
      "136410845 0.03 0.02\n",
      "135703058 0.11 0.07\n",
      "139025439 0.05 0.13\n",
      "139025835 0.18 0.21\n",
      "3896060010 0.42 0.35\n",
      "135703235 0.84 0.87\n",
      "200241 0.94 1.0\n",
      "0.012033333333333333\n",
      "2600204 0.01 0.0\n",
      "135751776 0.03 0.04\n",
      "135703886 0.06 0.06\n",
      "4180060010 0.13 0.17\n",
      "137517075 0.22 0.22\n",
      "138900783 0.45 0.42\n",
      "139179723 0.48 0.66\n",
      "9466050040 0.94 0.97\n",
      "0.012066666666666668\n",
      "270028 0.03 0.0\n",
      "4171110010 0.08 0.02\n",
      "139341810 0.07 0.08\n",
      "139027225 0.12 0.11\n",
      "400235 0.57 0.31\n",
      "136029695 0.27 0.37\n",
      "138872353 0.52 0.73\n",
      "139177982 0.95 1.0\n",
      "0.0121\n",
      "139070804 0.72 0.0\n",
      "9213110040 0.04 0.03\n",
      "4793110041 0.07 0.08\n",
      "136134573 0.24 0.2\n",
      "135698246 0.32 0.25\n",
      "137526936 0.45 0.4\n",
      "135787344 0.66 0.78\n",
      "9993060020 0.98 1.0\n",
      "0.012133333333333335\n",
      "139025804 0.03 0.0\n",
      "135787267 0.07 0.01\n",
      "31005 0.11 0.07\n",
      "11229080030 0.09 0.17\n",
      "138152120 0.21 0.35\n",
      "3896060010 0.44 0.35\n",
      "1234194 0.62 0.76\n",
      "310034 0.9 1.0\n",
      "0.012166666666666666\n",
      "2000108 0.03 0.0\n",
      "11046110030 0.01 0.02\n",
      "135672824 0.09 0.09\n",
      "4438050040 0.13 0.13\n",
      "135703027 0.2 0.21\n",
      "8797060010 0.36 0.42\n",
      "13826060040 0.58 0.74\n",
      "137517117 0.92 1.0\n",
      "0.0122\n",
      "139077894 0.01 0.0\n",
      "139077442 0.04 0.02\n",
      "139380041 0.12 0.06\n",
      "139077739 0.44 0.15\n",
      "135787269 0.22 0.31\n",
      "138302777 0.5 0.52\n",
      "135703988 0.65 0.88\n",
      "1234161 0.86 1.0\n",
      "0.012233333333333334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mt/jz1fs5g94191qf3jz9flw2vw0000gq/T/ipykernel_29038/909225048.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                      \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                                      \u001b[0mbeta_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                                      \u001b[0mft_lr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mft_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                                      })\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### from tqdm.notebook import trange\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "\n",
    "best_val = 0.72\n",
    "fine_tune = False\n",
    "ft_epochs = 0\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# loss2 125-160 is 0.4 alpha, >0.6 surface, 0.33 loss weight\n",
    "# loss45 is 0.4 alpha, >0.45 surface, 0.4 loss weight\n",
    "# loss45 250 - 300 is 0.4 alpha, >0.45 surface, 0.4 loss weight with minimum surface loss\n",
    "\n",
    "SWA = False\n",
    "for i in range(1, 10):\n",
    "    al = 0.3\n",
    "    ft_steps = (i - 1) * 240\n",
    "    ft_learning_rate = 0.02\n",
    "    #if nepochs < 5:\n",
    "     #   ft_learning_rate *= (0.2 * nepochs)\n",
    "    be = 0.0\n",
    "    test_al = al\n",
    "    #random.shuffle(train_xs)\n",
    "    op = train_op# if fine_tune else train_op\n",
    "    \n",
    "    print(f\"starting epoch {i}, \" \n",
    "          f\"alpha: {al}, beta: {be}, \"\n",
    "          f\"drop: {np.max(((1. - (i * 0.005)), 0.6))} \"\n",
    "          f\"Learning rate: {ft_learning_rate}\"\n",
    "         )\n",
    "    \n",
    "    loss = train_loss\n",
    "    losses = []\n",
    "    \n",
    "    for k in range(0, 240*8, 8):\n",
    "        ft_steps += 1\n",
    "        if ft_steps < 600:\n",
    "            ft_learning_rate = (ft_steps / 600) * 2e-2\n",
    "            print(ft_learning_rate)\n",
    "        else:\n",
    "            ft_learning_rate = 2e-2\n",
    "        #try:\n",
    "        x_batch = np.zeros((8, 5, 124, 124, 17), dtype = np.float32)\n",
    "        y_batch = np.zeros((8, 110, 110), dtype = np.float32)\n",
    "        mask_batch = np.zeros_like(y_batch)\n",
    "        \n",
    "        tochoose = [zeros, fives, tens, twenties, thirties, forties, fifties, seventies]\n",
    "        old_batch = np.copy(np.array(batches))\n",
    "        batches = []\n",
    "        for i in range(0, 8):\n",
    "            #print(train_xs[k + i])\n",
    "            rng = np.random.randint(len(tochoose[i]))\n",
    "            sample = train_xs[tochoose[i][rng]]\n",
    "            batches.append(sample)\n",
    "            x_batcha, y_batcha = load_and_augment_xy(x_path, y_path, sample)\n",
    "            y_batcha = np.pad(y_batcha, (48, 48), 'constant', constant_values=(0, 0))\n",
    "            mask_batcha = np.zeros_like(y_batcha)\n",
    "            mask_batcha[48:-48, 48:-48] = 1.\n",
    "            x_batch[i] = x_batcha\n",
    "            y_batch[i] = y_batcha\n",
    "            mask_batch[i] = mask_batcha\n",
    "        y_batch[y_batch > 0.9] = 1.\n",
    "        #x_batch = x_batch[:, :, 48:-48, 48:-48]\n",
    "        #y_batch = y_batch[:, 48:-48, 48:-48]\n",
    "        #mask_batch = np.ones_like(y_batch)\n",
    "        #print(x_batch.shape, y_batch.shape, mask_batch.shape)\n",
    "        out = sess.run([op, fm], #\n",
    "                          feed_dict={inp: x_batch,\n",
    "                                     length: np.full((8,), 4),\n",
    "                                     labels: y_batch,\n",
    "                                     mask: mask_batch,\n",
    "                                     is_training: True,\n",
    "                                     loss_weight: 1.,\n",
    "                                     keep_rate: 0.75,\n",
    "                                     alpha: al,\n",
    "                                     beta_: be,\n",
    "                                     ft_lr: ft_learning_rate,\n",
    "                                     })\n",
    "    \n",
    "        predmean = np.mean(out[1][:, 48:-48, 48:-48].squeeze(), axis = (1, 2))\n",
    "        labelmean = np.mean(y_batch[:, 48:-48, 48:-48].squeeze(), axis = (1, 2))\n",
    "        predmean = np.around(predmean, 2)\n",
    "        labelmean = np.around(labelmean, 2)\n",
    "        for a, b, c in zip(np.array(batches), predmean, labelmean):\n",
    "            print(a, b, c)\n",
    "        ##losses.append(tr)\n",
    "        #except KeyboardInterrupt:\n",
    "        #        print('Interrupted')\n",
    "        #        break\n",
    "        #except:\n",
    "        #    continue\n",
    "    \n",
    "    print(f\"Epoch {i}: Loss {np.around(np.mean(losses[:-1]), 3)}\")\n",
    "    #saver.save(sess, f\"../models/epoch30-{str(i)}/model\")\n",
    "    output_node_names = ['conv2d/Sigmoid']\n",
    "    frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "        sess,\n",
    "        sess.graph_def,\n",
    "        output_node_names)\n",
    "\n",
    "\n",
    "    # Save the frozen graph\n",
    "    with open(f'../models/tmp/predict_graph-{str(i)}.pb', 'wb') as f:\n",
    "        f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_node_names = ['conv2d/Sigmoid']\n",
    "frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess,\n",
    "    sess.graph_def,\n",
    "    output_node_names)\n",
    "\n",
    "\n",
    "# Save the frozen graph\n",
    "with open(f'../models/tmp/predict_graph-{str(i)}.pb', 'wb') as f:\n",
    "    f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "l = sns.heatmap(out[0][idx].squeeze(), vmin = 0.0, vmax = 1)\n",
    "l.set_title(old_batch[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_individual_sample(fpath, ypath, f):\n",
    "    ishkl = os.path.exists(fpath + f + '.hkl')\n",
    "    if ishkl:\n",
    "        x = hkl.load(fpath + f + '.hkl') / 65535\n",
    "    else:\n",
    "        x = np.load(fpath + f + \".npy\") / 65535\n",
    "    img = rs.open(fpath + f + \".tif\").read()\n",
    "    print(fpath + f + \".hkl\")\n",
    "    print(np.sum(x))\n",
    "    img = np.moveaxis(img, 0, 2)\n",
    "    #img = np.swapaxes(img, 0, 1)\n",
    "    #x = x[..., :13]\n",
    "    if x.shape[-1] == 13:\n",
    "        i = make_and_smooth_indices(x)\n",
    "        out = np.zeros((x.shape[0], x.shape[1], x.shape[2], 17), dtype = np.float32)\n",
    "        out[..., :13] = x \n",
    "        out[..., 13:] = i\n",
    "    else:\n",
    "        out = x\n",
    "        out[..., -1] *= 2\n",
    "        out[..., -1] -= 0.7193834232943873\n",
    "        \n",
    "        #out[-1] -= 0.7193834232943873\n",
    "        out[..., -2] -= 0.09731556326714398\n",
    "        out[..., -3] -= 0.4973397113668104,\n",
    "        out[..., -4] -= 0.1409399364817101\n",
    "        #out[]\n",
    "    median = np.median(out, axis = 0)\n",
    "    print(np.sum(out))\n",
    "    out = np.reshape(out, (4, 3, out.shape[1], out.shape[2], out.shape[3]))\n",
    "    out = np.median(out, axis = 1, overwrite_input = True)\n",
    "    print(np.sum(out))\n",
    "    out = np.concatenate([out, median[np.newaxis]], axis = 0)\n",
    "    print(np.sum(out))\n",
    "    return normalize_subtile(out[:, 2:-2, 2:-2, :]), rs.open(ypath + f + \".tif\").read(1) / 255\n",
    "\n",
    "def load_and_augment_xy(x, y, f):\n",
    "    x, y = load_individual_sample(x, y, f)\n",
    "    #x, y = augment_single_sample(x, y)\n",
    "    return x, y\n",
    "\n",
    "x, y = load_and_augment_xy(x_path, y_path, str(135840882))\n",
    "np.sum(x, axis = (0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('batch_x.npy', x_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(swa_to_weights)\n",
    "saver = tf.train.Saver(max_to_keep = 150)\n",
    "#os.mkdir(f\"../models/loss2/\")\n",
    "save_path = saver.save(sess, f\"../models/loss2/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_node_names = ['conv2d_5/Sigmoid']\n",
    "frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess,\n",
    "    sess.graph_def,\n",
    "    output_node_names)\n",
    "\n",
    "\n",
    "# Save the frozen graph\n",
    "with open('../models/loss3/predict_graph.pb', 'wb') as f:\n",
    "    f.write(frozen_graph_def.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
