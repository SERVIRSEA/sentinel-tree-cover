{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-64156d691fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/Documents/GitHub/sentinel-tree-cover/src/layers/convgru.py:156: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ../../src/layers/zoneout.py\n",
    "%run ../../src/layers/adabound.py\n",
    "%run ../../src/layers/convgru.py\n",
    "%run ../../src/layers/dropblock.py\n",
    "%run ../../src/layers/extra_layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cse_block(prevlayer, prefix):\n",
    "    '''Channel excitation and spatial squeeze layer. \n",
    "       Calculates the mean of the spatial dimensions and then learns\n",
    "       two dense layers, one with relu, and one with sigmoid, to rerank the\n",
    "       input channels\n",
    "       \n",
    "         Parameters:\n",
    "          prevlayer (tf.Variable): input layer\n",
    "          prefix (str): prefix for tensorflow scope\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): output of the cse_block\n",
    "    '''\n",
    "    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n",
    "    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + 'cse_lin1', activation='relu')(mean)\n",
    "    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + 'cse_lin2', activation='sigmoid')(lin1)\n",
    "    x = Multiply()([prevlayer, lin2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def sse_block(prevlayer, prefix):\n",
    "    '''Spatial excitation and channel squeeze layer.\n",
    "       Calculates a 1x1 convolution with sigmoid activation to create a \n",
    "       spatial map that is multiplied by the input layer\n",
    "\n",
    "         Parameters:\n",
    "          prevlayer (tf.Variable): input layer\n",
    "          prefix (str): prefix for tensorflow scope\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): output of the sse_block\n",
    "    '''\n",
    "    conv = Conv2D(1, (1, 1), padding=\"same\", kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "def csse_block(x, prefix):\n",
    "    '''Implementation of Concurrent Spatial and Channel \n",
    "       ‘Squeeze & Excitation’ in Fully Convolutional Networks\n",
    "    \n",
    "        Parameters:\n",
    "          prevlayer (tf.Variable): input layer\n",
    "          prefix (str): prefix for tensorflow scope\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): added output of cse and sse block\n",
    "          \n",
    "         References:\n",
    "          https://arxiv.org/abs/1803.02579\n",
    "    '''\n",
    "    #cse = cse_block(x, prefix)\n",
    "    sse = sse_block(x, prefix)\n",
    "    #x = Add(name=prefix + \"_csse_mul\")([cse, sse])\n",
    "\n",
    "    return sse\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        print(\"ZERO PADDING\")\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "    \n",
    "class ReflectionPadding5D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding5D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        print(\"ZERO PADDING\")\n",
    "        return tf.pad(x, [[0,0], [0, 0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_block(inp, length, size, flt, scope, train, normalize = True):\n",
    "    '''Bidirectional convolutional GRU block with \n",
    "       zoneout and CSSE blocks in each time step\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): (B, T, H, W, C) layer\n",
    "          length (tf.Variable): (B, T) layer denoting number of\n",
    "                                steps per sample\n",
    "          size (int): kernel size of convolution\n",
    "          flt (int): number of convolution filters\n",
    "          scope (str): tensorflow variable scope\n",
    "          train (tf.Bool): flag to differentiate between train/test ops\n",
    "          normalize (bool): whether to compute layer normalization\n",
    "\n",
    "         Returns:\n",
    "          gru (tf.Variable): (B, H, W, flt*2) bi-gru output\n",
    "          steps (tf.Variable): (B, T, H, W, flt*2) output of each step\n",
    "    '''\n",
    "    with tf.variable_scope(scope):\n",
    "        print(f\"GRU input shape {inp.shape}, zoneout: {0.1}\")\n",
    "        \"\"\"\n",
    "        cell_fw = ConvLSTMCell(shape = size, filters = flt,\n",
    "                               kernel = [3, 3], forget_bias=1.0, \n",
    "                               activation=tf.tanh, normalize=True, \n",
    "                               peephole=False, data_format='channels_last', reuse=None)\n",
    "        cell_bw = ConvLSTMCell(shape = size, filters = flt,\n",
    "                               kernel = [3, 3], forget_bias=1.0, \n",
    "                               activation=tf.tanh, normalize=True, \n",
    "                               peephole=False, data_format='channels_last', reuse=None)\n",
    "        \"\"\"\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', normalize = normalize, sse = True)\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', normalize = normalize, sse = True)\n",
    "        zoneout = 0.75\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "           cell_fw, zoneout_drop_prob = zoneout, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = zoneout, is_training = train)\n",
    "        print(inp.shape)\n",
    "        steps, out = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        print(f\"Zoneout: {zoneout}\")\n",
    "        gru = tf.concat(out, axis = -1)\n",
    "        steps = tf.concat(steps, axis = -1)\n",
    "        print(f\"Down block output shape {gru.shape}\")\n",
    "    return gru, steps\n",
    "\n",
    "\n",
    "def attention(inp, units):\n",
    "    weighted = TimeDistributed(Conv2D(units, (1, 1), padding = 'same', kernel_initializer = tf.keras.initializers.Ones(),\n",
    "                            activation = 'sigmoid', strides = (1, 1), use_bias = False, ))(inp) \n",
    "    alphas = tf.reduce_sum(weighted, axis = 1, keep_dims = True)\n",
    "    alphas = weighted / alphas\n",
    "    multiplied = tf.reduce_sum(alphas * inp, axis = 1)\n",
    "    print(multiplied.shape)\n",
    "    return multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class WSConv2D(tf.keras.layers.Conv2D):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=(1, 1),\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='he_normal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(WSConv2D, self).__init__(filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 strides=strides,\n",
    "                 padding=padding,\n",
    "                 data_format=data_format,\n",
    "                 dilation_rate=dilation_rate,\n",
    "                 activation=activation,\n",
    "                 use_bias=use_bias,\n",
    "                 kernel_initializer=kernel_initializer,\n",
    "                 bias_initializer=bias_initializer,\n",
    "                 kernel_regularizer=kernel_regularizer,\n",
    "                 bias_regularizer=bias_regularizer,\n",
    "                 activity_regularizer=activity_regularizer,\n",
    "                 kernel_constraint=kernel_constraint,\n",
    "                 bias_constraint=bias_constraint,\n",
    "                 **kwargs)\n",
    "        \n",
    "    def standardize_weight(self, weight, eps):\n",
    "\n",
    "        mean = tf.math.reduce_mean(weight, axis=(0, 1, 2), keepdims=True)\n",
    "        weight = weight - mean\n",
    "        var = tf.keras.backend.std(weight, axis=[0, 1, 2], keepdims=True)\n",
    "        weight = weight / (var + 1e-5)\n",
    "        return weight\n",
    "\n",
    "    def call(self, inputs):\n",
    "        weight = self.standardize_weight(self.kernel, 1e-5)\n",
    "\n",
    "        outputs = K.conv2d(\n",
    "            inputs,\n",
    "            weight,\n",
    "            strides=self.strides,\n",
    "            padding=self.padding,\n",
    "            data_format=self.data_format,\n",
    "            dilation_rate=self.dilation_rate)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "class WSConv2D(tf.keras.layers.Conv2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(WSConv2D, self).__init__(kernel_initializer=\"he_normal\", *args, **kwargs)\n",
    "\n",
    "    def standardize_weight(self, weight, eps):\n",
    "\n",
    "        mean = tf.math.reduce_mean(weight, axis=(0, 1, 2), keepdims=True)\n",
    "        weight = weight - mean\n",
    "        var = tf.keras.backend.std(weight, axis=[0, 1, 2], keepdims=True)\n",
    "        weight = weight / (var + 1e-5)\n",
    "        return weight\n",
    "\n",
    "    def call(self, inputs, eps=1e-4):\n",
    "        self.kernel.assign(self.standardize_weight(self.kernel, eps))\n",
    "        return super().call(inputs)\n",
    "def partial_conv(x, channels, kernel=3, stride=1, norm = True, use_bias=False, padding='SAME', scope='conv_0'):\n",
    "    \n",
    "\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        if padding.lower() == 'SAME'.lower() :\n",
    "            with tf.variable_scope('mask'):\n",
    "                _, h, w, _ = x.get_shape().as_list()\n",
    "\n",
    "                slide_window = kernel * kernel\n",
    "                mask = tf.ones(shape=[1, h, w, 1])\n",
    "\n",
    "                update_mask = tf.layers.conv2d(mask, filters=1,\n",
    "                                               kernel_size=kernel, kernel_initializer=tf.constant_initializer(1.0),\n",
    "                                               strides=stride, padding=padding, use_bias=False, trainable=False)\n",
    "\n",
    "                mask_ratio = slide_window / (update_mask + 1e-8)\n",
    "                update_mask = tf.clip_by_value(update_mask, 0.0, 1.0)\n",
    "                mask_ratio = mask_ratio * update_mask\n",
    "\n",
    "            with tf.variable_scope('x'):\n",
    "                if 5 > 3:\n",
    "                    x = WSConv2D(filters=channels, kernel_regularizer = None,\n",
    "                                     kernel_size=kernel,\n",
    "                                     strides=stride, padding=padding, use_bias=False).apply(x)\n",
    "                else:\n",
    "                    x = tf.layers.conv2d(x, filters=channels, kernel_regularizer = None,\n",
    "                                     kernel_size=kernel, kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                     strides=stride, padding=padding, use_bias=False)\n",
    "    \n",
    "                x = x * mask_ratio\n",
    "                \n",
    "                \n",
    "\n",
    "                if use_bias:\n",
    "                    bias = tf.get_variable(\"bias\", [channels], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "                    x = tf.nn.bias_add(x, bias)\n",
    "                    x = x * update_mask\n",
    "\n",
    "        else :\n",
    "            if 5 > 3:\n",
    "                x = WSConv2D(filters=channels,kernel_regularizer = None,\n",
    "                                 kernel_size=kernel,\n",
    "                                 strides=stride, padding=padding, use_bias=use_bias).apply(x)\n",
    "            else:\n",
    "                x = tf.layers.conv2d(x, filters=channels,kernel_regularizer = None,\n",
    "                                 kernel_size=kernel, kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                 strides=stride, padding=padding, use_bias=use_bias)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "def conv_swish_gn(inp, \n",
    "                 is_training, \n",
    "                 kernel_size,\n",
    "                 scope,\n",
    "                 filters, \n",
    "                 keep_rate,\n",
    "                 stride = (1, 1),\n",
    "                 activation = True,\n",
    "                 use_bias = False,\n",
    "                 norm = True,\n",
    "                 dropblock = True,\n",
    "                 csse = True,\n",
    "                 weight_decay = None,\n",
    "                 block_size = 5,\n",
    "                 padding = \"SAME\",\n",
    "                 window_size = 48):\n",
    "    '''2D convolution, batch renorm, relu block, 3x3 drop block. \n",
    "       Use_bias must be set to False for batch normalization to work. \n",
    "       He normal initialization is used with batch normalization.\n",
    "       RELU is better applied after the batch norm.\n",
    "       DropBlock performs best when applied last, according to original paper.\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): input layer\n",
    "          is_training (str): flag to differentiate between train/test ops\n",
    "          kernel_size (int): size of convolution\n",
    "          scope (str): tensorflow variable scope\n",
    "          filters (int): number of filters for convolution\n",
    "          clipping_params (dict): specifies clipping of \n",
    "                                  rmax, dmax, rmin for renormalization\n",
    "          activation (bool): whether to apply RELU\n",
    "          use_bias (str): whether to use bias. Should always be false\n",
    "\n",
    "         Returns:\n",
    "          bn (tf.Variable): output of Conv2D -> Batch Norm -> RELU\n",
    "        \n",
    "         References:\n",
    "          http://papers.nips.cc/paper/8271-dropblock-a-regularization-\n",
    "              method-for-convolutional-networks.pdf\n",
    "          https://arxiv.org/abs/1702.03275\n",
    "          \n",
    "    '''\n",
    "    \n",
    "    bn_flag = \"Group Norm\" if norm else \"\"\n",
    "    activation_flag = \"RELU\" if activation else \"Linear\"\n",
    "    csse_flag = \"CSSE\" if csse else \"No CSSE\"\n",
    "    bias_flag = \"Bias\" if use_bias else \"NoBias\"\n",
    "    drop_flag = \"DropBlock\" if dropblock else \"NoDrop\"\n",
    "        \n",
    "    \n",
    "    print(\"{} {} Conv 2D {} {} {} {} {}\".format(scope, kernel_size,\n",
    "                                                   bn_flag, activation_flag,\n",
    "                                                   csse_flag, bias_flag, drop_flag))\n",
    "    \n",
    "    with tf.variable_scope(scope + \"_conv\"):\n",
    "        #conv = Conv2D(filters = filters, kernel_size = (kernel_size, kernel_size),  strides = stride,\n",
    "        #              activation = None, padding = 'valid', use_bias = use_bias,\n",
    "                      #kernel_regularizer = weight_decay,\n",
    "        #              kernel_initializer = tf.keras.initializers.he_normal())(inp)\n",
    "        conv = partial_conv(inp, filters, kernel=kernel_size, stride=1, \n",
    "                            use_bias=False, padding=padding, scope = scope)\n",
    "    if activation:\n",
    "        conv = tf.nn.swish(conv)\n",
    "    print(conv)\n",
    "    # if dropblock:\n",
    "    #.    mask = make_mask()\n",
    "    \n",
    "    if norm:\n",
    "        conv = group_norm(x = conv, scope = scope, G = 8,#(filters // 4), \n",
    "                          window_size = window_size) #mask = mask\n",
    "    if csse:\n",
    "        conv = csse_block(conv, \"csse_\" + scope)\n",
    "    \n",
    "    if dropblock: \n",
    "        with tf.variable_scope(scope + \"_drop\"):\n",
    "            drop_block = DropBlock2D(keep_prob=keep_rate, block_size= block_size)\n",
    "            conv = drop_block(conv, is_training)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 28 #634 #230\n",
    "SIZE_X = 28 #254# 230\n",
    "\n",
    "n_bands = 17\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 5, INPUT_SIZE, SIZE_X, n_bands))\n",
    "length = tf.placeholder_with_default(np.full((1,), 12), shape = (None,))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, INPUT_SIZE - 14, INPUT_SIZE - 14))#, 1))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training') # For BN, DropBlock\n",
    "alpha = tf.placeholder(tf.float32, shape = ()) # For loss scheduling\n",
    "ft_lr = tf.placeholder_with_default(0.001, shape = ()) # For loss scheduling\n",
    "loss_weight = tf.placeholder_with_default(1.0, shape = ())\n",
    "beta_ = tf.placeholder_with_default(0.0, shape = ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU input shape (?, 4, 28, 28, 17), zoneout: 0.1\n",
      "(?, 4, 28, 28, 17)\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "(3, 3, 49, 64)\n",
      "(3, 3, 49, 64)\n",
      "Zoneout: 0.75\n",
      "Down block output shape (?, 28, 28, 64)\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jbrandt.terminal/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "conv_median 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Tensor(\"IdentityN:0\", shape=(?, 28, 28, 64), dtype=float32)\n",
      "Median conv: (?, 28, 28, 64)\n",
      "conv_concat 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Tensor(\"IdentityN_1:0\", shape=(?, 28, 28, 64), dtype=float32)\n",
      "Concat: (?, 28, 28, 64)\n",
      "conv1 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Tensor(\"IdentityN_2:0\", shape=(?, 12, 12, 128), dtype=float32)\n",
      "Conv1: (?, 12, 12, 128)\n",
      "conv2 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Tensor(\"IdentityN_3:0\", shape=(?, 4, 4, 256), dtype=float32)\n",
      "Encoded (?, 4, 4, 256)\n",
      "up2 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Tensor(\"IdentityN_4:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "(?, 8, 8, 128)\n",
      "up2_out 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Tensor(\"IdentityN_5:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "up3 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Tensor(\"IdentityN_6:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "(?, 16, 16, 64)\n",
      "(?, 16, 16, 64)\n",
      "out 3 Conv 2D Group Norm RELU CSSE NoBias NoDrop\n",
      "Tensor(\"IdentityN_7:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
      "The output is (?, 14, 14, 64), with a receptive field of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"The output, sigmoid is {fm.shape}, with a receptive field of {1}\")\\n\\n\\n\\nup4 = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\\n                    kernel_size = 3, scope = \\'outregressor\\', filters = 64, \\n                    keep_rate  = keep_rate, activation = True, use_bias = False, norm = True,\\n                    csse = True, dropblock = True, weight_decay = None, padding = \"SAME\")\\n\\nup5 = conv_swish_gn(inp = up4, is_training = is_training, stride = (1, 1),\\n                    kernel_size = 3, scope = \\'outregressor2\\', filters = 64, \\n                    keep_rate  = keep_rate, activation = True, use_bias = False, norm = True,\\n                    csse = True, dropblock = False, weight_decay = None, padding = \"VALID\")\\n\\nregression = Conv2D(filters = 1,\\n            kernel_size = (1, 1),\\n            padding = \\'valid\\',\\n            activation = \\'linear\\',\\n            #bias_initializer = init,\\n           )(up5)\\nregression = Cropping2D(1)(regression)\\noutput = regression + fm\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master modmel is 32, 64, 96, 230k paramms\n",
    "initial_flt = 64\n",
    "mid_flt = initial_flt * 2\n",
    "high_flt = 64 * 2 * 2\n",
    "#INPUT_SIZE = 28\n",
    "#SIZE_X = 28\n",
    "\n",
    "#inp = ReflectionPadding5D((1, 1))(inp)\n",
    "gru_input = inp[:, :-1, ...]\n",
    "gru, steps = gru_block(inp = gru_input, length = length,\n",
    "                            size = [INPUT_SIZE, SIZE_X, ], # + 2 here for refleclt pad\n",
    "                            flt = initial_flt // 2,\n",
    "                            scope = 'down_16',\n",
    "                            train = is_training)\n",
    "with tf.variable_scope(\"gru_drop\"):\n",
    "    drop_block = DropBlock2D(keep_prob=keep_rate, block_size=4)\n",
    "    gru = drop_block(gru, is_training)\n",
    "    \n",
    "# Median conv\n",
    "\n",
    "median_input = inp[:, -1, ...]\n",
    "median_conv = conv_swish_gn(inp = median_input, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_median', filters = initial_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None)\n",
    "print(f\"Median conv: {median_conv.shape}\")\n",
    "\n",
    "concat1 = tf.concat([gru, median_conv], axis = -1)\n",
    "\n",
    "\n",
    "concat = conv_swish_gn(inp = concat1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_concat', filters = initial_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, padding = \"SAME\")\n",
    "print(f\"Concat: {concat.shape}\")\n",
    "\n",
    "    \n",
    "# MaxPool-conv-swish-GroupNorm-csse\n",
    "pool1 = MaxPool2D()(concat)\n",
    "conv1 = conv_swish_gn(inp = pool1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv1', filters = mid_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True, padding = \"VALID\",\n",
    "            csse = True, dropblock = True, weight_decay = None)\n",
    "print(f\"Conv1: {conv1.shape}\")\n",
    "\n",
    "# MaxPool-conv-swish-csse-DropBlock\n",
    "pool2 = MaxPool2D()(conv1)\n",
    "conv2 = conv_swish_gn(inp = pool2, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv2', filters = high_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, block_size = 4, padding = \"VALID\")\n",
    "print(\"Encoded\", conv2.shape)\n",
    "\n",
    "# Decoder 4 - 8, upsample-conv-swish-csse-concat-conv-swish\n",
    "up2 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(conv2)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2', filters = mid_flt, \n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "conv1_crop = Cropping2D(2)(conv1)\n",
    "print(conv1_crop.shape)\n",
    "up2 = tf.concat([up2, conv1_crop], -1)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2_out', filters = mid_flt, \n",
    "                    keep_rate =  keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "\n",
    "# Decoder 8 - 14 upsample-conv-swish-csse-concat-conv-swish\n",
    "up3 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(up2)\n",
    "#up3 = ReflectionPadding2D((1, 1,))(up3)\n",
    "up3 = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up3', filters = initial_flt, \n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "gru_crop = Cropping2D(6)(concat)\n",
    "print(up3.shape)\n",
    "print(gru_crop.shape)\n",
    "up3 = tf.concat([up3, gru_crop], -1)\n",
    "\n",
    "up3out = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'out', filters = initial_flt, \n",
    "                    keep_rate  = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = False, weight_decay = None, padding = \"VALID\")\n",
    "\n",
    "\n",
    "#print(\"Initializing last sigmoid bias with -2.94 constant\")\n",
    "init = tf.constant_initializer([-np.log(0.7/0.3)]) # For focal loss\n",
    "print(f\"The output is {up3out.shape}, with a receptive field of {1}\")\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1),\n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init)(up3out)#,\n",
    "\n",
    "#fm = Cropping2D(1)(fm)\n",
    "\"\"\"\n",
    "print(f\"The output, sigmoid is {fm.shape}, with a receptive field of {1}\")\n",
    "\n",
    "\n",
    "\n",
    "up4 = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'outregressor', filters = 64, \n",
    "                    keep_rate  = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None, padding = \"SAME\")\n",
    "\n",
    "up5 = conv_swish_gn(inp = up4, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'outregressor2', filters = 64, \n",
    "                    keep_rate  = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = False, weight_decay = None, padding = \"VALID\")\n",
    "\n",
    "regression = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1),\n",
    "            padding = 'valid',\n",
    "            activation = 'linear',\n",
    "            #bias_initializer = init,\n",
    "           )(up5)\n",
    "regression = Cropping2D(1)(regression)\n",
    "output = regression + fm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'gru_drop/drop_block2d/cond/Merge:0' shape=(?, 28, 28, 64) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv_median_drop/drop_block2d_1/cond/Merge:0' shape=(?, 28, 28, 64) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "import math\n",
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "\n",
    "def calc_mask(seg):\n",
    "\n",
    "    res = np.zeros_like(seg)\n",
    "    posmask = seg.astype(np.bool)\n",
    "    loss_importance = np.array([x for x in range(0, 197, 1)])\n",
    "    loss_importance = loss_importance / 196\n",
    "    loss_importance = np.expm1(loss_importance)\n",
    "    loss_importance[:30] = 0.\n",
    "\n",
    "    if posmask.any():\n",
    "        negmask = ~posmask\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n",
    "    if np.sum(seg) == 196:\n",
    "        res = np.ones_like(seg)\n",
    "    if np.sum(seg) == 0:\n",
    "        res = np.ones_like(seg)\n",
    "    res[np.logical_and(res < 2, res > 0)] = 0.5\n",
    "    res[np.logical_or(res >= 2, res <= 0)] = 1.\n",
    "    return res\n",
    "\n",
    "def calc_mask_batch(y_true):\n",
    "    '''Applies calc_dist_map to each sample in an input batch\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "    '''\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    bce_batch = np.array([calc_mask(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "    return bce_batch\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight, mask = True, smooth = 0.03):\n",
    "    '''Calculates the weighted binary cross entropy loss between y_true and\n",
    "       y_pred with optional masking and smoothing for regularization\n",
    "       \n",
    "       For smoothing, we want to weight false positives as less important than\n",
    "       false negatives, so we smooth false negatives 2x as much. \n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          y_pred (arr):\n",
    "          weight (float):\n",
    "          mask (arr):\n",
    "          smooth (float):\n",
    "\n",
    "         Returns:\n",
    "          loss (float):\n",
    "    '''\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    y_true = K.clip(y_true, smooth, 1. - smooth)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        y_true,\n",
    "        logit_y_pred,\n",
    "        weight,\n",
    "    )\n",
    "\n",
    "    return loss\n",
    "\n",
    "def calc_dist_map(seg):\n",
    "    #Utility function for calc_dist_map_batch that calculates the loss\n",
    "    #   importance per pixel based on the surface distance function\n",
    "    \n",
    "     #    Parameters:\n",
    "    #      seg (arr):\n",
    "     #     \n",
    "    #     Returns:\n",
    "    #      res (arr):\n",
    "    #\n",
    "    res = np.zeros_like(seg)\n",
    "    posmask = seg.astype(np.bool)\n",
    "\n",
    "    mults = np.ones_like(seg)\n",
    "    ones = np.ones_like(seg)\n",
    "    for x in range(1, res.shape[0] -1 ):\n",
    "        for y in range(1, res.shape[0] - 1):\n",
    "            if seg[x, y] == 1:\n",
    "                l = seg[x - 1, y]\n",
    "                r = seg[x + 1, y]\n",
    "                u = seg[x, y + 1]\n",
    "                d = seg[x, y - 1]\n",
    "                lu = seg[x - 1, y + 1]\n",
    "                ru = seg[x + 1, y + 1]\n",
    "                rd = seg[x + 1, y - 1]\n",
    "                ld = seg[x -1, y - 1]\n",
    "                \n",
    "                sums = (l + r + u + d)\n",
    "                sums2 = (l + r + u + d + lu + ru +rd + ld)\n",
    "                if sums >= 2:\n",
    "                    mults[x, y] = 2\n",
    "                if sums2 <= 1:\n",
    "                    ones[x - 1, y] = 0.5\n",
    "                    ones[x + 1, y] = 0.5\n",
    "                    ones[x, y + 1] = 0.5\n",
    "                    ones[x, y - 1] = 0.5\n",
    "                    ones[x - 1, y + 1] = 0.5\n",
    "                    ones[x + 1, y + 1] = 0.5\n",
    "                    ones[x + 1, y - 1] = 0.5\n",
    "                    ones[x -1, y - 1] = 0.5\n",
    "\n",
    "    if posmask.any():\n",
    "        \n",
    "        negmask = ~posmask\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n",
    "        # When % = 1, 0 -> 1.75\n",
    "        # When % = 100, 0 -> 0\n",
    "        res = np.round(res, 0)\n",
    "        res[np.where(np.isclose(res, -.41421356, rtol = 1e-2))] = -1\n",
    "        res[np.where(res == -1)] = -1 * mults[np.where(res == -1)]\n",
    "        res[np.where(res == 0)] = -1  * mults[np.where(res == 0)]\n",
    "        # When % = 1, 1 -> 0\n",
    "        # When % = 100, 1 -> 1.75\n",
    "        res[np.where(res == 1)] = 1 * ones[np.where(res == 1)]\n",
    "        res[np.where(res == 1)] *= 0.67\n",
    "        #res[np.where(np.isclose(res, 1.41421356, rtol = 1e-2))] = loss_importance[sums]\n",
    "        \n",
    "    res[np.where(res < -3)] = -3\n",
    "    res[np.where(res > 3)] = 3\n",
    "    if np.sum(seg) == 196:\n",
    "        res = np.ones_like(seg)\n",
    "        res *= -1\n",
    "    if np.sum(seg) == 0:\n",
    "        res = np.ones_like(seg)\n",
    "    return res\n",
    "\n",
    "\n",
    "def calc_dist_map_batch(y_true):\n",
    "    '''Applies calc_dist_map to each sample in an input batch\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "    '''\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    return np.array([calc_dist_map(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "\n",
    "def surface_loss(y_true, y_pred):\n",
    "    '''Calculates the mean surface loss for the input batch\n",
    "       by multiplying the distance map by y_pred\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          y_pred (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "        \n",
    "         References:\n",
    "          https://arxiv.org/abs/1812.07032\n",
    "    '''\n",
    "    y_true_dist_map = tf.py_function(func=calc_dist_map_batch,\n",
    "                                     inp=[y_true],\n",
    "                                     Tout=tf.float32)\n",
    "    y_true_dist_map = tf.stack(y_true_dist_map, axis = 0)\n",
    "    multipled = y_pred * y_true_dist_map\n",
    "    loss = tf.reduce_mean(multipled, axis = (1, 2, 3))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_surf(y_true, y_pred, alpha, weight, beta):\n",
    "    \n",
    "    #lv = lovasz_softmax(probas = y_pred,\n",
    "    #                    labels = tf.reshape(y_true, (-1, 14, 14)), \n",
    "    #                    classes=[1],\n",
    "    #                    per_image=False) \n",
    "    \n",
    "    bce = weighted_bce_loss(y_true = y_true, \n",
    "                             y_pred = y_pred, \n",
    "                             weight = weight,\n",
    "                             smooth = 0.03)\n",
    "\n",
    "    bce = tf.reduce_mean(bce, axis = (1, 2, 3))\n",
    "    surface = surface_loss(y_true, y_pred)\n",
    "\n",
    "    #bce_mask = tf.math.reduce_sum(y_true, axis = (1, 2, 3))\n",
    "    #bce_mask = tf.cast(bce_mask, tf.float32)\n",
    "    #bce_mask_low = tf.math.less(bce_mask, tf.constant([1.]))\n",
    "    #bce_mask_high = tf.math.greater(bce_mask, tf.constant([195.]))\n",
    "    \n",
    "    #bce_mask_low = tf.cast(bce_mask_low, tf.float32)\n",
    "    #bce_mask_high = tf.cast(bce_mask_high, tf.float32)\n",
    "    #bce_mask = bce_mask_low + bce_mask_high\n",
    "    #print(\"BCE mask\", bce_mask.shape)\n",
    "    #surface = (surface * (1 - bce_mask)) + (bce_mask * bce)\n",
    "    surface = tf.reduce_mean(surface)\n",
    "    \n",
    "    \n",
    "   # lovasz = tf.reduce_mean(lv) * (alpha)\n",
    "    \n",
    "\n",
    "    bce = tf.reduce_mean(bce)\n",
    "    bce = (1 - alpha) * bce\n",
    "    surface_portion = alpha * surface\n",
    "    \n",
    "    #result = bce + lovasz\n",
    "    result = bce + surface_portion\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run ../../src/layers/adabound.py\n",
    "def grad_norm(gradients):\n",
    "        norm = tf.norm(\n",
    "            tf.stack([\n",
    "                tf.norm(grad) for grad in gradients if grad is not None\n",
    "            ])\n",
    "        )\n",
    "        return norm\n",
    "    \n",
    "\n",
    "optimizer = AdaBoundOptimizer(1e-3, 0.1)\n",
    "train_loss = lovasz_surf(tf.reshape(labels, (-1, INPUT_SIZE - 14, SIZE_X - 14, 1)), \n",
    "                         fm, weight = loss_weight, \n",
    "                         alpha = alpha, beta = beta_)\n",
    "l2_loss = tf.losses.get_regularization_loss()\n",
    "if len(tf.losses.get_regularization_losses()) > 0:\n",
    "    print(\"Adding L2 loss\")\n",
    "    train_loss = train_loss + l2_loss\n",
    "\n",
    "test_loss = lovasz_surf(tf.reshape(labels, (-1, INPUT_SIZE - 14, SIZE_X - 14, 1)),\n",
    "                        fm, weight = loss_weight, \n",
    "                        alpha = alpha, beta = beta_)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(train_loss)   \n",
    "\n",
    "trainable_params = tf.trainable_variables()\n",
    "gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "gradient_norm = grad_norm(gradients)\n",
    "scale = 0.05 / (gradient_norm + 1e-12)\n",
    "e_ws = []\n",
    "for (grad, param) in gradients:\n",
    "    e_w = grad * scale\n",
    "    param.assign_add(e_w)\n",
    "    e_ws.append(e_w)\n",
    "\n",
    "sam_gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "for (param, e_w) in zip(trainable_params, e_ws):\n",
    "    param.assign_sub(e_w)\n",
    "train_step = optimizer.apply_gradients(sam_gradients)\n",
    "\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "saver = tf.train.Saver(max_to_keep = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef initialize_uninitialized(sess):\\n    global_vars = tf.global_variables()\\n    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\\n    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\\n\\n    if len(not_initialized_vars):\\n        sess.run(tf.variables_initializer(not_initialized_vars))\\n\\ndef grad_norm(gradients):\\n        norm = tf.compat.v1.norm(\\n            tf.stack([\\n                tf.compat.v1.norm(grad) for grad in gradients if grad is not None\\n            ])\\n        )\\n        return norm\\n    \\nft_optimizer = tf.train.MomentumOptimizer(ft_lr, momentum = 0.8, use_nesterov = True)\\ntrain_loss = lovasz_surf(tf.reshape(labels, (-1, INPUT_SIZE - 14, SIZE_X - 14, 1)), \\n                         fm, weight = loss_weight, \\n                         alpha = alpha, beta = beta_)\\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\\n    \\nwith tf.control_dependencies(update_ops):\\n    #train_op = optimizer.minimize(train_loss)   \\n    ft_op = ft_optimizer.minimize(train_loss)\\n    \\ntrainable_params = tf.trainable_variables()\\ngradients = ft_optimizer.compute_gradients(loss=train_loss, var_list=None)\\ngradient_norm = grad_norm(gradients)\\nscale = 0.05 / (gradient_norm + 1e-12)\\ne_ws = []\\nfor (grad, param) in gradients:\\n    e_w = grad * scale\\n    param.assign_add(e_w)\\n    e_ws.append(e_w)\\n\\nsam_gradients = ft_optimizer.compute_gradients(loss=train_loss, var_list=None)\\nfor (param, e_w) in zip(trainable_params, e_ws):\\n    param.assign_sub(e_w)\\ntrain_step = ft_optimizer.apply_gradients(sam_gradients)\\ninitialize_uninitialized(sess)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "\n",
    "def grad_norm(gradients):\n",
    "        norm = tf.compat.v1.norm(\n",
    "            tf.stack([\n",
    "                tf.compat.v1.norm(grad) for grad in gradients if grad is not None\n",
    "            ])\n",
    "        )\n",
    "        return norm\n",
    "    \n",
    "ft_optimizer = tf.train.MomentumOptimizer(ft_lr, momentum = 0.8, use_nesterov = True)\n",
    "train_loss = lovasz_surf(tf.reshape(labels, (-1, INPUT_SIZE - 14, SIZE_X - 14, 1)), \n",
    "                         fm, weight = loss_weight, \n",
    "                         alpha = alpha, beta = beta_)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "with tf.control_dependencies(update_ops):\n",
    "    #train_op = optimizer.minimize(train_loss)   \n",
    "    ft_op = ft_optimizer.minimize(train_loss)\n",
    "    \n",
    "trainable_params = tf.trainable_variables()\n",
    "gradients = ft_optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "gradient_norm = grad_norm(gradients)\n",
    "scale = 0.05 / (gradient_norm + 1e-12)\n",
    "e_ws = []\n",
    "for (grad, param) in gradients:\n",
    "    e_w = grad * scale\n",
    "    param.assign_add(e_w)\n",
    "    e_ws.append(e_w)\n",
    "\n",
    "sam_gradients = ft_optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "for (param, e_w) in zip(trainable_params, e_ws):\n",
    "    param.assign_sub(e_w)\n",
    "train_step = ft_optimizer.apply_gradients(sam_gradients)\n",
    "initialize_uninitialized(sess)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../models/loss/model\n",
      "INFO:tensorflow:Froze 65 variables.\n",
      "INFO:tensorflow:Converted 65 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \"../../models/loss/\"\n",
    "saver = tf.train.Saver(max_to_keep = 150)\n",
    "saver.restore(sess, tf.train.latest_checkpoint(path))\n",
    "\n",
    "output_node_names = ['conv2d_5/Sigmoid']    # Output nodes\n",
    "\n",
    "frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess,\n",
    "    sess.graph_def,\n",
    "    output_node_names)\n",
    "\n",
    "\n",
    "# Save the frozen graph\n",
    "with open(f'../../models/loss/predict_graph.pb', 'wb') as f:\n",
    "    f.write(frozen_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9330127018922194"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_cosine_decay(epoch, maxepoch, offset):\n",
    "    import math\n",
    "    return 0.5 * (1 + math.cos(math.pi * (epoch - offset) / (maxepoch - offset)))\n",
    "                  \n",
    "calc_cosine_decay(5, 30, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../models/oct30/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6e354ab9267f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'../../models/oct30/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../models/oct30/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"ttt\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#fs = [x for x in fs if \"A15\"  in x]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../models/oct30/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "fs =['../../models/oct30/' + x + \"/\" for x in os.listdir(\"../../models/oct30/\")]\n",
    "fs = [x for x in fs if \"ttt\" not in x]\n",
    "#fs = [x for x in fs if \"A15\"  in x]\n",
    "fs = [x for x in fs if 'RESWA23' in x]\n",
    "fs = np.array(fs)\n",
    "epochs = [int(x.split(\"-\")[1]) for x in fs if \"-\" in x]\n",
    "epochs = np.array(epochs)\n",
    "print(epochs)\n",
    "fs = fs[np.logical_and(epochs >= 60, epochs <= 85)] # 20 36\n",
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('beta1_power', []), ('beta2_power', []), ('conv1_conv/conv1/ws_conv2d_2/kernel', [3, 3, 64, 128]), ('conv1_conv/conv1/ws_conv2d_2/kernel/AdaBound', [3, 3, 64, 128]), ('conv1_conv/conv1/ws_conv2d_2/kernel/AdaBound_1', [3, 3, 64, 128]), ('conv1_conv/conv1/ws_conv2d_2/kernel/AdaBound_2', [3, 3, 64, 128]), ('conv1_norm/beta_conv1', [128]), ('conv1_norm/beta_conv1/AdaBound', [128]), ('conv1_norm/beta_conv1/AdaBound_1', [128]), ('conv1_norm/beta_conv1/AdaBound_2', [128]), ('conv1_norm/gamma_conv1', [128]), ('conv1_norm/gamma_conv1/AdaBound', [128]), ('conv1_norm/gamma_conv1/AdaBound_1', [128]), ('conv1_norm/gamma_conv1/AdaBound_2', [128]), ('conv2_conv/conv2/ws_conv2d_3/kernel', [3, 3, 128, 256]), ('conv2_conv/conv2/ws_conv2d_3/kernel/AdaBound', [3, 3, 128, 256]), ('conv2_conv/conv2/ws_conv2d_3/kernel/AdaBound_1', [3, 3, 128, 256]), ('conv2_conv/conv2/ws_conv2d_3/kernel/AdaBound_2', [3, 3, 128, 256]), ('conv2_norm/beta_conv2', [256]), ('conv2_norm/beta_conv2/AdaBound', [256]), ('conv2_norm/beta_conv2/AdaBound_1', [256]), ('conv2_norm/beta_conv2/AdaBound_2', [256]), ('conv2_norm/gamma_conv2', [256]), ('conv2_norm/gamma_conv2/AdaBound', [256]), ('conv2_norm/gamma_conv2/AdaBound_1', [256]), ('conv2_norm/gamma_conv2/AdaBound_2', [256]), ('conv2d_5/bias', [1]), ('conv2d_5/bias/AdaBound', [1]), ('conv2d_5/bias/AdaBound_1', [1]), ('conv2d_5/bias/AdaBound_2', [1]), ('conv2d_5/kernel', [1, 1, 64, 1]), ('conv2d_5/kernel/AdaBound', [1, 1, 64, 1]), ('conv2d_5/kernel/AdaBound_1', [1, 1, 64, 1]), ('conv2d_5/kernel/AdaBound_2', [1, 1, 64, 1]), ('conv_concat_conv/conv_concat/mask/conv2d/kernel', [3, 3, 1, 1]), ('conv_concat_conv/conv_concat/x/ws_conv2d_1/kernel', [3, 3, 128, 64]), ('conv_concat_conv/conv_concat/x/ws_conv2d_1/kernel/AdaBound', [3, 3, 128, 64]), ('conv_concat_conv/conv_concat/x/ws_conv2d_1/kernel/AdaBound_1', [3, 3, 128, 64]), ('conv_concat_conv/conv_concat/x/ws_conv2d_1/kernel/AdaBound_2', [3, 3, 128, 64]), ('conv_concat_norm/beta_conv_concat', [64]), ('conv_concat_norm/beta_conv_concat/AdaBound', [64]), ('conv_concat_norm/beta_conv_concat/AdaBound_1', [64]), ('conv_concat_norm/beta_conv_concat/AdaBound_2', [64]), ('conv_concat_norm/gamma_conv_concat', [64]), ('conv_concat_norm/gamma_conv_concat/AdaBound', [64]), ('conv_concat_norm/gamma_conv_concat/AdaBound_1', [64]), ('conv_concat_norm/gamma_conv_concat/AdaBound_2', [64]), ('conv_median_conv/conv_median/mask/conv2d/kernel', [3, 3, 1, 1]), ('conv_median_conv/conv_median/x/ws_conv2d/kernel', [3, 3, 17, 64]), ('conv_median_conv/conv_median/x/ws_conv2d/kernel/AdaBound', [3, 3, 17, 64]), ('conv_median_conv/conv_median/x/ws_conv2d/kernel/AdaBound_1', [3, 3, 17, 64]), ('conv_median_conv/conv_median/x/ws_conv2d/kernel/AdaBound_2', [3, 3, 17, 64]), ('conv_median_norm/beta_conv_median', [64]), ('conv_median_norm/beta_conv_median/AdaBound', [64]), ('conv_median_norm/beta_conv_median/AdaBound_1', [64]), ('conv_median_norm/beta_conv_median/AdaBound_2', [64]), ('conv_median_norm/gamma_conv_median', [64]), ('conv_median_norm/gamma_conv_median/AdaBound', [64]), ('conv_median_norm/gamma_conv_median/AdaBound_1', [64]), ('conv_median_norm/gamma_conv_median/AdaBound_2', [64]), ('csse_conv1_conv/bias', [1]), ('csse_conv1_conv/bias/AdaBound', [1]), ('csse_conv1_conv/bias/AdaBound_1', [1]), ('csse_conv1_conv/bias/AdaBound_2', [1]), ('csse_conv1_conv/kernel', [1, 1, 128, 1]), ('csse_conv1_conv/kernel/AdaBound', [1, 1, 128, 1]), ('csse_conv1_conv/kernel/AdaBound_1', [1, 1, 128, 1]), ('csse_conv1_conv/kernel/AdaBound_2', [1, 1, 128, 1]), ('csse_conv2_conv/bias', [1]), ('csse_conv2_conv/bias/AdaBound', [1]), ('csse_conv2_conv/bias/AdaBound_1', [1]), ('csse_conv2_conv/bias/AdaBound_2', [1]), ('csse_conv2_conv/kernel', [1, 1, 256, 1]), ('csse_conv2_conv/kernel/AdaBound', [1, 1, 256, 1]), ('csse_conv2_conv/kernel/AdaBound_1', [1, 1, 256, 1]), ('csse_conv2_conv/kernel/AdaBound_2', [1, 1, 256, 1]), ('csse_conv_concat_conv/bias', [1]), ('csse_conv_concat_conv/bias/AdaBound', [1]), ('csse_conv_concat_conv/bias/AdaBound_1', [1]), ('csse_conv_concat_conv/bias/AdaBound_2', [1]), ('csse_conv_concat_conv/kernel', [1, 1, 64, 1]), ('csse_conv_concat_conv/kernel/AdaBound', [1, 1, 64, 1]), ('csse_conv_concat_conv/kernel/AdaBound_1', [1, 1, 64, 1]), ('csse_conv_concat_conv/kernel/AdaBound_2', [1, 1, 64, 1]), ('csse_conv_median_conv/bias', [1]), ('csse_conv_median_conv/bias/AdaBound', [1]), ('csse_conv_median_conv/bias/AdaBound_1', [1]), ('csse_conv_median_conv/bias/AdaBound_2', [1]), ('csse_conv_median_conv/kernel', [1, 1, 64, 1]), ('csse_conv_median_conv/kernel/AdaBound', [1, 1, 64, 1]), ('csse_conv_median_conv/kernel/AdaBound_1', [1, 1, 64, 1]), ('csse_conv_median_conv/kernel/AdaBound_2', [1, 1, 64, 1]), ('csse_out_conv/bias', [1]), ('csse_out_conv/bias/AdaBound', [1]), ('csse_out_conv/bias/AdaBound_1', [1]), ('csse_out_conv/bias/AdaBound_2', [1]), ('csse_out_conv/kernel', [1, 1, 64, 1]), ('csse_out_conv/kernel/AdaBound', [1, 1, 64, 1]), ('csse_out_conv/kernel/AdaBound_1', [1, 1, 64, 1]), ('csse_out_conv/kernel/AdaBound_2', [1, 1, 64, 1]), ('csse_up2_conv/bias', [1]), ('csse_up2_conv/bias/AdaBound', [1]), ('csse_up2_conv/bias/AdaBound_1', [1]), ('csse_up2_conv/bias/AdaBound_2', [1]), ('csse_up2_conv/kernel', [1, 1, 128, 1]), ('csse_up2_conv/kernel/AdaBound', [1, 1, 128, 1]), ('csse_up2_conv/kernel/AdaBound_1', [1, 1, 128, 1]), ('csse_up2_conv/kernel/AdaBound_2', [1, 1, 128, 1]), ('csse_up2_out_conv/bias', [1]), ('csse_up2_out_conv/bias/AdaBound', [1]), ('csse_up2_out_conv/bias/AdaBound_1', [1]), ('csse_up2_out_conv/bias/AdaBound_2', [1]), ('csse_up2_out_conv/kernel', [1, 1, 128, 1]), ('csse_up2_out_conv/kernel/AdaBound', [1, 1, 128, 1]), ('csse_up2_out_conv/kernel/AdaBound_1', [1, 1, 128, 1]), ('csse_up2_out_conv/kernel/AdaBound_2', [1, 1, 128, 1]), ('csse_up3_conv/bias', [1]), ('csse_up3_conv/bias/AdaBound', [1]), ('csse_up3_conv/bias/AdaBound_1', [1]), ('csse_up3_conv/bias/AdaBound_2', [1]), ('csse_up3_conv/kernel', [1, 1, 64, 1]), ('csse_up3_conv/kernel/AdaBound', [1, 1, 64, 1]), ('csse_up3_conv/kernel/AdaBound_1', [1, 1, 64, 1]), ('csse_up3_conv/kernel/AdaBound_2', [1, 1, 64, 1]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/candidate/candidate_y_norm/beta_candidate_y', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/candidate/candidate_y_norm/beta_candidate_y/AdaBound', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/candidate/candidate_y_norm/beta_candidate_y/AdaBound_1', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/candidate/candidate_y_norm/beta_candidate_y/AdaBound_2', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/candidate/candidate_y_norm/gamma_candidate_y', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/candidate/candidate_y_norm/gamma_candidate_y/AdaBound', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/candidate/candidate_y_norm/gamma_candidate_y/AdaBound_1', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/candidate/candidate_y_norm/gamma_candidate_y/AdaBound_2', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_r_norm/beta_gates_r', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_r_norm/beta_gates_r/AdaBound', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_r_norm/beta_gates_r/AdaBound_1', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_r_norm/beta_gates_r/AdaBound_2', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_r_norm/gamma_gates_r', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_r_norm/gamma_gates_r/AdaBound', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_r_norm/gamma_gates_r/AdaBound_1', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_r_norm/gamma_gates_r/AdaBound_2', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_u_norm/beta_gates_u', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_u_norm/beta_gates_u/AdaBound', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_u_norm/beta_gates_u/AdaBound_1', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_u_norm/beta_gates_u/AdaBound_2', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_u_norm/gamma_gates_u', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_u_norm/gamma_gates_u/AdaBound', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_u_norm/gamma_gates_u/AdaBound_1', [32]), ('down_16/bidirectional_rnn/bw/bw/while/bw/conv_gru_cell/gates/gates_u_norm/gamma_gates_u/AdaBound_2', [32]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/candidate/kernel', [3, 3, 49, 32]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/candidate/kernel/AdaBound', [3, 3, 49, 32]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/candidate/kernel/AdaBound_1', [3, 3, 49, 32]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/candidate/kernel/AdaBound_2', [3, 3, 49, 32]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/candidate/kernel_1', [1, 1, 32, 1]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/candidate/kernel_1/AdaBound', [1, 1, 32, 1]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/candidate/kernel_1/AdaBound_1', [1, 1, 32, 1]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/candidate/kernel_1/AdaBound_2', [1, 1, 32, 1]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/gates/kernel', [3, 3, 49, 64]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/gates/kernel/AdaBound', [3, 3, 49, 64]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/gates/kernel/AdaBound_1', [3, 3, 49, 64]), ('down_16/bidirectional_rnn/bw/conv_gru_cell/gates/kernel/AdaBound_2', [3, 3, 49, 64]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/candidate/kernel', [3, 3, 49, 32]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/candidate/kernel/AdaBound', [3, 3, 49, 32]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/candidate/kernel/AdaBound_1', [3, 3, 49, 32]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/candidate/kernel/AdaBound_2', [3, 3, 49, 32]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/candidate/kernel_1', [1, 1, 32, 1]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/candidate/kernel_1/AdaBound', [1, 1, 32, 1]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/candidate/kernel_1/AdaBound_1', [1, 1, 32, 1]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/candidate/kernel_1/AdaBound_2', [1, 1, 32, 1]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/gates/kernel', [3, 3, 49, 64]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/gates/kernel/AdaBound', [3, 3, 49, 64]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/gates/kernel/AdaBound_1', [3, 3, 49, 64]), ('down_16/bidirectional_rnn/fw/conv_gru_cell/gates/kernel/AdaBound_2', [3, 3, 49, 64]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/candidate/candidate_y_norm/beta_candidate_y', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/candidate/candidate_y_norm/beta_candidate_y/AdaBound', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/candidate/candidate_y_norm/beta_candidate_y/AdaBound_1', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/candidate/candidate_y_norm/beta_candidate_y/AdaBound_2', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/candidate/candidate_y_norm/gamma_candidate_y', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/candidate/candidate_y_norm/gamma_candidate_y/AdaBound', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/candidate/candidate_y_norm/gamma_candidate_y/AdaBound_1', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/candidate/candidate_y_norm/gamma_candidate_y/AdaBound_2', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_r_norm/beta_gates_r', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_r_norm/beta_gates_r/AdaBound', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_r_norm/beta_gates_r/AdaBound_1', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_r_norm/beta_gates_r/AdaBound_2', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_r_norm/gamma_gates_r', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_r_norm/gamma_gates_r/AdaBound', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_r_norm/gamma_gates_r/AdaBound_1', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_r_norm/gamma_gates_r/AdaBound_2', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_u_norm/beta_gates_u', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_u_norm/beta_gates_u/AdaBound', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_u_norm/beta_gates_u/AdaBound_1', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_u_norm/beta_gates_u/AdaBound_2', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_u_norm/gamma_gates_u', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_u_norm/gamma_gates_u/AdaBound', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_u_norm/gamma_gates_u/AdaBound_1', [32]), ('down_16/bidirectional_rnn/fw/fw/while/fw/conv_gru_cell/gates/gates_u_norm/gamma_gates_u/AdaBound_2', [32]), ('gamma_multi', []), ('global_step', []), ('out_conv/out/ws_conv2d_7/kernel', [3, 3, 128, 64]), ('out_conv/out/ws_conv2d_7/kernel/AdaBound', [3, 3, 128, 64]), ('out_conv/out/ws_conv2d_7/kernel/AdaBound_1', [3, 3, 128, 64]), ('out_conv/out/ws_conv2d_7/kernel/AdaBound_2', [3, 3, 128, 64]), ('out_norm/beta_out', [64]), ('out_norm/beta_out/AdaBound', [64]), ('out_norm/beta_out/AdaBound_1', [64]), ('out_norm/beta_out/AdaBound_2', [64]), ('out_norm/gamma_out', [64]), ('out_norm/gamma_out/AdaBound', [64]), ('out_norm/gamma_out/AdaBound_1', [64]), ('out_norm/gamma_out/AdaBound_2', [64]), ('up2_conv/up2/mask/conv2d/kernel', [3, 3, 1, 1]), ('up2_conv/up2/x/ws_conv2d_4/kernel', [3, 3, 256, 128]), ('up2_conv/up2/x/ws_conv2d_4/kernel/AdaBound', [3, 3, 256, 128]), ('up2_conv/up2/x/ws_conv2d_4/kernel/AdaBound_1', [3, 3, 256, 128]), ('up2_conv/up2/x/ws_conv2d_4/kernel/AdaBound_2', [3, 3, 256, 128]), ('up2_norm/beta_up2', [128]), ('up2_norm/beta_up2/AdaBound', [128]), ('up2_norm/beta_up2/AdaBound_1', [128]), ('up2_norm/beta_up2/AdaBound_2', [128]), ('up2_norm/gamma_up2', [128]), ('up2_norm/gamma_up2/AdaBound', [128]), ('up2_norm/gamma_up2/AdaBound_1', [128]), ('up2_norm/gamma_up2/AdaBound_2', [128]), ('up2_out_conv/up2_out/mask/conv2d/kernel', [3, 3, 1, 1]), ('up2_out_conv/up2_out/x/ws_conv2d_5/kernel', [3, 3, 256, 128]), ('up2_out_conv/up2_out/x/ws_conv2d_5/kernel/AdaBound', [3, 3, 256, 128]), ('up2_out_conv/up2_out/x/ws_conv2d_5/kernel/AdaBound_1', [3, 3, 256, 128]), ('up2_out_conv/up2_out/x/ws_conv2d_5/kernel/AdaBound_2', [3, 3, 256, 128]), ('up2_out_norm/beta_up2_out', [128]), ('up2_out_norm/beta_up2_out/AdaBound', [128]), ('up2_out_norm/beta_up2_out/AdaBound_1', [128]), ('up2_out_norm/beta_up2_out/AdaBound_2', [128]), ('up2_out_norm/gamma_up2_out', [128]), ('up2_out_norm/gamma_up2_out/AdaBound', [128]), ('up2_out_norm/gamma_up2_out/AdaBound_1', [128]), ('up2_out_norm/gamma_up2_out/AdaBound_2', [128]), ('up3_conv/up3/mask/conv2d/kernel', [3, 3, 1, 1]), ('up3_conv/up3/x/ws_conv2d_6/kernel', [3, 3, 128, 64]), ('up3_conv/up3/x/ws_conv2d_6/kernel/AdaBound', [3, 3, 128, 64]), ('up3_conv/up3/x/ws_conv2d_6/kernel/AdaBound_1', [3, 3, 128, 64]), ('up3_conv/up3/x/ws_conv2d_6/kernel/AdaBound_2', [3, 3, 128, 64]), ('up3_norm/beta_up3', [64]), ('up3_norm/beta_up3/AdaBound', [64]), ('up3_norm/beta_up3/AdaBound_1', [64]), ('up3_norm/beta_up3/AdaBound_2', [64]), ('up3_norm/gamma_up3', [64]), ('up3_norm/gamma_up3/AdaBound', [64]), ('up3_norm/gamma_up3/AdaBound_1', [64]), ('up3_norm/gamma_up3/AdaBound_2', [64])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 09:57:50.117542: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-05 09:57:50.117573: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-03-05 09:57:50.154724: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.411499: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.421646: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.436105: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.446883: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.456881: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.466465: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.475837: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.484882: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.494041: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.503281: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.512340: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.521444: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.530619: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.541057: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.550845: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.561092: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.571256: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.581763: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.592289: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.602333: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.612023: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.622341: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.632380: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.642186: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.651976: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.661447: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.671283: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.680552: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.689837: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.699158: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.709043: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.718601: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.728716: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.739001: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.749123: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.759604: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.770435: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.780943: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.791241: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.807472: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.818205: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.829434: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.839426: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.850352: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.861807: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.871978: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.882832: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.894446: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.905534: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.915499: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.926147: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.936775: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.947898: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.957705: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.968560: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.978646: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.987981: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:50.997771: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.007165: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.016769: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.027049: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.037982: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.048483: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.058881: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.069694: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.080230: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.090545: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.100590: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.111280: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.121168: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.131272: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.141228: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.151296: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.161446: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.170938: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.180578: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.190508: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.200148: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.210800: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.220313: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.230679: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.241200: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.252511: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.263146: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.273537: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.284112: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.294215: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.303878: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.315007: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.326249: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.336367: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.347012: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.357595: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.367688: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.378956: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.389366: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.399625: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.409756: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.420157: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.430386: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.441106: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.451681: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.463897: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.481579: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.491865: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.501552: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.511816: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.522209: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.532902: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.543226: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.553177: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.563807: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.574054: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.584624: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.594220: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.603413: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.613022: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.622558: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.632058: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.642127: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.651794: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.667668: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.678844: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.690005: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.700918: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.710967: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.721282: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.732298: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.742182: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.752505: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.765018: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.776171: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.786486: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.797262: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.808216: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.818126: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.828771: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.839612: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.849981: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.859389: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.869798: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.879765: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.890994: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.901869: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.913081: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.922972: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.932726: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.947203: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.957811: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.968003: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.978072: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.988525: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:51.998635: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.008271: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.019209: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.028790: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.039734: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.050933: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.061108: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.071568: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.082965: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.093301: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.103934: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.115842: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.126310: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.135818: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.146340: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.157627: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.166932: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.177245: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.188289: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.198454: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.208363: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.217878: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.227912: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.239070: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.249378: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.259473: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.269131: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.279651: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.291297: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.306185: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.316774: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.327057: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.336936: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.346847: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.356464: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.366053: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.375726: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.385351: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.396057: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.406151: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.416109: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.426168: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.436042: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.446544: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.456790: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.466890: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.477017: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.487895: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.499032: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.509780: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.520257: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.530644: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.541661: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.552752: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.563600: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.574597: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.586238: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.597026: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.607788: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.618235: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.628381: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.638901: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.649246: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.659052: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.668987: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.678628: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.688738: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.698934: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.709817: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.720099: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.732913: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.742899: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.753375: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.763995: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.774472: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.785009: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.795487: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.806132: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.816612: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.826329: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.836157: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.846851: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.857808: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.867431: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.877113: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.888078: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.897778: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.907406: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.919256: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.929067: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.938338: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.947733: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.956885: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.966212: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.975624: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.984959: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:52.993634: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-05 09:57:53.002219: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import six\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import numpy as np\n",
    "import os \n",
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "checkpoints = ['../../models/master-28',\n",
    "               '../../models/master-76',\n",
    "               '../../models/master-76'\n",
    "              ]\n",
    "\n",
    "def checkpoint_exists(path):\n",
    "    return (tf.gfile.Exists(path) or tf.gfile.Exists(path + \".meta\") or\n",
    "            tf.gfile.Exists(path + \".index\"))\n",
    "\n",
    "\n",
    "def main():\n",
    "    #checkpoints = [c.strip() for c in checkpoints.split(\",\")]\n",
    "    #checkpoints = [c for c in checkpoints if c]\n",
    "    #checkpoints = [c for c in checkpoints if checkpoint_exists(c)]\n",
    "\n",
    "  # Read variables from all checkpoints and average them.\n",
    "    tf.logging.info(\"Reading variables and averaging checkpoints:\")\n",
    "    for c in checkpoints:\n",
    "        tf.logging.info(\"%s \", c)\n",
    "    var_list = tf.train.list_variables(checkpoints[0])\n",
    "    var_values, var_dtypes = {}, {}\n",
    "    print([x for x in var_list])\n",
    "    for (name, shape) in var_list:\n",
    "        if not name.startswith(\"global_step\"):\n",
    "            var_values[name] = np.zeros(shape)\n",
    "    for checkpoint in checkpoints:\n",
    "        reader = tf.train.load_checkpoint(checkpoint)\n",
    "        for name in var_values:\n",
    "            tensor = reader.get_tensor(name)\n",
    "            var_dtypes[name] = tensor.dtype\n",
    "            var_values[name] += tensor\n",
    "        tf.logging.info(\"Read from checkpoint %s\", checkpoint)\n",
    "    for name in var_values:  # Average.\n",
    "        var_values[name] /= len(checkpoints)\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "        tf_vars = [\n",
    "            tf.get_variable(v, shape=var_values[v].shape, dtype=var_dtypes[v])\n",
    "            for v in var_values\n",
    "        ]\n",
    "    placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars]\n",
    "    assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n",
    "    global_step = tf.Variable(\n",
    "        0, name=\"global_step\", trainable=False, dtype=tf.int64)\n",
    "    var_list = [x for x in tf.all_variables() if 'Momentum' not in x.name]\n",
    "    saver = tf.train.Saver(var_list)\n",
    "\n",
    "  # Build a model consisting only of variables, set them to the average values.\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for p, assign_op, (name, value) in zip(placeholders, assign_ops,\n",
    "                                           six.iteritems(var_values)):\n",
    "            sess.run(assign_op, {p: value})\n",
    "    # Use the built saver to save the averaged checkpoint.\n",
    "        saver.save(sess, \"../../models/master-76-28/\", global_step=global_step)\n",
    "    tf.logging.info(\"Averaged checkpoints saved in %s\", \"../../models/tml-average-5/\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-9",
   "language": "python",
   "name": "tf2-9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
