{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p37/gpu_cuda10.0/lib/python3.7/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "from tensorflow.initializers import glorot_normal, lecun_normal\n",
    "from scipy.ndimage import median_filter\n",
    "from skimage.transform import resize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.python.util import deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/src/layers/convgru.py:30: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run src/layers/zoneout.py\n",
    "%run src/layers/adabound.py\n",
    "#%run src/layers/adabelief.py\n",
    "%run src/layers/convgru.py\n",
    "%run src/layers/dropblock.py\n",
    "%run src/layers/extra_layers.py\n",
    "%run src/layers/stochastic_weight_averaging.py\n",
    "%run src/preprocessing/indices.py\n",
    "%run src/preprocessing/slope.py\n",
    "%run src/utils/metrics.py\n",
    "%run src/utils/lovasz.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/src/layers/adabelief.py:8: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run src/layers/adabelief.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE_OUT_PROB = 0.9\n",
    "\n",
    "ACTIVATION_FUNCTION = 'swish'\n",
    "\n",
    "INITIAL_LR = 5e-5\n",
    "DROPBLOCK_MAXSIZE = 4\n",
    "DECONV = 'upconv'\n",
    "N_CONV_BLOCKS = 1\n",
    "FINAL_ALPHA = 0.33\n",
    "LABEL_SMOOTHING = 0.1\n",
    "BATCH_RENORM = 'renorm'\n",
    "\n",
    "L2_REG = 0.\n",
    "BN_MOMENTUM = 0.90\n",
    "BATCH_SIZE = 20\n",
    "MAX_DROPBLOCK = 0.8\n",
    "\n",
    "GRU_FLT = 24\n",
    "OUT_FLT = 48\n",
    "\n",
    "IMAGE_SIZE = 24\n",
    "LABEL_SIZE = 14\n",
    "LEN = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layer definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility blocks (Batch norm, cSSE, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cse_block(prevlayer, prefix):\n",
    "    '''Channel excitation and spatial squeeze layer. \n",
    "       Calculates the mean of the spatial dimensions and then learns\n",
    "       two dense layers, one with relu, and one with sigmoid, to rerank the\n",
    "       input channels\n",
    "       \n",
    "         Parameters:\n",
    "          prevlayer (tf.Variable): input layer\n",
    "          prefix (str): prefix for tensorflow scope\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): output of the cse_block\n",
    "    '''\n",
    "    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n",
    "    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + 'cse_lin1', activation='relu')(mean)\n",
    "    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + 'cse_lin2', activation='sigmoid')(lin1)\n",
    "    x = Multiply()([prevlayer, lin2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def sse_block(prevlayer, prefix):\n",
    "    '''Spatial excitation and channel squeeze layer.\n",
    "       Calculates a 1x1 convolution with sigmoid activation to create a \n",
    "       spatial map that is multiplied by the input layer\n",
    "\n",
    "         Parameters:\n",
    "          prevlayer (tf.Variable): input layer\n",
    "          prefix (str): prefix for tensorflow scope\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): output of the sse_block\n",
    "    '''\n",
    "    conv = Conv2D(1, (1, 1), padding=\"same\", kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "def csse_block(x, prefix):\n",
    "    '''Implementation of Concurrent Spatial and Channel \n",
    "       ‘Squeeze & Excitation’ in Fully Convolutional Networks\n",
    "    \n",
    "        Parameters:\n",
    "          prevlayer (tf.Variable): input layer\n",
    "          prefix (str): prefix for tensorflow scope\n",
    "\n",
    "         Returns:\n",
    "          x (tf.Variable): added output of cse and sse block\n",
    "          \n",
    "         References:\n",
    "          https://arxiv.org/abs/1803.02579\n",
    "    '''\n",
    "    #cse = cse_block(x, prefix)\n",
    "    sse = sse_block(x, prefix)\n",
    "    #x = Add(name=prefix + \"_csse_mul\")([cse, sse])\n",
    "\n",
    "    return sse\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        print(\"ZERO PADDING\")\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv GRU Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_block(inp, length, size, flt, scope, train, normalize = False):\n",
    "    '''Bidirectional convolutional GRU block with \n",
    "       zoneout and CSSE blocks in each time step\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): (B, T, H, W, C) layer\n",
    "          length (tf.Variable): (B, T) layer denoting number of\n",
    "                                steps per sample\n",
    "          size (int): kernel size of convolution\n",
    "          flt (int): number of convolution filters\n",
    "          scope (str): tensorflow variable scope\n",
    "          train (tf.Bool): flag to differentiate between train/test ops\n",
    "          normalize (bool): whether to compute layer normalization\n",
    "\n",
    "         Returns:\n",
    "          gru (tf.Variable): (B, H, W, flt*2) bi-gru output\n",
    "          steps (tf.Variable): (B, T, H, W, flt*2) output of each step\n",
    "    '''\n",
    "    with tf.variable_scope(scope):\n",
    "        print(f\"GRU input shape {inp.shape}, zoneout: {0.5}\")\n",
    "        \"\"\"\n",
    "        cell_fw = ConvLSTMCell(shape = size, filters = flt,\n",
    "                               kernel = [3, 3], forget_bias=1.0, \n",
    "                               activation=tf.tanh, normalize=True, \n",
    "                               peephole=False, data_format='channels_last', reuse=None)\n",
    "        cell_bw = ConvLSTMCell(shape = size, filters = flt,\n",
    "                               kernel = [3, 3], forget_bias=1.0, \n",
    "                               activation=tf.tanh, normalize=True, \n",
    "                               peephole=False, data_format='channels_last', reuse=None)\n",
    "        \"\"\"\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', normalize = True, sse = True)\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'VALID', normalize = True, sse = True)\n",
    "        zoneout = 0.75\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "           cell_fw, zoneout_drop_prob = zoneout, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = zoneout, is_training = train)\n",
    "        print(inp.shape)\n",
    "        steps, out = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        print(f\"Zoneout: {zoneout}\")\n",
    "        gru = tf.concat(out, axis = -1)\n",
    "        steps = tf.concat(steps, axis = -1)\n",
    "        print(f\"Down block output shape {gru.shape}\")\n",
    "    return gru, steps\n",
    "\n",
    "\n",
    "def attention(inp, units):\n",
    "    weighted = TimeDistributed(Conv2D(units, (1, 1), padding = 'same', kernel_initializer = tf.keras.initializers.Ones(),\n",
    "                            activation = 'sigmoid', strides = (1, 1), use_bias = False, ))(inp) \n",
    "    alphas = tf.reduce_sum(weighted, axis = 1, keep_dims = True)\n",
    "    alphas = weighted / alphas\n",
    "    multiplied = tf.reduce_sum(alphas * inp, axis = 1)\n",
    "    print(multiplied.shape)\n",
    "    return multiplied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Conv\n",
    "\n",
    "def ws_reg(kernel):\n",
    "    kernel_mean = tf.math.reduce_mean(kernel, axis=[0, 1, 2], keepdims=True, name='kernel_mean')\n",
    "    kernel = kernel - kernel_mean\n",
    "  #kernel_std = tf.math.reduce_std(kernel, axis=[0, 1, 2], keepdims=True, name='kernel_std')\n",
    "    kernel_std = tf.keras.backend.std(kernel, axis=[0, 1, 2], keepdims=True)\n",
    "    kernel = kernel / (kernel_std + 1e-5)\n",
    "    return kernel\n",
    "\n",
    "class WSConv2D(tf.keras.layers.Conv2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(WSConv2D, self).__init__(kernel_initializer=\"he_normal\", *args, **kwargs)\n",
    "\n",
    "    def standardize_weight(self, weight, eps):\n",
    "\n",
    "        mean = tf.math.reduce_mean(weight, axis=(0, 1, 2), keepdims=True)\n",
    "        weight = weight - mean\n",
    "        var = tf.keras.backend.std(weight, axis=[0, 1, 2], keepdims=True)\n",
    "        weight = weight / (var + 1e-5)\n",
    "        return weight\n",
    "\n",
    "    def call(self, inputs, eps=1e-4):\n",
    "        self.kernel.assign(self.standardize_weight(self.kernel, eps))\n",
    "        return super().call(inputs)\n",
    "\n",
    "class WSConv2D(tf.keras.layers.Conv2D):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=(1, 1),\n",
    "                 activation=None,\n",
    "                 use_bias=False,\n",
    "                 kernel_initializer='he_normal',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(WSConv2D, self).__init__(filters=filters,\n",
    "                 kernel_size=kernel_size,\n",
    "                 strides=strides,\n",
    "                 padding=padding,\n",
    "                 data_format=data_format,\n",
    "                 dilation_rate=dilation_rate,\n",
    "                 activation=activation,\n",
    "                 use_bias=use_bias,\n",
    "                 kernel_initializer=kernel_initializer,\n",
    "                 bias_initializer=bias_initializer,\n",
    "                 kernel_regularizer=kernel_regularizer,\n",
    "                 bias_regularizer=bias_regularizer,\n",
    "                 activity_regularizer=activity_regularizer,\n",
    "                 kernel_constraint=kernel_constraint,\n",
    "                 bias_constraint=bias_constraint,\n",
    "                 **kwargs)\n",
    "        \n",
    "    def standardize_weight(self, weight, eps):\n",
    "\n",
    "        mean = tf.math.reduce_mean(weight, axis=(0, 1, 2), keepdims=True)\n",
    "        weight = weight - mean\n",
    "        var = K.sqrt(K.var(weight, axis=[0,1,2], keepdims=True) + 1e-8)\n",
    "        weight = weight / (var + 1e-5)\n",
    "        return weight\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #weight = self.standardize_weight(self.kernel, 1e-5)\n",
    "\n",
    "        outputs = K.conv2d(\n",
    "            inputs,\n",
    "            self.kernel,\n",
    "            strides=self.strides,\n",
    "            padding=self.padding,\n",
    "            data_format=self.data_format,\n",
    "            dilation_rate=self.dilation_rate)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class WSConv2D(tf.keras.layers.Conv2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(WSConv2D, self).__init__(kernel_initializer=\"he_normal\", *args, **kwargs)\n",
    "\n",
    "    def standardize_weight(self, weight, eps):\n",
    "\n",
    "        mean = tf.math.reduce_mean(weight, axis=(0, 1, 2), keepdims=True)\n",
    "        weight = weight - mean\n",
    "        var = tf.keras.backend.std(weight, axis=[0, 1, 2], keepdims=True)\n",
    "        weight = weight / (var + 1e-5)\n",
    "        return weight\n",
    "\n",
    "    def call(self, inputs, eps=1e-4):\n",
    "        self.kernel.assign(self.standardize_weight(self.kernel, eps))\n",
    "        return super().call(inputs)\n",
    "    \n",
    "def partial_conv(x, channels, kernel=3, stride=1, norm = True, use_bias=False, padding='SAME', scope='conv_0'):\n",
    "    \n",
    "\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        if padding.lower() == 'SAME'.lower() :\n",
    "            with tf.variable_scope('mask'):\n",
    "                _, h, w, _ = x.get_shape().as_list()\n",
    "\n",
    "                slide_window = kernel * kernel\n",
    "                mask = tf.ones(shape=[1, h, w, 1])\n",
    "\n",
    "                update_mask = tf.layers.conv2d(mask, filters=1,\n",
    "                                               kernel_size=kernel, kernel_initializer=tf.constant_initializer(1.0),\n",
    "                                               strides=stride, padding=padding, use_bias=False, trainable=False)\n",
    "\n",
    "                mask_ratio = slide_window / (update_mask + 1e-8)\n",
    "                update_mask = tf.clip_by_value(update_mask, 0.0, 1.0)\n",
    "                mask_ratio = mask_ratio * update_mask\n",
    "\n",
    "            with tf.variable_scope('x'):\n",
    "                if 3 > 2:\n",
    "                    x = WSConv2D(filters=channels, kernel_regularizer = None,\n",
    "                                     kernel_size=kernel,\n",
    "                                     strides=stride, padding=padding, use_bias=False).apply(x)\n",
    "                else:\n",
    "                    x = tf.layers.conv2d(x, filters=channels, kernel_regularizer = None,\n",
    "                                     kernel_size=kernel, kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                     strides=stride, padding=padding, use_bias=False)\n",
    "    \n",
    "                x = x * mask_ratio\n",
    "                \n",
    "                \n",
    "\n",
    "                if use_bias:\n",
    "                    bias = tf.get_variable(\"bias\", [channels], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "                    x = tf.nn.bias_add(x, bias)\n",
    "                    x = x * update_mask\n",
    "\n",
    "        else :\n",
    "            if 3 > 2:\n",
    "                x = WSConv2D(filters=channels,kernel_regularizer = None,\n",
    "                                 kernel_size=kernel,\n",
    "                                 strides=stride, padding=padding, use_bias=use_bias).apply(x)\n",
    "            else:\n",
    "                x = tf.layers.conv2d(x, filters=channels,kernel_regularizer = None,\n",
    "                                 kernel_size=kernel, kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                 strides=stride, padding=padding, use_bias=use_bias)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "def conv_swish_gn(inp, \n",
    "                 is_training, \n",
    "                 kernel_size,\n",
    "                 scope,\n",
    "                 filters, \n",
    "                 keep_rate,\n",
    "                 stride = (1, 1),\n",
    "                 activation = True,\n",
    "                 use_bias = False,\n",
    "                 norm = True,\n",
    "                 dropblock = True,\n",
    "                 csse = True,\n",
    "                 weight_decay = None,\n",
    "                 block_size = 5,\n",
    "                 padding = \"SAME\",\n",
    "                 partial = True, window_size = 5):\n",
    "    '''2D convolution, batch renorm, relu block, 3x3 drop block. \n",
    "       Use_bias must be set to False for batch normalization to work. \n",
    "       He normal initialization is used with batch normalization.\n",
    "       RELU is better applied after the batch norm.\n",
    "       DropBlock performs best when applied last, according to original paper.\n",
    "\n",
    "         Parameters:\n",
    "          inp (tf.Variable): input layer\n",
    "          is_training (str): flag to differentiate between train/test ops\n",
    "          kernel_size (int): size of convolution\n",
    "          scope (str): tensorflow variable scope\n",
    "          filters (int): number of filters for convolution\n",
    "          clipping_params (dict): specifies clipping of \n",
    "                                  rmax, dmax, rmin for renormalization\n",
    "          activation (bool): whether to apply RELU\n",
    "          use_bias (str): whether to use bias. Should always be false\n",
    "\n",
    "         Returns:\n",
    "          bn (tf.Variable): output of Conv2D -> Batch Norm -> RELU\n",
    "        \n",
    "         References:\n",
    "          http://papers.nips.cc/paper/8271-dropblock-a-regularization-\n",
    "              method-for-convolutional-networks.pdf\n",
    "          https://arxiv.org/abs/1702.03275\n",
    "          \n",
    "    '''\n",
    "    \n",
    "    bn_flag = \"Group Norm\" if norm else \"\"\n",
    "    activation_flag = \"RELU\" if activation else \"Linear\"\n",
    "    csse_flag = \"CSSE\" if csse else \"No CSSE\"\n",
    "    bias_flag = \"Bias\" if use_bias else \"NoBias\"\n",
    "    drop_flag = \"DropBlock\" if dropblock else \"NoDrop\"\n",
    "        \n",
    "    \n",
    "    print(\"{} {} Conv 2D {} {} {} {} {}\".format(scope, kernel_size,\n",
    "                                                   bn_flag, activation_flag,\n",
    "                                                   csse_flag, bias_flag, drop_flag))\n",
    "    \n",
    "    with tf.variable_scope(scope + \"_conv\"):\n",
    "        if not partial:\n",
    "            conv = Conv2D(filters = filters, kernel_size = (kernel_size, kernel_size),  strides = stride,\n",
    "                          activation = None, padding = 'valid', use_bias = use_bias,\n",
    "                          #kernel_regularizer = weight_decay,\n",
    "                          kernel_initializer = tf.keras.initializers.he_normal())(inp)\n",
    "        if partial:\n",
    "            conv = partial_conv(inp, filters, kernel=kernel_size, stride=1, norm = norm,\n",
    "                                use_bias= use_bias, padding=padding, scope = scope)\n",
    "    if activation:\n",
    "        conv = tf.nn.swish(conv)\n",
    "    if dropblock:\n",
    "        _mask = DropBlockMask(keep_prob=keep_rate, block_size= block_size)\n",
    "        mask = _mask(conv, is_training)\n",
    "    else:\n",
    "        _mask = DropBlockMask(keep_prob=1., block_size= block_size)\n",
    "        mask = _mask(conv, is_training)\n",
    "    if norm:\n",
    "        if filters > 80:\n",
    "            G = 40\n",
    "        if filters == 80:\n",
    "            G = 20\n",
    "        if filters == 40:\n",
    "            G = 10\n",
    "        #conv = tf.layers.batch_normalization(conv, training=is_training)\n",
    "        conv = weighted_group_norm(x = conv, mask = mask, scope = scope, G = 8, window_size = window_size)\n",
    "        #conv = group_norm(x = conv, scope = scope, G = 8, window_size = window_size)\n",
    "        \n",
    "    if csse:\n",
    "        conv = csse_block(conv, \"csse_\" + scope)\n",
    "    if dropblock: \n",
    "        with tf.variable_scope(scope + \"_drop\"):\n",
    "            drop_block = DoDropBlock(keep_prob=keep_rate, block_size= block_size)\n",
    "            #drop_block = DropBlock(keep_prob=keep_rate, block_size= block_size)\n",
    "            conv = drop_block([conv, mask], is_training)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "reg = tf.contrib.layers.l2_regularizer(0.)\n",
    "temporal_model = True\n",
    "input_size = 28\n",
    "n_bands = 17\n",
    "output_size = 14\n",
    "\n",
    "if temporal_model:\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, LEN + 1, input_size, input_size, n_bands))\n",
    "    length = tf.placeholder_with_default(np.full((1,), LEN + 1), shape = (None,))\n",
    "else:\n",
    "    inp = tf.placeholder(tf.float32, shape=(None, input_size, input_size, n_bands))\n",
    "    \n",
    "labels = tf.placeholder(tf.float32, shape=(None, output_size, output_size))#, 1))\n",
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training') # For DropBlock\n",
    "alpha = tf.placeholder(tf.float32, shape = ()) # For loss scheduling\n",
    "ft_lr = tf.placeholder_with_default(0.1, shape = ()) # For loss scheduling\n",
    "init_lr = tf.placeholder_with_default(0.001, shape = ()) # For loss scheduling\n",
    "loss_weight = tf.placeholder_with_default(1.0, shape = ())\n",
    "beta_ = tf.placeholder_with_default(0.0, shape = ()) # For loss scheduling, not currently implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_rate = tf.placeholder_with_default(1.0, ()) # For DropBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU input shape (?, 4, 28, 28, 17), zoneout: 0.5\n",
      "(?, 4, 28, 28, 17)\n",
      "(?, 4, 28, 28, 17)\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p37/gpu_cuda10.0/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "(3, 3, 49, 64)\n",
      "(3, 3, 49, 64)\n",
      "Zoneout: 0.75\n",
      "Down block output shape (?, 28, 28, 64)\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p37/gpu_cuda10.0/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p37/gpu_cuda10.0/lib/python3.7/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "conv_median 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Median conv: (?, 28, 28, 64)\n",
      "conv_concat 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Concat: (?, 28, 28, 64)\n",
      "conv1 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Conv1: (?, 12, 12, 128)\n",
      "conv2 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "Encoded (?, 4, 4, 256)\n",
      "up2 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "(?, 8, 8, 128)\n",
      "up2_out 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "up3 3 Conv 2D Group Norm RELU CSSE NoBias DropBlock\n",
      "(?, 16, 16, 64)\n",
      "(?, 16, 16, 64)\n",
      "out 3 Conv 2D Group Norm RELU CSSE NoBias NoDrop\n",
      "The output is (?, 8, 8, 128), with a receptive field of 1\n",
      "The output, sigmoid is (?, 14, 14, 1), with a receptive field of 1\n"
     ]
    }
   ],
   "source": [
    "# master modmel is 32, 64, 96, 230k paramms\n",
    "initial_flt = 64\n",
    "mid_flt = initial_flt * 2\n",
    "high_flt = initial_flt * 2 * 2\n",
    "INPUT_SIZE = 28\n",
    "SIZE_X = 28\n",
    "\n",
    "#inp = ReflectionPadding5D((1, 1))(inp)\n",
    "gru_input = inp[:, :-1, ...]\n",
    "#gru_input = tf.concat([inp[:, :-1, ..., :10], inp[:, :-1, ..., 11:]], -1)\n",
    "gru, steps = gru_block(inp = gru_input, length = length,\n",
    "                            size = [INPUT_SIZE, SIZE_X, ], # + 2 here for refleclt pad\n",
    "                            flt = initial_flt // 2,\n",
    "                            scope = 'down_16',\n",
    "                            train = is_training)\n",
    "with tf.variable_scope(\"gru_drop\"):\n",
    "    _mask = DropBlockMask(keep_prob=keep_rate, block_size= 5)\n",
    "    mask = _mask(gru, is_training)\n",
    "    drop_block = DoDropBlock(keep_prob=keep_rate, block_size=5)\n",
    "    gru = drop_block([gru, mask], is_training)\n",
    "    \n",
    "# Median conv\n",
    "median_input = inp[:, -1, ...]\n",
    "median_conv = conv_swish_gn(inp = median_input, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_median', filters = initial_flt, \n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, window_size = 15)\n",
    "\n",
    "print(f\"Median conv: {median_conv.shape}\")\n",
    "\n",
    "concat1 = tf.concat([gru, median_conv], axis = -1)\n",
    "concat = conv_swish_gn(inp = concat1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv_concat', filters = initial_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, padding = \"SAME\", window_size = 15)\n",
    "print(f\"Concat: {concat.shape}\")\n",
    "\n",
    "    \n",
    "# MaxPool-conv-swish-GroupNorm-csse\n",
    "pool1 = MaxPool2D()(concat)\n",
    "conv1 = conv_swish_gn(inp = pool1, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv1', filters = mid_flt,\n",
    "            keep_rate = keep_rate, activation = True, use_bias = False, norm = True, padding = \"VALID\",\n",
    "            csse = True, dropblock = True, weight_decay = None, window_size = 7)\n",
    "print(f\"Conv1: {conv1.shape}\")\n",
    "\n",
    "# MaxPool-conv-swish-csse-DropBlock\n",
    "pool2 = MaxPool2D()(conv1)\n",
    "conv2 = conv_swish_gn(inp = pool2, is_training = is_training, stride = (1, 1),\n",
    "            kernel_size = 3, scope = 'conv2', filters = high_flt, \n",
    "            keep_rate = keep_rate, activation = True,  use_bias = False, norm = True,\n",
    "            csse = True, dropblock = True, weight_decay = None, block_size = 4, padding = \"VALID\",\n",
    "                     window_size = 1)\n",
    "print(\"Encoded\", conv2.shape)\n",
    "\n",
    "# Decoder 4 - 8, upsample-conv-swish-csse-concat-conv-swish\n",
    "up2 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(conv2)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2', filters = mid_flt, \n",
    "                    keep_rate = keep_rate, activation = True, use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None, window_size = 3)\n",
    "conv1_crop = Cropping2D(2)(conv1)\n",
    "print(conv1_crop.shape)\n",
    "up2 = tf.concat([up2, conv1_crop], -1)\n",
    "#up2 = ReflectionPadding2D((1, 1,))(up2)\n",
    "up2 = conv_swish_gn(inp = up2, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up2_out', filters = mid_flt, \n",
    "                    keep_rate =  keep_rate, activation = True,  use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None, window_size = 3)\n",
    "\n",
    "# Decoder 8 - 14 upsample-conv-swish-csse-concat-conv-swish\n",
    "up3 = tf.keras.layers.UpSampling2D((2, 2), interpolation = 'nearest')(up2)\n",
    "#up3 = ReflectionPadding2D((1, 1,))(up3)\n",
    "up3 = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'up3', filters = initial_flt, \n",
    "                    keep_rate = keep_rate, activation = True,  use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = True, weight_decay = None)\n",
    "gru_crop = Cropping2D(6)(concat)\n",
    "print(up3.shape)\n",
    "print(gru_crop.shape)\n",
    "up3 = tf.concat([up3, gru_crop], -1)\n",
    "\n",
    "up3 = conv_swish_gn(inp = up3, is_training = is_training, stride = (1, 1),\n",
    "                    kernel_size = 3, scope = 'out', filters = initial_flt, \n",
    "                    keep_rate  = keep_rate, activation = True,  use_bias = False, norm = True,\n",
    "                    csse = True, dropblock = False, weight_decay = None, padding = \"VALID\")\n",
    "\n",
    "\n",
    "#print(\"Initializing last sigmoid bias with -2.94 constant\")\n",
    "init = tf.constant_initializer([-np.log(0.68/0.32)]) # For focal loss\n",
    "print(f\"The output is {up2.shape}, with a receptive field of {1}\")\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1),\n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init,\n",
    "           )(up3) # For focal loss\n",
    "#fm = Cropping2D(1)(fm)\n",
    "\n",
    "print(f\"The output, sigmoid is {fm.shape}, with a receptive field of {1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 1277321 parameters\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(f\"This model has {total_parameters} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "import math\n",
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "\n",
    "def calc_mask(seg):\n",
    "\n",
    "    res = np.zeros_like(seg)\n",
    "    posmask = seg.astype(np.bool)\n",
    "    loss_importance = np.array([x for x in range(0, 197, 1)])\n",
    "    loss_importance = loss_importance / 196\n",
    "    loss_importance = np.expm1(loss_importance)\n",
    "    loss_importance[:30] = 0.\n",
    "\n",
    "    if posmask.any():\n",
    "        negmask = ~posmask\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n",
    "    if np.sum(seg) == 196:\n",
    "        res = np.ones_like(seg)\n",
    "    if np.sum(seg) == 0:\n",
    "        res = np.ones_like(seg)\n",
    "    res[np.logical_and(res < 2, res > 0)] = 0.5\n",
    "    res[np.logical_or(res >= 2, res <= 0)] = 1.\n",
    "    return res\n",
    "\n",
    "def calc_mask_batch(y_true):\n",
    "    '''Applies calc_dist_map to each sample in an input batch\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "    '''\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    bce_batch = np.array([calc_mask(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "    return bce_batch\n",
    "\n",
    "\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight, mask = True, smooth = 0.06):\n",
    "    '''Calculates the weighted binary cross entropy loss between y_true and\n",
    "       y_pred with optional masking and smoothing for regularization\n",
    "       \n",
    "       For smoothing, we want to weight false positives as less important than\n",
    "       false negatives, so we smooth false negatives 2x as much. \n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          y_pred (arr):\n",
    "          weight (float):\n",
    "          mask (arr):\n",
    "          smooth (float):\n",
    "\n",
    "         Returns:\n",
    "          loss (float):\n",
    "    '''\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    y_true = K.clip(y_true, 0.0125, 1. - smooth)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        y_true,\n",
    "        logit_y_pred,\n",
    "        weight,\n",
    "    )\n",
    "\n",
    "    return loss \n",
    "\n",
    "\n",
    "def calc_dist_map(seg):\n",
    "    #Utility function for calc_dist_map_batch that calculates the loss\n",
    "    #   importance per pixel based on the surface distance function\n",
    "    \n",
    "     #    Parameters:\n",
    "    #      seg (arr):\n",
    "     #     \n",
    "    #     Returns:\n",
    "    #      res (arr):\n",
    "    #\n",
    "    res = np.zeros_like(seg)\n",
    "    posmask = seg.astype(np.bool)\n",
    "\n",
    "    mults = np.ones_like(seg)\n",
    "    ones = np.ones_like(seg)\n",
    "    for x in range(1, res.shape[0] -1 ):\n",
    "        for y in range(1, res.shape[0] - 1):\n",
    "            # If > 1 px, double the weight of the positive\n",
    "            # If == 1 px, half the weight of the negative\n",
    "            if seg[x, y] == 1:\n",
    "                l = seg[x - 1, y]\n",
    "                r = seg[x + 1, y]\n",
    "                u = seg[x, y + 1]\n",
    "                d = seg[x, y - 1]\n",
    "                lu = seg[x - 1, y + 1]\n",
    "                ru = seg[x + 1, y + 1]\n",
    "                rd = seg[x + 1, y - 1]\n",
    "                ld = seg[x -1, y - 1]\n",
    "                \n",
    "                sums = (l + r + u + d)\n",
    "                sums2 = (l + r + u + d + lu + ru +rd + ld)\n",
    "                if sums >= 2:\n",
    "                    mults[x, y] = 2\n",
    "                if sums2 <= 1:\n",
    "                    ones[x - 1, y] = 0.5\n",
    "                    ones[x + 1, y] = 0.5\n",
    "                    ones[x, y + 1] = 0.5\n",
    "                    ones[x, y - 1] = 0.5\n",
    "                    ones[x - 1, y + 1] = 0.5\n",
    "                    ones[x + 1, y + 1] = 0.5\n",
    "                    ones[x + 1, y - 1] = 0.5\n",
    "                    ones[x -1, y - 1] = 0.5\n",
    "\n",
    "    if posmask.any():\n",
    "        \n",
    "        negmask = ~posmask\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n",
    "        # When % = 1, 0 -> 1.75\n",
    "        # When % = 100, 0 -> 0\n",
    "        res = np.round(res, 0)\n",
    "        res[np.where(np.isclose(res, -.41421356, rtol = 1e-2))] = -1\n",
    "        res[np.where(res == -1)] = -1 * mults[np.where(res == -1)]\n",
    "        res[np.where(res == 0)] = -1  * mults[np.where(res == 0)]\n",
    "        # When % = 1, 1 -> 0\n",
    "        # When % = 100, 1 -> 1.75\n",
    "        res[np.where(res == 1)] = 1 * ones[np.where(res == 1)]\n",
    "        res[np.where(res == 1)] *= 0.67\n",
    "        #res[np.where(np.isclose(res, 1.41421356, rtol = 1e-2))] = loss_importance[sums]\n",
    "        \n",
    "    res[np.where(res < -3)] = -3\n",
    "    res[np.where(res > 3)] = 3\n",
    "    if np.sum(seg) == 196:\n",
    "        res = np.ones_like(seg)\n",
    "        res *= -1\n",
    "    if np.sum(seg) == 0:\n",
    "        res = np.ones_like(seg)\n",
    "    return res\n",
    "\n",
    "\n",
    "def calc_dist_map_batch(y_true):\n",
    "    '''Applies calc_dist_map to each sample in an input batch\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "    '''\n",
    "    y_true_numpy = y_true.numpy()\n",
    "    return np.array([calc_dist_map(y)\n",
    "                     for y in y_true_numpy]).astype(np.float32)\n",
    "\n",
    "def surface_loss(y_true, y_pred):\n",
    "    '''Calculates the mean surface loss for the input batch\n",
    "       by multiplying the distance map by y_pred\n",
    "    \n",
    "         Parameters:\n",
    "          y_true (arr):\n",
    "          y_pred (arr):\n",
    "          \n",
    "         Returns:\n",
    "          loss (arr):\n",
    "        \n",
    "         References:\n",
    "          https://arxiv.org/abs/1812.07032\n",
    "    '''\n",
    "    y_true_dist_map = tf.py_function(func=calc_dist_map_batch,\n",
    "                                     inp=[y_true],\n",
    "                                     Tout=tf.float32)\n",
    "    y_true_dist_map = tf.stack(y_true_dist_map, axis = 0)\n",
    "    multipled = y_pred * y_true_dist_map\n",
    "    loss = tf.reduce_mean(multipled, axis = (1, 2, 3))\n",
    "    return loss\n",
    "\n",
    "def tf_percentile(x, p):\n",
    "    with tf.name_scope('percentile'):\n",
    "        y = tf.transpose(x)  # take percentile over batch dimension\n",
    "        sorted_y = tf.sort(y)\n",
    "        frac_idx = tf.cast(p, tf.float64) / 100. * (tf.cast(tf.shape(y)[-1], tf.float64) - 1.)\n",
    "        return 0.5 * (  # using midpoint rule\n",
    "            tf.gather(sorted_y, tf.cast(tf.math.ceil(frac_idx), tf.int32), axis=-1)\n",
    "            + tf.gather(sorted_y, tf.cast(tf.math.floor(frac_idx), tf.int32), axis=-1))\n",
    "\n",
    "def lovasz_surf(y_true, y_pred, alpha, weight, beta):\n",
    "    \n",
    "    #lv = lovasz_softmax(probas = y_pred,\n",
    "    #                    labels = tf.reshape(y_true, (-1, 14, 14)), \n",
    "    #                    classes=[1],\n",
    "    #                    per_image=False) \n",
    "    \n",
    "    bce = weighted_bce_loss(y_true = y_true, \n",
    "                             y_pred = y_pred, \n",
    "                             weight = weight,\n",
    "                             smooth = 0.0625)\n",
    "\n",
    "\n",
    "    bce = tf.reduce_mean(bce, axis = (1, 2, 3))\n",
    "    surface = surface_loss(tf.cast(tf.math.greater(y_true, 0.1), tf.float32), y_pred)\n",
    "    surface = tf.reshape(surface, tf.shape(bce))\n",
    "    print(surface.shape)\n",
    "\n",
    "    bce = (1 - alpha) * bce\n",
    "    surface_portion = alpha * surface\n",
    "    \n",
    "    #result = bce + lovasz\n",
    "    result = bce + surface_portion\n",
    "    #upper_bound = tf_percentile(result, 90)\n",
    "    #result = tf.clip_by_value(result, tf.reduce_min(result), upper_bound)\n",
    "    result = tf.reduce_mean(result)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model with: \n",
      " 0.9 zone out \n",
      " 0.0 l2 \n",
      "5e-05 initial LR \n",
      " 1277321 parameters\n",
      "(?,)\n",
      "(?,)\n"
     ]
    }
   ],
   "source": [
    "def grad_norm(gradients):\n",
    "        norm = tf.compat.v1.norm(\n",
    "            tf.stack([\n",
    "                tf.compat.v1.norm(grad) for grad in gradients if grad is not None\n",
    "            ])\n",
    "        )\n",
    "        return norm\n",
    "\n",
    "FRESH_START = True\n",
    "print(f\"Starting model with: \\n {ZONE_OUT_PROB} zone out \\n {L2_REG} l2 \\n\"\n",
    "      f\"{INITIAL_LR} initial LR \\n {total_parameters} parameters\")  \n",
    "\n",
    "finetune_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_15\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_17\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"outregressor\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_outregressor\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"outregressor_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"outregressor2\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_outregressor2\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"conv2d_13\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up3\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up3_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up3\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_out_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up2_out\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"up2_drop\") + \\\n",
    "                tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"csse_up2\")# + \\\n",
    "\n",
    "if FRESH_START:\n",
    "    # We use the Adabound optimizer\n",
    "    \n",
    "    train_loss = lovasz_surf(tf.reshape(labels, (-1, 14, 14, 1)), \n",
    "                             fm, weight = loss_weight, \n",
    "                             alpha = alpha, beta = beta_)\n",
    "    \n",
    "    # If there is any L2 regularization, add it. Current model does not use\n",
    "    #l2_loss = tf.losses.get_regularization_loss()\n",
    "    #if len(tf.losses.get_regularization_losses()) > 0:\n",
    "    #    train_loss = train_loss + l2_loss\n",
    "        \n",
    "    # If necessary to switch to SGD at any point, make that optimizer here\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_loss = lovasz_surf(tf.reshape(labels, (-1, 14, 14, 1)),\n",
    "                            fm, weight = loss_weight, \n",
    "                            alpha = alpha, beta = beta_)\n",
    "    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        #optimizer = AdaBeliefOptimizer(init_lr,epsilon=1e-14,\n",
    "        #                               warmup_proportion=0.05,\n",
    "        #weight_decay=1e-4,\n",
    "        #rectify=True,\n",
    "        #total_steps=1050*200)\n",
    "        #gstep = tf.train.get_or_create_global_step()\n",
    "        optimizer = AdaBoundOptimizer(init_lr, ft_lr, weight_decay = 2e-5)\n",
    "        ft_optimizer = tf.train.MomentumOptimizer(ft_lr, momentum = 0.8, use_nesterov = True)\n",
    "        #train_op = optimizer.minimize(train_loss, global_step = gstep)#, var_list = finetune_vars)   \n",
    "        ft_op = ft_optimizer.minimize(train_loss, var_list = finetune_vars)\n",
    "\n",
    "        # The following code blocks are for sharpness aware minimization\n",
    "        # Adapted from https://github.com/sayakpaul/Sharpness-Aware-Minimization-TensorFlow\n",
    "        # For tensorflow 1.15\n",
    "\n",
    "        trainable_params = tf.trainable_variables()\n",
    "        gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "        gradient_norm = grad_norm(gradients)\n",
    "        scale = 0.05 / (gradient_norm + 1e-12)\n",
    "        e_ws = []\n",
    "        for (grad, param) in gradients:\n",
    "            e_w = grad * scale\n",
    "            param.assign_add(e_w)\n",
    "            e_ws.append(e_w)\n",
    "\n",
    "        sam_gradients = optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "        for (param, e_w) in zip(trainable_params, e_ws):\n",
    "            param.assign_sub(e_w)\n",
    "        train_step = optimizer.apply_gradients(sam_gradients)#, global_step = gstep)\n",
    "        \n",
    "        gradients2 = ft_optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "        gradient_norm2 = grad_norm(gradients2)\n",
    "        scale2 = 0.05 / (gradient_norm2 + 1e-12)\n",
    "        e_ws2 = []\n",
    "        for (grad, param) in gradients2:\n",
    "            e_w2 = grad * scale\n",
    "            param.assign_add(e_w2)\n",
    "            e_ws2.append(e_w2)\n",
    "\n",
    "        sam_gradients2 = ft_optimizer.compute_gradients(loss=train_loss, var_list=None)\n",
    "        for (param, e_w) in zip(trainable_params, e_ws2):\n",
    "            param.assign_sub(e_w)\n",
    "        ft_step = ft_optimizer.apply_gradients(sam_gradients2)#, global_step = gstep)\n",
    "    \n",
    "    # Create a saver to save the model each epoch\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting anew\n"
     ]
    }
   ],
   "source": [
    "model_path  = \"models/rmapper/2023-feb9/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")\n",
    "else:\n",
    "    print(\"Starting anew\")\n",
    "    metrics = np.zeros((6, 300))\n",
    "\n",
    "path = model_path\n",
    "#saver.restore(sess, tf.train.latest_checkpoint(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "# SWA BLOCKS\n",
    "model_vars = tf.trainable_variables()\n",
    "swa = StochasticWeightAveraging()\n",
    "swa_op = swa.apply(var_list=model_vars)\n",
    "with tf.variable_scope('BackupVariables'):\n",
    "    # force tensorflow to keep theese new variables on the CPU ! \n",
    "    backup_vars = [tf.get_variable(var.op.name, dtype=var.value().dtype, trainable=False,\n",
    "                                   initializer=var.initialized_value())\n",
    "                   for var in model_vars]\n",
    "\n",
    "# operation to assign SWA weights to model\n",
    "swa_to_weights = tf.group(*(tf.assign(var, swa.average(var).read_value()) for var in model_vars))\n",
    "# operation to store model into backup variables\n",
    "save_weight_backups = tf.group(*(tf.assign(bck, var.read_value()) for var, bck in zip(model_vars, backup_vars)))\n",
    "# operation to get back values from backup variables to model\n",
    "restore_weight_backups = tf.group(*(tf.assign(var, bck.read_value()) for var, bck in zip(model_vars, backup_vars)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "\n",
    "    # for i in not_initialized_vars: # only for testing\n",
    "    #    print(i.name)\n",
    "\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "initialize_uninitialized(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "*  Load in CSV data from Collect Earth\n",
    "*  Reconstruct the X, Y grid for the Y data per sample\n",
    "*  Calculate NDVI, EVI, SAVI, BI, MSAVI2, and SI\n",
    "*  Stack X, Y, length data\n",
    "*  Apply median filter to DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17763, 12, 28, 28, 13)\n"
     ]
    }
   ],
   "source": [
    "import hickle as hkl\n",
    "normalize = False\n",
    "train_x = hkl.load(\"data/master-may-2023/train_x.hkl\")\n",
    "train_y = hkl.load(\"data/master-may-2023/train_y.hkl\")\n",
    "data = pd.read_csv(\"data/master-may-2023/train_x.csv\")\n",
    "\n",
    "if not isinstance(train_x.flat[0], np.floating):\n",
    "    assert np.max(train_x) > 1\n",
    "    train_x = train_x.astype(np.float32) / 65535.\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_db(x, min_db):\n",
    "    x = 10 * np.log10(x + 1/65535)\n",
    "    x[x < -min_db] = -min_db\n",
    "    x = x + min_db\n",
    "    x = x / min_db\n",
    "    x = np.clip(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "def grndvi(array):\n",
    "    nir = np.clip(array[..., 3], 0, 1)\n",
    "    green = np.clip(array[..., 1], 0, 1)\n",
    "    red = np.clip(array[..., 2], 0, 1)\n",
    "    denominator = (nir+(green+red)) + 1e-5\n",
    "    return (nir-(green+red)) / denominator\n",
    "\n",
    "def evi(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates the enhanced vegetation index\n",
    "    2.5 x (08 - 04) / (08 + 6 * 04 - 7.5 * 02 + 1)\n",
    "    '''\n",
    "\n",
    "    BLUE = x[..., 0]\n",
    "    GREEN = x[..., 1]\n",
    "    RED = x[..., 2]\n",
    "    NIR = x[..., 3]\n",
    "    evis = 2.5 * ( (NIR-RED) / (NIR + (6*RED) - (7.5*BLUE) + 1))\n",
    "    evis = np.clip(evis, -1.5, 1.5)\n",
    "    return evis\n",
    "\n",
    "def msavi2(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates the modified soil-adjusted vegetation index 2\n",
    "    (2 * NIR + 1 - sqrt((2*NIR + 1)^2 - 8*(NIR-RED)) / 2\n",
    "    '''\n",
    "    BLUE = x[..., 0]\n",
    "    GREEN = x[..., 1]\n",
    "    RED = np.clip(x[..., 2], 0, 1)\n",
    "    NIR = np.clip(x[..., 3], 0, 1)\n",
    "\n",
    "    msavis = (2 * NIR + 1 - np.sqrt( (2*NIR+1)**2 - 8*(NIR-RED) )) / 2\n",
    "    return msavis\n",
    "\n",
    "def bi(x: np.ndarray, verbose: bool = False) -> np.ndarray:\n",
    "    B11 = np.clip(x[..., 8], 0, 1)\n",
    "    B4 = np.clip(x[..., 2], 0, 1)\n",
    "    B8 = np.clip(x[..., 3], 0, 1)\n",
    "    B2 = np.clip(x[..., 0], 0, 1)\n",
    "    bis = ((B11 + B4) - (B8 + B2)) / ((B11 + B4) + (B8 + B2))\n",
    "    return bis\n",
    "\n",
    "def ndwi(array):\n",
    "    return (array[..., 1] - array[..., 3]) / (array[..., 1] + array[..., 3])\n",
    "\n",
    "def ndmi(arr):\n",
    "    return (arr[..., 3] - arr[..., 8]) / (arr[..., 3] + arr[..., 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17763, 13, 28, 28, 17)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.float32(train_x)\n",
    "train_x[..., -1] = convert_to_db(train_x[..., -1], 22)\n",
    "train_x[..., -2] = convert_to_db(train_x[..., -2], 22)\n",
    "\n",
    "indices = np.zeros((train_x.shape[0], train_x.shape[1], 28, 28, 4), dtype = np.float32)\n",
    "indices[..., 0] = evi(train_x)\n",
    "indices[..., 1] = bi(train_x)\n",
    "indices[..., 2] = msavi2(train_x)\n",
    "indices[..., 3] = grndvi(train_x)\n",
    "#indices[..., 4] = ndwi(train_x)\n",
    "#indices[..., 5] = ndmi(train_x)\n",
    "\n",
    "train_x = np.concatenate([train_x, indices], axis = -1)\n",
    "med = np.median(train_x, axis = 1)\n",
    "med = med[:, np.newaxis, :, :, :]\n",
    "train_x = np.concatenate([train_x, med], axis = 1)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data preprocessing\n",
    "\n",
    "*  Identify and remove samples with time steps / channels that have a 0. or 1. value, which indicates missing data\n",
    "*  Identify and remove samples with time steps / channels with no variation, which indicates missing data\n",
    "*  Identify and remove samples with values above or below the allowable values for the band\n",
    "*  Identify and remove samples with null data, or samples with extreme band 0 data (which squash all the \"clean\" samples)\n",
    "*  Smooth per-pixel temporal data with Whittaker smoother, d = 2, lambda = 0.5 to reduce sample noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_all = [0.006576638437476157, 0.0162050812542916, 0.010040436408026246, 0.013351644159609368, \n",
    "           0.01965362020294499, 0.014229037918669413, 0.015289539940489814, 0.011993591210803388,\n",
    "           0.008239871824216068, 0.006546120393682765, 0.0, 0.0, 0.0, -0.1409399364817101,\n",
    "           -0.4973397113668104, -0.09731556326714398, -0.7193834232943873]\n",
    "max_all = [0.2691233691920348, 0.3740291447318227, 0.5171435111009385, 0.6027466239414053,\n",
    "           0.5650263218127718, 0.5747005416952773, 0.5933928435187305, 0.6034943160143434, \n",
    "           0.7472037842374304, 0.7000076295109483, 0.509269855802243, 0.948334642387533,\n",
    "           0.6729257769285485, 0.8177635298774327, 0.35768999002433816, 0.7545951919107605, 0.7602693339366691]\n",
    "\n",
    "#min_all = [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -1, -1, -1, -1]\n",
    "#max_all = [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_x(extra_x):\n",
    "\n",
    "    if not isinstance(extra_x.flat[0], np.floating):\n",
    "        assert np.max(extra_x) > 1\n",
    "        extra_x = extra_x / 65535.\n",
    "\n",
    "    extra_x[..., -1] = convert_to_db(extra_x[..., -1], 22)\n",
    "    extra_x[..., -2] = convert_to_db(extra_x[..., -2], 22)\n",
    "    \n",
    "    indices = np.zeros((extra_x.shape[0], extra_x.shape[1], 28, 28, 4), dtype = np.float32)\n",
    "    indices[..., 0] = evi(extra_x)\n",
    "    indices[..., 1] = bi(extra_x)\n",
    "    indices[..., 2] = msavi2(extra_x)\n",
    "    indices[..., 3] = grndvi(extra_x)\n",
    "    #indices[..., 4] = ndwi(extra_x)\n",
    "    #indices[..., 5] = ndmi(extra_x)\n",
    "\n",
    "    extra_x = np.concatenate([extra_x, indices], axis = -1)\n",
    "    med = np.median(extra_x, axis = 1)\n",
    "    med = med[:, np.newaxis, :, :, :]\n",
    "    extra_x = np.concatenate([extra_x, med], axis = 1)\n",
    "\n",
    "    print(f\"The extra data is shape {extra_x.shape}, {extra_y.shape}\")\n",
    "    return extra_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(975, 12, 28, 28, 13)\n",
      "(975, 5, 28, 28, 17)\n",
      "There are 0 outliers: []\n",
      "[]\n",
      "(975, 5, 28, 28, 17)\n"
     ]
    }
   ],
   "source": [
    "import hickle as hkl\n",
    "test_x = hkl.load(\"data/test/test_x.hkl\")\n",
    "test_y = hkl.load(\"data/test/test_y.hkl\")\n",
    "test_data = pd.read_csv(\"data/test/test_plot_ids.csv\")\n",
    "\n",
    "test_x = np.delete(test_x, 11, -1)\n",
    "\n",
    "#test_x = np.delete(test_x, [8, 9], axis = -1)\n",
    "print(test_x.shape)\n",
    "\n",
    "if not isinstance(test_x.flat[0], np.floating):\n",
    "    assert np.max(test_x) > 1\n",
    "    test_x = test_x / 65535.\n",
    "\n",
    "test_x = np.float32(test_x)\n",
    "test_x[..., -1] = convert_to_db(test_x[..., -1], 22)\n",
    "test_x[..., -2] = convert_to_db(test_x[..., -2], 22)\n",
    "\n",
    "#s1 = test_x[..., 11:13]\n",
    "#s1 = np.reshape(s1, (s1.shape[0], s1.shape[1], 12, 2, 12, 2, 2))\n",
    "#s1 = np.mean(s1, axis = (3, 5))\n",
    "#s1 = resize(s1, (s1.shape[0], 12, 24, 24, 2), order = 0)\n",
    "#test_x[..., 11:13] = s1\n",
    "\n",
    "indices = np.zeros((test_x.shape[0], 12, 28, 28, 4), dtype = np.float32)\n",
    "indices[..., 0] = evi(test_x)\n",
    "indices[..., 1] = bi(test_x)\n",
    "indices[..., 2] = msavi2(test_x)\n",
    "indices[..., 3] = grndvi(test_x)\n",
    "#indices[..., 4] = ndwi(test_x)\n",
    "#indices[..., 5] = ndmi(test_x)\n",
    "\n",
    "test_x = np.concatenate([test_x, indices], axis = -1)\n",
    "\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], 4, 3, 28, 28, n_bands))\n",
    "test_x = np.median(test_x, axis = 2, overwrite_input = True)\n",
    "\n",
    "med = np.median(test_x, axis = 1)\n",
    "med = med[:, np.newaxis, :, :, :]\n",
    "test_x = np.concatenate([test_x, med], axis = 1)\n",
    "print(test_x.shape)\n",
    "\n",
    "\n",
    "\n",
    "below_1 = [i for i, val in enumerate(test_x[..., :-2]) if np.min(val) < -1.66]\n",
    "above_1 = [i for i, val in enumerate(test_x[..., :-2]) if np.max(val) > 1.66]\n",
    "nans = [i for i, val in enumerate(test_x) if np.sum(np.isnan(val)) > 0]\n",
    "outliers = below_1 + above_1 + nans\n",
    "outliers = list(set(outliers))\n",
    "print(\"There are {} outliers: {}\".format(len(outliers), outliers))\n",
    "print([x for x in test_data['plot_id'].iloc[outliers]])\n",
    "\n",
    "test_x = np.delete(test_x, outliers, 0)\n",
    "test_y = np.delete(test_y, outliers, 0)\n",
    "test_data = test_data.drop(outliers, 0)\n",
    "test_data = test_data.reset_index(drop = True)\n",
    "\n",
    "\n",
    "print(test_x.shape)\n",
    "#print(test_data.shape)\n",
    "#test_x = test_x[..., :-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 12 outlying testing data points\n",
      "[72, 82, 92, 105, 240, 249, 287, 392, 398, 399, 407, 910]\n",
      "Removing 3 outlying testing data points\n",
      "[647, 661, 809]\n",
      "Removing 5 outlying testing data points\n",
      "[170, 255, 540, 899, 902]\n"
     ]
    }
   ],
   "source": [
    "outliers = [72, 82, 92, 105, 240, 249, 287, 392, 398, 399, 407,910,]\n",
    "print(\"Removing {} outlying testing data points\".format(len(outliers)))\n",
    "print(sorted(outliers))\n",
    "test_x = np.delete(test_x, outliers, 0)\n",
    "test_y = np.delete(test_y, outliers, 0)\n",
    "test_data = test_data.drop(outliers, 0)\n",
    "test_data.reset_index(inplace = True, drop = True)\n",
    "\n",
    "outliers = [647, 661, 809]\n",
    "\n",
    "print(\"Removing {} outlying testing data points\".format(len(outliers)))\n",
    "print(sorted(outliers))\n",
    "test_x = np.delete(test_x, outliers, 0)\n",
    "test_y = np.delete(test_y, outliers, 0)\n",
    "test_data = test_data.drop(outliers, 0)\n",
    "test_data.reset_index(inplace = True, drop = True)\n",
    "\n",
    "outliers = [ 170, 255, 540, 899, 902, ]\n",
    "print(\"Removing {} outlying testing data points\".format(len(outliers)))\n",
    "print(sorted(outliers))\n",
    "test_x = np.delete(test_x, outliers, 0)\n",
    "test_y = np.delete(test_y, outliers, 0)\n",
    "test_data = test_data.drop(outliers, 0)\n",
    "test_data.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[383, 532, 731, 900] 4\n",
      "Removing 4 outlying testing data points\n",
      "[383, 532, 731, 900]\n"
     ]
    }
   ],
   "source": [
    "outliers = [141019215, 141019262, 141018675, 141018193, 141019296,\n",
    "            141018219, 140860675, 141019143, 141019227, 141019261,\n",
    "            141018695, 141019181, 140860629, 139270391, 139191492,\n",
    "            139190835, 141018193, 139190474, 139264567, 139190813, 139190835,\n",
    "            141018193, 139190474, 139264567, 139190813]\n",
    "outliers = test_data[test_data['plot_id'].isin(outliers)]\n",
    "outliers = list(outliers.index)\n",
    "print(outliers, len(outliers))\n",
    "\n",
    "print(\"Removing {} outlying testing data points\".format(len(outliers)))\n",
    "print(sorted(outliers))\n",
    "test_x = np.delete(test_x, outliers, 0)\n",
    "test_y = np.delete(test_y, outliers, 0)\n",
    "test_data = test_data.drop(outliers, 0)\n",
    "test_data.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 2 outlying testing data points\n",
      "[527, 699]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "outliers = [527, 699]\n",
    "\n",
    "print(\"Removing {} outlying testing data points\".format(len(outliers)))\n",
    "print(sorted(outliers))\n",
    "test_x = np.delete(test_x, outliers, 0)\n",
    "test_y = np.delete(test_y, outliers, 0)\n",
    "test_data = test_data.drop(outliers, 0)\n",
    "test_data.reset_index(inplace = True, drop = True)\n",
    "\n",
    "outliers = [417, 896]\n",
    "test_x = np.delete(test_x, outliers, 0)\n",
    "test_y = np.delete(test_y, outliers, 0)\n",
    "test_data = test_data.drop(outliers, 0)\n",
    "test_data.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outliers = [591, 1372, 1450, 1599, 5492, 5589, 5612, 6858, 6859, 9829, 9899]\n",
    "#train_x = np.delete(train_x, outliers, 0)\n",
    "#train_y = np.delete(train_y, outliers, 0)\n",
    "#data = data.drop(outliers, 0)\n",
    "#data.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023 data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extra data is shape (4886, 13, 28, 28, 17), (4886, 14, 14)\n",
      "SUBSETTED!!!!!!!\n",
      "Deleted  [[2498]\n",
      " [2745]\n",
      " [2757]] (7765, 13, 28, 28, 17) (7765, 14, 14)\n",
      "Deleted  [3635] (7764, 13, 28, 28, 17) (7764, 14, 14)\n",
      "B: 0.1935322\n",
      "A: 0.19952792\n"
     ]
    }
   ],
   "source": [
    "import hickle as hkl\n",
    "extra_x = hkl.load(\"data/meta/train_x2.hkl\").astype(np.float32) / 65535 \n",
    "extra_y = hkl.load(\"data/meta/train_y2.hkl\").astype(np.float32) \n",
    "is_one = np.argwhere(np.logical_and(np.min(extra_y, axis = (1, 2)) == 1, \n",
    "                                    np.mean(extra_y, axis = (1, 2)) == 1))\n",
    "#extra_y[is_one] *= 255\n",
    "#print(np.mean(extra_y[is_one]))\n",
    "extra_y = extra_y / 255\n",
    "extra_y = extra_y ** 0.85\n",
    "extra_d = pd.read_csv(\"data/meta/train_plot_ids2.csv\")\n",
    "\n",
    "extra_x = np.delete(extra_x, 11, -1)\n",
    "extra_x = process_x(extra_x)\n",
    "\n",
    "print(\"SUBSETTED!!!!!!!\")\n",
    "extra_x2 = np.load(\"orbit_x.npy\").astype(np.float32)\n",
    "extra_y2 = np.load(\"orbit_y.npy\").astype(np.float32)\n",
    "#indices = np.zeros((extra_x2.shape[0], 13, 28, 28, 2), dtype = np.float32)\n",
    "#indices[..., 0] = ndwi(extra_x2)\n",
    "#indices[..., 1] = ndmi(extra_x2)\n",
    "\n",
    "#extra_x2 = np.concatenate([extra_x2, indices], axis = -1)\n",
    "\n",
    "ms = np.mean(extra_x2[..., 11], axis = (1, 2, 3))\n",
    "to_remove = np.argwhere(ms[:3000] < .35)\n",
    "extra_x2 = np.delete(extra_x2, to_remove, 0)\n",
    "extra_y2 = np.delete(extra_y2, to_remove, 0)\n",
    "print(\"Deleted \", to_remove, extra_x2.shape, extra_y2.shape)\n",
    "\n",
    "to_remove_na = np.argwhere(np.sum(np.isnan(extra_y2), axis = (1, 2)) > 0).flatten()\n",
    "extra_x2 = np.delete(extra_x2, to_remove_na, 0)\n",
    "extra_y2 = np.delete(extra_y2, to_remove_na, 0)\n",
    "print(\"Deleted \", to_remove_na, extra_x2.shape, extra_y2.shape)\n",
    "\n",
    "print(\"B:\", np.mean(extra_y2[:315]))\n",
    "extra_y2[:315] = extra_y2[:315] ** 0.97\n",
    "print(\"A:\", np.mean(extra_y2[:315]))\n",
    "\n",
    "    \n",
    "extra_x = np.concatenate([extra_x, extra_x2], axis = 0)\n",
    "\n",
    "train_x = train_x.astype(np.float32)\n",
    "train_x = np.concatenate([train_x, extra_x], axis = 0)\n",
    "train_y = np.concatenate([train_y, extra_y], axis = 0)\n",
    "#train_y[-110:] *= 255 # Not withh train_x2\n",
    "train_y = np.concatenate([train_y, extra_y2], axis = 0)\n",
    "\n",
    "train_y = np.clip(train_y, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been scaled to [-1.0000001192092896, 1.0000001192092896]\n",
      "The data has been scaled to [-1.0000001192092896, 1.0000001192092896]\n"
     ]
    }
   ],
   "source": [
    "min_all = [0.006576638437476157, 0.0162050812542916, 0.010040436408026246, \n",
    "               0.013351644159609368, 0.01965362020294499, 0.014229037918669413, \n",
    "               0.015289539940489814, 0.011993591210803388, 0.008239871824216068,\n",
    "               0.006546120393682765, 0.0, 0.0, 0.0, -0.1409399364817101,\n",
    "               -0.4973397113668104, -0.09731556326714398, -0.7193834232943873,]\n",
    "\n",
    "max_all = [0.2691233691920348, 0.3740291447318227, 0.5171435111009385, \n",
    "               0.6027466239414053, 0.5650263218127718, 0.5747005416952773,\n",
    "               0.5933928435187305, 0.6034943160143434, 0.7472037842374304,\n",
    "               0.7000076295109483, \n",
    "               0.4,\n",
    "               #0.509269855802243, \n",
    "               #0.4,\n",
    "               0.948334642387533, \n",
    "               0.6729257769285485, 0.8177635298774327, 0.35768999002433816,\n",
    "               0.7545951919107605, 0.7602693339366691,]\n",
    "\n",
    "\n",
    "\n",
    "for band in range(0, train_x.shape[-1]):\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    train_x[..., band] = np.clip(train_x[..., band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (train_x[..., band] - midrange) / (rng / 2)\n",
    "    train_x[..., band] = standardized\n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(train_x), np.max(train_x)))\n",
    "\n",
    "for band in range(0, test_x.shape[-1]):\n",
    "    mins = min_all[band]\n",
    "    maxs = max_all[band]\n",
    "    test_x[..., band] = np.clip(test_x[..., band], mins, maxs)\n",
    "    midrange = (maxs + mins) / 2\n",
    "    rng = maxs - mins\n",
    "    standardized = (test_x[..., band] - midrange) / (rng / 2)\n",
    "    test_x[..., band] = standardized\n",
    "\n",
    "print(\"The data has been scaled to [{}, {}]\".format(np.min(test_x), np.max(test_x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,  28.,  13.,  57.,  17., 182.,  13.,  22., 196.,  75.,  16.,\n",
       "        12.,  49., 143.,   0.,  68., 196.,   7.,   2.,   0.,  98., 196.,\n",
       "         0.,   7.,   0., 127.,   0.,   0.,   0.,   0., 196.,   0.,   1.,\n",
       "         4.,   0.,   0., 182., 196.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addl_to_rm = [750,1100,1189,1264,1449,1648,2223,4348,4362,\n",
    "5569,6208,6798,7452,7453,7454,7461,8082,9547,9813,9814,10193,\n",
    "10281,10506,10534,10986,13225,13529,13791,13813,13968,14035,\n",
    "15518,15583,15584,15972,16862,17246,17385]\n",
    "\n",
    "np.sum(train_y,axis = (1, 2))[addl_to_rm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([189.,   4.,  33.,   0.,   0.,  31., 192., 185., 168., 119., 167.,\n",
       "       157., 196., 194., 186., 122.,   0.,   0., 196., 190.,   0., 180.,\n",
       "       196., 148., 175., 116.,  15., 185., 156., 139.,   0.,   0.,   6.,\n",
       "        23.,   0., 185., 156.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addl = [135542492,135680064,135680284,135680827,135697669,135702984,135732257,136434864,\n",
    "136434916,138303016,138948676,139047085,\n",
    "139146595,139146596,139146597,139146607,139177994,\n",
    "139189890,139207969,139207971,139277636,139291533,\n",
    "139291956,139292000,139338608,140396315,\n",
    "140750997,141237869,141237904,141238341,141370634,\n",
    "400158,500111,500113,800223,240041,2000133,2100220]\n",
    "addl = data[data['plot_id'].isin(addl)].index\n",
    "np.sum(train_y,axis = (1, 2))[addl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1404, 17563, 17567, 17584, 17585, 17590]\n"
     ]
    }
   ],
   "source": [
    "# samples w loss\n",
    "samples_w_loss  = np.load(\"samples_w_loss.npy\")\n",
    "samples_w_loss = data[data['plot_id'].isin(samples_w_loss)].index\n",
    "ghana = [135698176, 1234122, 1234099, 1234095, 1234116, 1234117]\n",
    "ghana = list(data[data['plot_id'].isin(ghana)].index)\n",
    "print(ghana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equibatch creation\n",
    "\n",
    "The modelling approach uses equibatch sampling to ensure that there is a near constant standard deviation of the percent tree cover in the output labels for each batch. This helps ensure that the model performs equally well across gradients of tree cover, by mitigating the random possibility that many batches in a row near the end of sampling may be randomly biased towards a tree cover range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kenya 56\n",
    "# rotation 65\n",
    "# pineapple 112\n",
    "# fn 71\n",
    "\n",
    "# kenya 17457, 17458, 17459\n",
    "# rotation remove 17516\n",
    "# pineapple include 17627, 28, 49-56\n",
    "# sisal 17726-17728, 17735 17736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29004, 29005, 29007, 29010, 29014, 29022, 29023, 29024, 29026, 29027, 29028, 29030, 29032, 29037, 29039, 29040, 29041, 29043, 29044, 29045, 29046, 29049, 29051, 29066, 29067, 29095, 29097, 29098, 29105, 29110, 29139, 29149, 29151, 29159, 29171, 29174, 29176, 29177, 29178, 29182, 29183, 29193, 29197, 29199, 29200, 29203, 29204, 29213, 29233, 29240, 29252, 29263, 29264, 29269, 29282, 29307, 29256, 29288, 29315, 28873, 28874, 28875, 28876, 28877, 28878, 28879, 28880, 28881, 28882, 28883, 28884, 28885, 28886, 28887, 28888, 28889, 28890, 28891, 28892, 28893, 28894, 28895, 28896, 28897, 28898, 28899, 28900, 28901, 28902, 28903, 28904, 28905, 28906, 28907, 28908, 28909, 28910, 28911, 28912, 28913, 28914, 28915, 28916, 28917, 28918, 28919, 28920, 28921, 28922, 28923, 28924, 28925, 28926, 28927, 28928, 28929, 28930, 28931, 28932, 28933, 28934, 28935, 28936, 28937, 28938, 28939, 28940, 28941, 28942, 28943, 28944, 28945, 28946, 28947, 28948, 28949, 28950, 28951, 28952, 28953, 28954, 28955, 28956, 28957, 28958, 28959, 28960, 28961, 28962, 28963, 28964, 28965, 28966, 28967, 28968, 28969, 28970, 28971, 28972, 28973, 28974, 28975, 28976, 28977, 28978, 28979, 28980, 28981, 28982, 28983, 28984, 28985, 28986, 28987, 28988, 28989, 28990, 28991, 28992, 28993, 28994, 28995, 28996, 28997, 28998, 28999, 29000, 29001, 29002, 29351, 29352, 29353, 29355, 29356, 29363, 29365, 29366, 29372, 29374, 29375, 29378, 29379, 29380, 29382, 29384, 29386, 29387, 29393, 29395, 29411, 29413, 29438, 29443, 29446, 29474, 29479, 29482, 29491]\n"
     ]
    }
   ],
   "source": [
    "pineapple = list(np.arange(17619-11, 17666, 1))\n",
    "\n",
    "kenya = [17457, 17458, 17459, 17453, 17453, 17453, 17453, 17453, 17453, 17453, 17453, 17453, 17453]\n",
    "fns = list(np.arange(train_x.shape[0] - 71 - 138 - 5401, train_x.shape[0] - 138 - 5401))\n",
    "sisal = [17726, 17728, 17735, 17736]\n",
    "wetlands = [\n",
    "    135703501,135703510,135787111,138173603,138948366,139146619,139160337,139160360,139160361,139190000,\n",
    "139207922,139208275,139208276,139208279,139208280,139365491,139365492,139365776,139379423,139379424,\n",
    "139379425,139745935,139745936,139745937,140750842,140750843,140750844,140750846,140750848,140750849,140750854,\n",
    "140750855,140750857,140750863,140750868,140750869,220039,220040,220041,220043,220044,220045,220046,\n",
    "220048,220049,220050,220052,220053\n",
    "]\n",
    "\n",
    "rotation = 17489#17677 - 65 - 112 - 71\n",
    "rotation = list(np.arange(rotation, rotation + 65, 1))\n",
    "rotation = [x for x in rotation if x != 17516]\n",
    "eurot = list(np.arange(len(train_x) -5, len(train_x) -5, 1))\n",
    "change = list(np.arange(17875 - 138, 17875, 1))\n",
    "\n",
    "pineapple = [x - 112 for x in pineapple]\n",
    "kenya = [x - 112 for x in kenya]\n",
    "fns = [x - 112 for x in fns]\n",
    "rotation = [x - 112 for x in rotation]\n",
    "change = [x - 112 for x in change]\n",
    "\n",
    "bad_plots = list(set(wetlands))\n",
    "bad_index = data[data['plot_id'].isin(bad_plots)]\n",
    "plots_to_rm = list(bad_index.index)\n",
    "\n",
    "\n",
    "malawi = [15, 17, 26, 29, 37,  42, 43, 45, 49, 54, 59, 75, 91, 105, 130,\n",
    "131, 137, 161, 169, 176, 190, 206, 208, 216, 227, 238, 247, 249, 264, 281]\n",
    "malawi = malawi + [0, 9, 11, 31, 32, 51, 73, 81, 82, 111, 120, 136, 141, 146, 151, 158,\n",
    "         166, 178, 182, 188, 206, 216, 240, 241]\n",
    "malawi = list(np.array(malawi) + train_x.shape[0] - 4199 - 300 - 35 - 27 - 137 - 81 - 129 - 84 - 130 - 343 - 211 - 5 - 134 - 717)\n",
    "malawi2 = [x for x in np.arange(len(train_x) - 985 - 46 - 72 - 35 - 27 - 137 - 81 - 129 - 84 - 130 - 343- 211- 5 - 134 - 717,\n",
    "                                len(train_x) - 938 - 46 - 72 - 35 - 27 - 137 - 81 - 129 - 84 - 130 - 343- 211- 5 - 134 - 717)]\n",
    "malawi = malawi+ 2 * (malawi2)\n",
    "#[ 174, 175,  179, 180, 190, 194, 196, 197, 200, 230, 237, 249,266, 253, 285, 312\n",
    "colorado = len(train_x) - 211 - 343 - 5 - 134 -717+ np.array([1, 2, 4, 7, 11, 19, 20,  21, 23, 24, 25, 27,  29, 34, 36,\n",
    "                                                    37, 38, 40, 41, 42, 43, 46,  48, 63, 64, 92, 94, 95, 102, \n",
    "                                                    107, 136, 146, 148, 156, 168, 171, 173, 174, 175,  179, 180, 190, 194, 196, 197, 200, 201, \n",
    "                                                    210, 230, 237, 249, 260, 261, 266, 279, 304, 253, 285, 312,]) \n",
    "                                                  #17, 19, 25, 30, 146, 148, 156, 159, 160, 165, 186, 187, 189, 228, ])\n",
    "s25 = len(train_x) - 211 - 343 - 130 - 5 - 134 - 717+ np.arange(0, 130, 1)\n",
    "#np.array([3, 5, 16, 18, 20, 24, 28, 32, 34, 36, 38, 39, 42, 44, 45, 46, \n",
    "                                               #     6, 52, 67, 69, 70, 72, 73, 74, 75,77,  81, 82, 86, \n",
    "                                                #              51, 62, 67, 69, 70, 72, 73, 74 , 80, 81, 82, 86, 100, 101, \n",
    "                                                #              102,  103, 105, 116, 113, 114, 116, 119, 120,122, 126, 129])\n",
    "cr = len(train_x) - 211 - 5 - 134- 717 + np.array([5, 6, 7, 9, 10, 17, 19, 20, 26, 28, 29, 32, 33, 34, 36,  38, 40, 41, 47, 49, 65, 67,\n",
    "                                        92, 97, 100, 128, 133, 136, 145])\n",
    "colorado = list(colorado) + list(s25) + list(cr)\n",
    "print(colorado)\n",
    "fneurope = [0, 77]#, 127, 155,162, 192, 261]\n",
    "fneurope = list(np.array(fneurope) + train_x.shape[0] - 2421 - 46 - 72 - 35 - 27 - 137 - 81 - 129 - 84 - 130 - 343- 211- 134 - 717 -5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = [29004, 29005, 29007, 29010, 29014, 29022, 29023, 29024, 29026, 29027, 29028, 29030, 29032, 29037, 29039, 29040, 29041, 29043, 29044, 29045, 29046, 29049, 29051, 29066, 29067, 29095, 29097, 29098, 29105, 29110, 29139, 29149, 29151, 29159, 29171, 29174, 29176, 29177, 29178, 29182, 29183, 29193, 29197, 29199, 29200, 29203, 29204, 29213, 29233, 29240, 29252, 29263, 29264, 29269, 29307, 29256, 29288, 29315, 28876, 28878, 28889, 28891, 28893, 28897, 28901, 28905, 28907, 28909, 28911, 28912, 28915, 28917, 28918, 28919, 28924, 28935, 28940, 28942, 28943, 28945, 28946, 28947, 28953, 28954, 28955, 28959, 28973, 28974, 28975, 28976, 28978, 28989, 28986, 28987, 28989, 28992, 28993, 28995, 28999, 29002, 29351, 29352, 29353, 29355, 29356, 29363, 29365, 29366, 29372, 29374, 29375, 29378, 29379, 29380, 29382, 29384, 29386, 29387, 29393, 29395, 29411, 29413, 29438, 29443, 29446, 29474, 29479, 29482, 29491]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5387, 12205, 12207, 12208, 12211, 12212, 12214, 12215, 12220, 12221, 12222, 12367]\n"
     ]
    }
   ],
   "source": [
    "bad_veg = [138444883,\n",
    " 139407017, 139413175, 139413227, 139413230,139413231, 139413235, 139413236, 139413238,\n",
    " 139413239, 139413245,139413247,139413248, 139413251, 139413557, 139413597,\n",
    " 139419949, 139419961,139420071,139420078, 139420356, 139420357,139420359,\n",
    " 139420360,139420373,139420378,139420384, 139420404,139420417, 139420422, 139420427,\n",
    " 139420428, 139420429, 139420434, 139420436, 139420438, 139420442, 139420445]\n",
    "\n",
    "bad_veg = data[data['plot_id'].isin(bad_veg)]\n",
    "bad_veg = list(bad_veg.index)\n",
    "print(bad_veg)\n",
    "bad_veg2 = [139064524, 139064525,\n",
    " 139207938, 139207939,139207942,139207943, 139207948, 139207956,\n",
    " 139208250, 139208251, 139208253, 139208254, 139208255,139208256,\n",
    " 139208257, 139208258, 139208259, 139208260,\n",
    " 139208261, 139208262,139208263,\n",
    " 139208264, 139208265,139208268,\n",
    " 139208269, 139208270,139208271]\n",
    "\n",
    "\n",
    "bad_veg2 = data[data['plot_id'].isin(bad_veg2)]\n",
    "bad_veg2 = list(bad_veg2.index)\n",
    "bad_veg = bad_veg + bad_veg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "equibatch_plot_ids = (pineapple) + (fns) + (plots_to_rm ) + (rotation) + (sisal) + (kenya) + (change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "to_remove_na = np.argwhere(np.sum(np.isnan(train_y), axis = (1, 2)) > 0).flatten()\n",
    "print(to_remove_na)\n",
    "train_x = np.delete(train_x, to_remove_na, 0)\n",
    "train_y = np.delete(train_y, to_remove_na, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter, maximum_filter, percentile_filter\n",
    "from scipy.ndimage.morphology import binary_dilation, generate_binary_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31005\n",
      "[10494, 2648, 1833, 1549, 1598, 1681, 2017, 2236, 6949]\n",
      "31005\n",
      "This batch uses 26630 samples\n",
      "64.21112212465306\n",
      "1001\n",
      "1007\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "train_ids = [x for x in range(0, len(train_y))]\n",
    "\n",
    "def multiplot(matrices):\n",
    "    '''Plot multiple heatmaps with subplots\n",
    "    \n",
    "         Parameters:\n",
    "          matrices (list of arrays):\n",
    "\n",
    "         Returns:\n",
    "          None\n",
    "    '''\n",
    "    fig, axs = plt.subplots(ncols=4)\n",
    "    fig.set_size_inches(20, 4)\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        sns.heatmap(data = matrix, ax = axs[i], vmin = 0, vmax = 0.9)\n",
    "        axs[i].set_xlabel(\"\")\n",
    "        axs[i].set_ylabel(\"\")\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([])\n",
    "\n",
    "percents = [9.0, 17.0, 27.0, 40.0, 63.0, 105.0, 158.0]\n",
    "def equibatch(train_ids, p = percents):\n",
    "    '''Docstring\n",
    "    \n",
    "         Parameters:\n",
    "          train_ids (list):\n",
    "          p (list):\n",
    "\n",
    "         Returns:\n",
    "          equibatches (list):\n",
    "    '''\n",
    "    \n",
    "    fn2 = [17223,17350,17371,17389,17402,17410,17412,17413,17423]\n",
    "    cr2 = list(np.arange(len(train_x) - 211 - 134 - 717, len(train_x) - 134 - 717 - 5, 1))\n",
    "    mhigh = list(np.arange(len(train_x) - 17, len(train_x) - 5, 1))\n",
    "    mhigh = mhigh + [0, 1,0,1,0,1,0,1]\n",
    "    #new = np\n",
    "    guat = [29693, 29713,29724, 29725, 29729] * 2\n",
    "    guatlow = list(np.arange(len(train_x) - 368, len(train_x) - 328, 1)) * 1\n",
    "    percents = [9.0, 17.0, 27.0, 40.0, 63.0, 105.0, 158.0]\n",
    "    brazil5 = [x for x in np.arange(len(train_y) - 78 - 35 - 27 - 137 - 81 - 129 - 84 - 130 - 343- 211 - 134 - 717,\n",
    "                                    len(train_y) - 61 - 35 - 27 - 137 - 81 - 129 - 84 - 130 - 343- 211 - 134 - 717)] * 2\n",
    "\n",
    "    #elephantgrass = train_ids[-(300+72):(-50)]\n",
    "    kenya2 = ([17345, 17388, 17390, 17398]) * 2\n",
    "    brazil = ([27983, 27992, 27995, 27997, 28015, 28026]) * 1\n",
    "    #elephantgrass = train_ids[-(300+72):(-300)]\n",
    "    train_ids_cp = (train_ids) +(sisal * 1) + kenya2 + (kenya * 2) +(change * 1) + (rotation * 1) + (fn2 * 1) + (change * 1)\n",
    "    train_ids_cp = train_ids\n",
    "    train_ids_cp = (train_ids_cp) + guat + guatlow + (cr2 * 1) + (mhigh * 2) + (malawi * 2)# + (ghana * 2) + (eurot * 2) + brazil + brazil5#+ (plots_to_rm * 1) \n",
    "    np.random.shuffle(train_ids_cp) \n",
    "    ix = train_ids_cp\n",
    "    print(len(ix))\n",
    "    percs = [np.sum(x) for x in train_y[ix]]\n",
    "    ids0 = [x for x, z in zip(ix, percs) if z <= 2]\n",
    "    #ids1 = [x for x, z in zip(ix, percs) if z <= 2]\n",
    "    #np.random.shuffle(ids1)\n",
    "    ids30 = [x for x, z in zip(ix, percs) if 2 < z <= percents[0]]\n",
    "    ids40 = [x for x, z in zip(ix, percs) if percents[0] < z <= percents[1]]\n",
    "    ids50 = [x for x, z in zip(ix, percs) if percents[1] < z <= percents[2]]\n",
    "    ids60 = [x for x, z in zip(ix, percs) if percents[2] < z <= percents[3]]\n",
    "    ids70 = [x for x, z in zip(ix, percs) if percents[3] < z <= percents[4]]\n",
    "    ids80 = [x for x, z in zip(ix, percs) if percents[4] < z <= percents[5]]\n",
    "    ids90 = [x for x, z in zip(ix, percs) if percents[5] < z <= percents[6]]\n",
    "    ids100 = [x for x, z in zip(ix, percs) if percents[6] < z]\n",
    "    \n",
    "    new_batches = []\n",
    "    maxes = [len(ids0), len(ids30), len(ids40), len(ids50), len(ids60), len(ids70),\n",
    "             len(ids80), len(ids90), len(ids100)]\n",
    "    print(maxes)\n",
    "    print(np.sum(maxes))\n",
    "    cur_ids = [0] * len(maxes)\n",
    "    iter_len = len(train_ids)//(len(maxes))\n",
    "    for i in range(0, iter_len):\n",
    "        for i, val in enumerate(cur_ids):\n",
    "            if val > maxes[i] - 1:\n",
    "                cur_ids[i] = 0\n",
    "        if cur_ids[0] >= (maxes[0] - 3):\n",
    "            cur_ids[0] = 0\n",
    "        #if cur_ids[8] >= (maxes[8] - 2):\n",
    "        #    cur_ids[8] = 0\n",
    "        to_append = [ids0[cur_ids[0]], ids0[cur_ids[0] + 1], ids0[cur_ids[0] + 2],\n",
    "                    ids30[cur_ids[1]], ids40[cur_ids[2]],\n",
    "                    ids50[cur_ids[3]], ids60[cur_ids[4]], \n",
    "                    ids70[cur_ids[5]], ids80[cur_ids[6]],\n",
    "                    ids90[cur_ids[7]], ids100[cur_ids[8]]]#,\n",
    "                    #ids100[cur_ids[8] + 1]]\n",
    "        \n",
    "        np.random.shuffle(to_append)\n",
    "        new_batches.append(to_append)\n",
    "        cur_ids = [x + 1 for x in cur_ids]\n",
    "        cur_ids[0] += 2\n",
    "        #cur_ids[8] += 1\n",
    "        \n",
    "        \n",
    "    new_batches = [item for sublist in new_batches for item in sublist]\n",
    "    print(f\"This batch uses {len(set(new_batches))} samples\")\n",
    "    print(np.mean(np.array(percs)))\n",
    "    MODIFIER = 824 + 343 + 211 + 134 +717\n",
    "    #17382, 17409,17423\n",
    "    #print(new_batches)\n",
    "    #print(np.sum(new_batches == 813))\n",
    "    #print(np.mean(np.array(percs)[new_batches]))\n",
    "    \n",
    "    set_0 = set(np.array(new_batches))\n",
    "    remove = [6792,7200,10805,12009,12037,12063,12158,12302, 17494, 17509, 17516, 17742, 17817,\n",
    "             17741, 17817, 17824, 17838, 17740, 17840, 27376, 27377, 27378, 27379, 27380, 27381, 27382,\n",
    "             ]\n",
    "    rmcosta = len(train_y) - 140 - 5 - 134 - 717+ np.array([6, 7, 9, 10, 17, 19, 20, 26, 28, 29, 32, 36,\n",
    "                                                38, 40, 47, 49, 65, 67, 92, 97, 100, 128, 133, 136])\n",
    "    \n",
    "    rmcolo = colorado + list(rmcosta)\n",
    "    remove = remove + list(np.arange(17810, 17821, 1))\n",
    "    remove = remove + [x for x in np.arange(len(train_x) - MODIFIER, len(train_x) - (MODIFIER - 125))]\n",
    "    #remove = remove + list(np.arange(len(train_y) - 84 , len(train_y) - 5, 1))\n",
    "  \n",
    "    remove = remove + list(addl) + list(rmcolo)\n",
    "    remove = remove + list(samples_w_loss)\n",
    "    \n",
    "    tmeans = np.mean(train_y[-(MODIFIER + 143):-MODIFIER], axis = (1, 2))\n",
    "    #tmeans[60:88] = 1. #argentina\n",
    "    remove3 = np.argwhere(tmeans < 0.68).flatten() \n",
    "    #print(remove3)\n",
    "    #remove4 = np.argwhere(tmeans > 0.95).flatten()\n",
    "    \n",
    "    remove3 = np.array(list(remove3)) #+ list(remove4))\n",
    "    remove3 = len(train_y) - (MODIFIER + 143) + remove3 #ghana, rwanda\n",
    "    ethsshrub = [x for x in np.arange(len(train_x) - 195 - 279 - 90, len(train_x) -  155 - 279 - 90)]\n",
    "    remove = remove + list(remove3) + ethsshrub\n",
    "    #remove = remove + remove2\n",
    "    #nans = np.argwhere(np.sum(np.isnan(train_x[..., -2:]), axis = (1, 2, 3, 4)) > 0).flatten()\n",
    "    #print(nans)\n",
    "    nans = []\n",
    "    remove = list(remove) + list(nans)# + list(urban_fp)# + list(evi_rm)\n",
    "    ca = [30039, 30042, 30044, 30045, 30046, 30051, 30053, 30055, 30056,\n",
    "    30057, 30058, 30059, 30060, 30067, 30071, 30072, 30073, 30076, 30077]\n",
    "    #print(ca)\n",
    "    remove = remove + ca\n",
    "    print(len(remove))\n",
    "    remove = remove + [x for x in np.arange(len(train_x) - 75, len(train_x) - 57, 3)] # ghana high\n",
    "    print(len(remove))\n",
    "    new_batches = [x for x in list(new_batches) if x not in np.array(remove)]\n",
    "    #new_batches = [x for x in list(new_batches) if x not in np.array(cr)]\n",
    "    new_batches = [x for x in new_batches if x not in pineapple]\n",
    "    #new_batches = [x for x in new_batches if x not in bad_veg]\n",
    "    print([x for x in new_batches if x in np.array(ethsshrub)])\n",
    "    return new_batches, remove\n",
    "\n",
    "batch, rm = equibatch(train_ids)\n",
    "# [4552, 1607, 1149, 959, 837, 834, 672, 747, 1945]\n",
    "# [4575, 1618, 1154, 971, 846, 784, 750, 711, 1895] # aprili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30400\n"
     ]
    }
   ],
   "source": [
    "rr = len(train_x) - 13\n",
    "print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30077, 30078]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca = list([x for x in np.arange(len(train_x) - 95, len(train_x) - 55)])\n",
    "list(30039 + np.argwhere(np.mean(train_y[ca], axis = (1, 2)) < 0.5).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = {'all': [0, 1150]}\n",
    "\n",
    "def dice_loss_tolerance(y_true, y_pred):\n",
    "    numerator_data = np.zeros_like(y_true)\n",
    "    for x in range(y_true.shape[0]):\n",
    "        for y in range(y_true.shape[1]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([y_true.shape[0], y+2])\n",
    "            max_x = np.min([y_true.shape[0], x+2])\n",
    "            if y_true[x, y] == 1:\n",
    "                numerator_data[x, y] = np.max(y_pred[min_x:max_x, min_y:max_y])\n",
    "                \n",
    "    numerator = 2 * np.sum(y_true * numerator_data, axis=-1)\n",
    "    denominator = np.sum(y_true + y_pred, axis=-1)\n",
    "    return (numerator + 1) / (denominator + 1)\n",
    "                    \n",
    "            \n",
    "def compute_f1_score_at_tolerance(true, pred, tolerance = 1):\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    tp = np.zeros_like(true)\n",
    "    fp = np.zeros_like(true)\n",
    "    fn = np.zeros_like(true)\n",
    "    \n",
    "    for x in range(true.shape[0]):\n",
    "        for y in range(true.shape[1]):\n",
    "            min_x = np.max([0, x-1])\n",
    "            min_y = np.max([0, y-1])\n",
    "            max_y = np.min([true.shape[0], y+2])\n",
    "            max_x = np.min([true.shape[0], x+2])\n",
    "            if true[x, y] == 1:\n",
    "                if np.sum(pred[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    tp[x, y] = 1\n",
    "                else:\n",
    "                    fn[x, y] = 1\n",
    "            if pred[x, y] == 1:\n",
    "                if np.sum(true[min_x:max_x, min_y:max_y]) > 0:\n",
    "                    if true[x, y] == 1:\n",
    "                        tp[x, y] = 1\n",
    "                else:\n",
    "                    fp[x, y] = 1                \n",
    "                \n",
    "    return np.sum(tp), np.sum(fp), np.sum(fn)\n",
    "\n",
    "def calculate_metrics(country, al = 0.4, canopy_thresh = 100):\n",
    "    '''Calculates the following metrics for an input country, based on\n",
    "       indexing of the country dictionary:\n",
    "       \n",
    "         - Loss\n",
    "         - F1\n",
    "         - Precision\n",
    "         - Recall\n",
    "         - Dice\n",
    "         - Mean surface distance\n",
    "         - Average error\n",
    "    \n",
    "         Parameters:\n",
    "          country (str):\n",
    "          al (float):\n",
    "          \n",
    "         Returns:\n",
    "          val_loss (float):\n",
    "          best_dice (float):\n",
    "          error (float):\n",
    "    '''\n",
    "    print(canopy_thresh)\n",
    "    start_idx = 0\n",
    "    stop_idx = 968 #len(test_x)\n",
    "    best_f1 = 0\n",
    "    best_dice = 0\n",
    "    best_thresh = 0\n",
    "    hausdorff = 0\n",
    "    relaxed_f1 = 0\n",
    "    preds = []\n",
    "    vls = []\n",
    "    trues = []\n",
    "    test_ids = [x for x in range(len(test_x))]\n",
    "    for test_sample in test_ids[start_idx:stop_idx]:\n",
    "        if np.sum(test_y[test_sample]) < ((canopy_thresh/100) * 197):\n",
    "            x_input = test_x[test_sample, ..., ].reshape(1, test_x.shape[1], 28, 28, n_bands)\n",
    "            #x_input = np.delete(x_input, [11, 12], axis = -1)\n",
    "            x_median_input = calc_median_input(x_input)\n",
    "            y, vl = sess.run([fm, test_loss], feed_dict={inp: x_input,\n",
    "                                                          #inp_median: x_median_input,\n",
    "                                                          length: np.full((1,), LEN),\n",
    "                                                          is_training: False,\n",
    "                                                          labels: test_y[test_sample].reshape(1, 14, 14),\n",
    "                                                          loss_weight: 1.0,\n",
    "                                                          alpha: 0.33,\n",
    "                                                          })\n",
    "            preds.append(y.reshape((14, 14)))\n",
    "            vls.append(vl)\n",
    "            trues.append(test_y[test_sample].reshape((14, 14)))\n",
    "    dice_losses = []\n",
    "    for thresh in range(7, 9):\n",
    "        tps_relaxed = np.empty((len(preds), ))\n",
    "        fps_relaxed = np.empty((len(preds), ))\n",
    "        fns_relaxed = np.empty((len(preds), ))\n",
    "        abs_error = np.empty((len(preds), ))\n",
    "        \n",
    "        for sample in range(len(preds)):\n",
    "            pred = np.copy(preds[sample])\n",
    "            true = trues[sample]\n",
    "            if thresh == 8:\n",
    "                if np.sum(true + pred) > 0:\n",
    "                    dice_losses.append(0.5)\n",
    "                   # dice_losses.append(dice_loss_tolerance(np.array(true), np.array(pred)))\n",
    "                else:\n",
    "                    dice_losses.append(1.)\n",
    "            pred[np.where(pred >= thresh*0.05)] = 1\n",
    "            pred[np.where(pred < thresh*0.05)] = 0\n",
    "            \n",
    "            true_s = np.sum(true[1:-1])\n",
    "            pred_s = np.sum(pred[1:-1])\n",
    "            abs_error[sample] = abs(true_s - pred_s)\n",
    "            tp_relaxed, fp_relaxed, fn_relaxed = compute_f1_score_at_tolerance(true, pred)\n",
    "            tps_relaxed[sample] = tp_relaxed\n",
    "            fps_relaxed[sample] = fp_relaxed\n",
    "            fns_relaxed[sample] = fn_relaxed                   \n",
    "            \n",
    "        oa_error = np.mean(abs_error)\n",
    "        precision_r = np.sum(tps_relaxed) / (np.sum(tps_relaxed) + np.sum(fps_relaxed))\n",
    "        recall_r = np.sum(tps_relaxed) / (np.sum(tps_relaxed) + np.sum(fns_relaxed))\n",
    "        f1_r = 2*((precision_r* recall_r) / (precision_r + recall_r))\n",
    "        print(f1_r)\n",
    "        if f1_r > best_f1:\n",
    "            haus = np.zeros((len(preds), ))\n",
    "            for sample in range(len(preds)):\n",
    "                pred = np.copy(preds[sample])\n",
    "                pred[np.where(pred >= thresh*0.05)] = 1\n",
    "                pred[np.where(pred < thresh*0.05)] = 0\n",
    "                true = trues[sample]\n",
    "\n",
    "            dices = np.mean(dice_losses)\n",
    "            haus = np.mean(haus)\n",
    "            best_dice = 0.5\n",
    "            best_f1 = f1_r\n",
    "            p = precision_r\n",
    "            r = recall_r\n",
    "            error = oa_error\n",
    "            best_thresh = thresh*0.05\n",
    "            best_haus = 0.5\n",
    "            print(f\"{country}: Val loss: {np.around(np.mean(vls), 3)}\"\n",
    "                  f\" Thresh: {np.around(best_thresh, 2)}\"\n",
    "                  f\" F1: {np.around(best_f1, 3)} R: {np.around(p, 3)} P: {np.around(r, 3)}\"\n",
    "                  f\" D: {np.around(np.mean(best_dice), 3)} H: {np.around(best_haus, 3)}\"\n",
    "                  f\" Error: {np.around(error, 3)}\")\n",
    "            \n",
    "    return np.mean(vls), best_f1, error, best_haus, np.mean(best_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fftIndgen(n):\n",
    "    a = range(0, n//2+1)\n",
    "    b = range(1, n//2)\n",
    "    b = [x for x in b]\n",
    "    b.reverse()\n",
    "    a = [x for x in a]\n",
    "   \n",
    "    b = [-i for i in b]\n",
    "    return a + b\n",
    "\n",
    "def gaussian_random_field(Pk = lambda k : k**-3.0, size = 100):\n",
    "    def Pk2(kx, ky):\n",
    "        if kx == 0 and ky == 0:\n",
    "            return 0.0\n",
    "        return np.sqrt(Pk(np.sqrt(kx**2 + ky**2)))\n",
    "    noise = np.fft.fft2(np.random.normal(size = (size, size)))\n",
    "    amplitude = np.zeros((size,size))\n",
    "    for i, kx in enumerate(fftIndgen(size)):\n",
    "        for j, ky in enumerate(fftIndgen(size)):            \n",
    "            amplitude[i, j] = Pk2(kx, ky)\n",
    "    return np.fft.ifft2(noise * amplitude).astype(np.float32)\n",
    "\n",
    "def make_aug_masks(n, perc):\n",
    "    masks = np.zeros((n, 28, 28))\n",
    "    for i in range(n):\n",
    "        gas = gaussian_random_field(Pk = lambda k: k**-5, size=28)\n",
    "        #percentile = np.clip(np.random.normal(perc, 0.1, 1), 0, 1)\n",
    "        #percentile = np.percentile(gas, percentile * 100)\n",
    "        #gas = gas <= percentile\n",
    "        masks[i] = gas\n",
    "    return masks\n",
    "\n",
    "\n",
    "def make_aug_masks(n, perc):\n",
    "    masks = np.zeros((n, 28, 28))\n",
    "    for i in range(n):\n",
    "        gas = gaussian_random_field(Pk = lambda k: k**-5, size=28)\n",
    "        #percentile = np.clip(np.random.normal(perc, 0.1, 1), 0, 1)\n",
    "        #percentile = np.percentile(gas, percentile * 100)\n",
    "        #gas = gas <= percentile\n",
    "        masks[i] = gas\n",
    "    return masks\n",
    "\n",
    "# Realistically rather than sampling from the entire dataset,\n",
    "# We would want to match up each pair of samples such that they share the same\n",
    "# principal components (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_all[10] = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_batch(batch_ids, batch_size):\n",
    "    '''Performs random flips and rotations of the X and Y\n",
    "       data for a total of 4 x augmentation\n",
    "    \n",
    "         Parameters:\n",
    "          batch_ids (list):\n",
    "          batch_size (int):\n",
    "          \n",
    "         Returns:\n",
    "          x_batch (arr):\n",
    "          y_batch (arr):\n",
    "    '''\n",
    "    \n",
    "    def _unapply(x, idx):\n",
    "        _max = max_all[idx]\n",
    "        _min =  min_all[idx]\n",
    "        midrange = (_max + _min) / 2\n",
    "        rng = max_all[idx]\n",
    "        return x * (rng / 2) + midrange\n",
    "    \n",
    "    def _apply(x, idx):\n",
    "        _max = max_all[idx]\n",
    "        _min =  min_all[idx]\n",
    "        midrange = (_max + _min) / 2\n",
    "        rng = max_all[idx]\n",
    "        return (x - midrange) / (rng / 2)\n",
    "    \n",
    "    x = np.copy(train_x[batch_ids])\n",
    "    samples_to_median = np.random.randint(0, 12, size=(batch_size, 12)) #[32, 6]\n",
    "    samples_to_select = np.zeros((batch_size, 4))\n",
    "    flast = np.array([0, 1, 2, 3, 8, 9, 10, 11])\n",
    "    #samples_to_select[:, 0] = np.random.choice(flast, size=(batch_size))\n",
    "    samples_to_select[:, 0] = np.random.randint(0, 4, size=(batch_size))\n",
    "    samples_to_select[:, 1] = np.random.randint(3, 7, size=(batch_size))\n",
    "    samples_to_select[:, 2] = np.random.randint(6, 10, size=(batch_size))\n",
    "    samples_to_select[:, 3] = np.random.randint(9, 12, size=(batch_size))\n",
    "    samples_to_select = samples_to_select.astype(np.int)\n",
    "    n_samples = np.random.randint(2, 5, size=(batch_size)) \n",
    "    \n",
    "    x_batch = np.zeros((x.shape[0], LEN + 1, 28, 28, n_bands))\n",
    "    for samp in range(batch_size):\n",
    "        samps = samples_to_median[samp, :]#:np.random.randint(6, 12)]\n",
    "        x_samp = train_x[samp]\n",
    "        samps = np.unique(samps)\n",
    "        med_samp = np.median(x_samp[samps], axis = 0)\n",
    "\n",
    "        x_batch[samp, :-1, ...] = x[samp, samples_to_select[samp]]\n",
    "        x_batch[samp, -1, ...] = med_samp\n",
    "        \n",
    "    y = train_y[batch_ids]\n",
    "    \n",
    "    y_batch = np.zeros_like(y)\n",
    "    \n",
    "    flips = np.random.choice(np.array([0, 1, 2, 3]), batch_size, replace = True)\n",
    "    for i in range(x_batch.shape[0]):\n",
    "        current_flip = flips[i]\n",
    "        if current_flip == 0:\n",
    "            x_batch[i] = x_batch[i]\n",
    "            y_batch[i] = y[i]\n",
    "        if current_flip == 1:\n",
    "            x_batch[i] = np.flip(x_batch[i], 1)\n",
    "            y_batch[i] = np.flip(y[i], 0)\n",
    "        if current_flip == 2:\n",
    "            x_batch[i] = np.flip(x_batch[i], [2, 1])\n",
    "            y_batch[i] = np.flip(y[i], [1, 0])\n",
    "        if current_flip == 3:\n",
    "            x_batch[i] = np.flip(x_batch[i], 2)\n",
    "            y_batch[i] = np.flip(y[i], 1)\n",
    "    \n",
    "    for b in range(10, 11):\n",
    "        slope = _unapply(x_batch[..., b], b)\n",
    "        mults = np.clip(np.random.normal(1, 0.06, size = (batch_size, 1, 1, 1,)), 0.5, 2)\n",
    "        slope = slope * mults\n",
    "        slope = _apply(slope, b)\n",
    "        x_batch[..., b] = slope\n",
    " \n",
    "    \n",
    "    y_batch = y_batch.reshape((batch_size, 14, 14))\n",
    "    return x_batch, y_batch\n",
    "\n",
    "x_batch_test, y_batch_test = augment_batch([x for x in range(32)], 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_evi = train_x[:, -1, 7:-7, 7:-7, 13] < -0.35\n",
    "med_evi = np.sum(med_evi * (train_y > 0.15), axis = (1, 2))\n",
    "evi_rm = np.argwhere(med_evi > 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_median_input(x_batch):\n",
    "    x_median = np.percentile(x_batch, 25, axis = (1))\n",
    "    return x_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting anew\n"
     ]
    }
   ],
   "source": [
    "#model_path  = \"models/rmapper/2023-fast/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")\n",
    "else:\n",
    "    print(\"Starting anew\")\n",
    "    metrics = np.zeros((6, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_train = test_x[..., 11:13]\n",
    "s1_train = np.reshape(s1_train, (s1_train.shape[0], s1_train.shape[1], s1_train.shape[2] // 4, 4,\n",
    "                                 s1_train.shape[3] // 4, 4, 2))\n",
    "\n",
    "s1_train = np.mean(s1_train, axis = (3, 5))\n",
    "s1_train = s1_train.repeat(4, axis = 2).repeat(4, axis = 3)\n",
    "\n",
    "#s1_train[:, :3] = np.mean(s1_train[:, :3], axis = 1)[:, np.newaxis, ...]\n",
    "#s1_train[:, 3:6] = np.mean(s1_train[:, 3:6], axis = 1)[:, np.newaxis, ...]\n",
    "#s1_train[:, 6:9] = np.mean(s1_train[:, 6:9], axis = 1)[:, np.newaxis, ...]\n",
    "#s1_train[:, 9:12] = np.mean(s1_train[:, 9:12], axis = 1)[:, np.newaxis, ...]\n",
    "test_x[..., 11:13] = s1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_train = train_x[..., 11:13]\n",
    "s1_train = np.reshape(s1_train, (s1_train.shape[0], s1_train.shape[1], s1_train.shape[2] // 4, 4,\n",
    "                                 s1_train.shape[3] // 4, 4, 2))\n",
    "\n",
    "s1_train = np.mean(s1_train, axis = (3, 5))\n",
    "s1_train = s1_train.repeat(4, axis = 2).repeat(4, axis = 3)\n",
    "\n",
    "#s1_train[:, :3] = np.mean(s1_train[:, :3], axis = 1)[:, np.newaxis, ...]\n",
    "#s1_train[:, 3:6] = np.mean(s1_train[:, 3:6], axis = 1)[:, np.newaxis, ...]\n",
    "#s1_train[:, 6:9] = np.mean(s1_train[:, 6:9], axis = 1)[:, np.newaxis, ...]\n",
    "#s1_train[:, 9:12] = np.mean(s1_train[:, 9:12], axis = 1)[:, np.newaxis, ...]\n",
    "train_x[..., 11:13] = s1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tensors_in_checkpoint_file(file_name,all_tensors=True,tensor_name=None):\n",
    "    varlist=[]\n",
    "    var_value =[]\n",
    "    reader = tf.compat.v1.train.NewCheckpointReader(file_name)\n",
    "    if all_tensors:\n",
    "        var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "        for key in sorted(var_to_shape_map):\n",
    "            varlist.append(key)\n",
    "            var_value.append(reader.get_tensor(key))\n",
    "    else:\n",
    "        varlist.append(tensor_name)\n",
    "        var_value.append(reader.get_tensor(tensor_name))\n",
    "    return (varlist, var_value)\n",
    "\n",
    "def build_tensors_in_checkpoint_file(loaded_tensors):\n",
    "    full_var_list = list()\n",
    "    # Loop all loaded tensors\n",
    "    for i, tensor_name in enumerate(loaded_tensors[0]):\n",
    "        # Extract tensor\n",
    "        try:\n",
    "            tensor_aux = tf.get_default_graph().get_tensor_by_name(tensor_name+\":0\")\n",
    "            full_var_list.append(tensor_aux)\n",
    "        except:\n",
    "            print('Not found: '+tensor_name)\n",
    "        \n",
    "    return full_var_list\n",
    "\n",
    "checkpoint_dir = \"models/rmapper/2023-m-ws-wd-all2/\"\n",
    "all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "all_vars = [x for x in all_vars if 'Momentum' not in x.name]\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep = 150, var_list = all_vars)\n",
    "#restored_vars[0]\n",
    "#tensors_to_load = build_tensors_in_checkpoint_file(restored_vars)\n",
    "#saver.restore(sess, CHECKPOINT_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models/rmapper/2023-m-ws-wd-master-RETRAIN-oct30-2/metrics.npy\n",
      "Loading models/rmapper/2023-m-ws-wd-nov6/metrics.npy\n",
      "INFO:tensorflow:Restoring parameters from models/rmapper/2023-m-ws-wd-nov6/RESWA23-2-62-7/model\n"
     ]
    }
   ],
   "source": [
    "#saver = tf.train.Saver(max_to_keep = 1000)\n",
    "\n",
    "#/\" #\"ftmay7-93-90-6/\"#\"78-90-4/\"\n",
    "\n",
    "#print(path)\n",
    "\n",
    "#saver.restore(sess, tf.train.latest_checkpoint(path))\n",
    "\n",
    "model_path = 'models/rmapper/2023-m-ws-wd-master-RETRAIN-oct30-2/'\n",
    "path = model_path + \"RESWA23-29-89-1/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")#path = model_path + \"RESWA21-53-89-6/\"\n",
    "\n",
    "#/\" #\"ftmay7-93-90-6/\"#\"78-90-4/\"\n",
    "#saver.restore(sess, tf.train.latest_checkpoint(path))\n",
    "model_path = 'models/rmapper/2023-m-ws-wd-nov6/'\n",
    "path = model_path + \"RESWA23-2-62-7/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")\n",
    "saver.restore(sess, tf.train.latest_checkpoint(path))\n",
    "\n",
    "model_path = 'models/rmapper/dec9/'\n",
    "path = model_path + \"RESWA25-16-88-1/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if os.path.isfile(f\"{model_path}metrics.npy\"):\n",
    "    metrics = np.load(f\"{model_path}metrics.npy\")\n",
    "    print(f\"Loading {model_path}metrics.npy\")\n",
    "#saver.restore(sess, tf.train.latest_checkpoint(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80612245, 0.95918367, 0.83673469, 0.93877551, 0.89285714,\n",
       "       0.63265306, 0.87755102, 0.83163265, 0.69897959, 0.65306122,\n",
       "       0.64795918, 0.65816327, 0.92857143, 0.70408163, 0.87755102,\n",
       "       0.62244898, 0.59183673, 0.84693878, 0.76020408, 0.54081633,\n",
       "       0.92346939, 0.55612245, 0.58673469, 0.67346939, 0.43367347,\n",
       "       0.93877551, 0.94387755, 0.44387755, 0.64285714, 0.54081633,\n",
       "       0.92346939, 0.53571429, 0.92346939, 0.70918367, 0.54081633,\n",
       "       0.35714286, 0.04081633, 0.35204082, 0.16326531, 0.40816327,\n",
       "       0.28061224, 0.17857143, 0.40816327, 0.37755102, 0.12755102,\n",
       "       0.33673469, 0.13265306, 0.34183673, 0.39285714, 0.20918367,\n",
       "       0.48979592, 0.03061224, 0.34183673, 0.1122449 , 0.33163265,\n",
       "       0.1122449 , 0.10714286, 0.08163265, 0.08673469, 0.12755102,\n",
       "       0.05612245, 0.01530612])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_y[-642:-580], axis = (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.concatenate([test_x[635, :-1].repeat(3, axis = 0), test_x[635, -1][np.newaxis]], axis = 0)\n",
    "train_x[1] = l\n",
    "train_y[1] = test_y[635]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.concatenate([test_x[886, :-1].repeat(3, axis = 0), test_x[886, -1][np.newaxis]], axis = 0)\n",
    "train_x[0] = l\n",
    "train_y[0] = test_y[886]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-78.9995472  -80.26402433 -73.74412635 -68.81523812 -73.42237436]\n"
     ]
    }
   ],
   "source": [
    "train_ids = [x for x in range(len(train_y)) ]\n",
    "kenya2 = ([x for x in np.arange(len(train_x) - 78, len(train_x) - 5)])\n",
    "kenya2 = ([x for x in np.arange(len(train_x) - 642, len(train_x) - 536)]) # nz\n",
    "kenya2 = ([x for x in np.arange(len(train_x) - 536, len(train_x) - 344)]) # br1-3\n",
    "kenya2 = ([x for x in np.arange(len(train_x) -284, len(train_x) - 256)]) # argentina\n",
    "kenya2 = ([x for x in np.arange(len(train_x) -203, len(train_x) - 78)]) # brazil3\n",
    "kenya2 = ([x for x in np.arange(len(train_x) -194, len(train_x) - 154)]) # brazil3\n",
    "kenya2 = [29693, 29713, 29724, 29725, 29729]#([x for x in np.arange(len(train_x) -15, len(train_x)-5)]) # brazil3\n",
    "# 642 - 536 = nz\n",
    "#kenya2 = [27983, 27992, 27995, 27997, 28015, 28026]\n",
    "x_batch_test2, y_batch_test2 = augment_batch(kenya2, len(kenya2))  \n",
    "def evaluate_on_plots(batch):\n",
    "    preds = []\n",
    "    trues = np.sum(y_batch_test2, axis = (1, 2))\n",
    "    \n",
    "    for i in range(batch.shape[0]):\n",
    "        idx = i\n",
    "        x_input = batch[idx].reshape(1, 5, 28, 28, n_bands)\n",
    "\n",
    "        y = sess.run([fm], feed_dict={inp: x_input,\n",
    "                                      length: np.full((1,), 4),\n",
    "                                      is_training: False,\n",
    "                                        })\n",
    "        y = np.array(y).reshape(14, 14)   \n",
    "        preds.append(y)\n",
    "    preds = np.array(preds)\n",
    "    ms = np.mean(preds - y_batch_test2, axis = (1, 2)) * 100\n",
    "    #print(list(len(train_y) - 60 + np.argwhere(ms < -10).flatten()))\n",
    "    print(ms)\n",
    "    return preds#np.mean(preds)\n",
    "np.set_printoptions(suppress=True)\n",
    "p = evaluate_on_plots(x_batch_test2)\n",
    "##print(np.argwhere(np.mean((y_batch_test2 - p) > 0.5, axis = (1,2)) > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7efd41e5bdd0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(train_y[-88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(354+279, 368+279):\n",
    "        train_y[-i] = (1 - binary_dilation(1 - train_y[-i], iterations = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = len(train_y) - 205\n",
    "for i in range(79, 79+200):\n",
    "        train_y[-i] = (1 - binary_dilation(1 - train_y[-i], iterations = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y[train_y > 0.92] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y[-200:][train_y[-200:] > 0.75] = 1.\n",
    "train_y[-(107+279):-(97+279)][train_y[-(107+279):-(97+279)] < 0.7] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_loss, f1, error, haus, dice = calculate_metrics('all', al = 0.33, canopy_thresh = 75)\n",
    "#save_path = saver.save(sess, f\"{model_path}/24-{str(f1*100)[:2]}-{str(f1*100)[3]}/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26649 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26636 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "1000\n",
      "starting epoch 2, alpha: 0.01, beta: 0.0 drop: 1.0 Learning rate: 0.07987971764671169\n",
      "[-11.67968633 -30.7392227  -12.80473685  -7.75991551 -11.93753498]\n",
      "[-13.36027156 -31.92040952 -15.81093648  -9.00783387 -16.21025989]\n",
      "[-10.34247714 -27.57445194 -11.69524696  -7.02937017 -11.97110244]\n",
      "[-11.16131082 -30.82540052 -12.28578313  -7.3903491  -12.93198069]\n",
      "[-12.97554596 -33.1602241  -13.32662623  -7.55594047 -15.14500421]\n",
      "[-15.27144784 -38.15233561 -14.81900355  -7.90905697 -16.49813798]\n",
      "75\n",
      "0.6172211883255413\n",
      "all: Val loss: 0.17900000512599945 Thresh: 0.35 F1: 0.617 R: 0.846 P: 0.486 D: 0.5 H: 0.5 Error: 10.574\n",
      "0.592929149510152\n",
      "[ -9.83466193 -26.7012589   -9.56626845  -5.63706518  -7.76406159]\n",
      "[-14.22748158 -35.77778925 -13.76410954  -7.89708206 -15.69424007]\n",
      "[-15.63569329 -31.71871955 -14.37033652  -9.57131486 -14.39545641]\n",
      "[-10.47726888 -23.82127731  -8.86523432  -6.53004838  -7.76991601]\n",
      "[-15.4083966  -34.72381254 -11.3623582   -8.23907302 -12.44822315]\n",
      "Epoch 2: Loss 0.38600000739097595\n",
      "75\n",
      "0.6368390981409046\n",
      "all: Val loss: 0.18299999833106995 Thresh: 0.35 F1: 0.637 R: 0.827 P: 0.518 D: 0.5 H: 0.5 Error: 10.62\n",
      "0.6083643035235452\n",
      "Saving model with 0.6368390981409046\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26644 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "2000\n",
      "starting epoch 3, alpha: 0.02, beta: 0.0 drop: 1.0 Learning rate: 0.07972953430967772\n",
      "[-15.88200912 -33.39379285 -11.29430879  -7.89030799 -11.7988771 ]\n",
      "[-20.26812011 -36.69478696 -15.77452278 -10.73375238 -18.60129414]\n",
      "[-13.91898999 -31.87748238 -11.1547638   -8.13993003 -13.71516178]\n",
      "[-18.16742001 -31.75205132 -13.72847365  -9.41166114 -16.35330341]\n",
      "[-18.6164145  -41.5512048  -13.44224008 -11.07221501 -19.51209906]\n",
      "[-29.59755762 -44.21912566 -25.49222364 -21.80640555 -35.83525675]\n",
      "75\n",
      "0.6438388292133778\n",
      "all: Val loss: 0.17399999499320984 Thresh: 0.35 F1: 0.644 R: 0.937 P: 0.49 D: 0.5 H: 0.5 Error: 10.054\n",
      "0.6039763567974207\n",
      "[-10.10308421 -23.9521293   -8.06981508  -6.0758712   -9.33401773]\n",
      "[-21.44514648 -37.98898212 -16.45198103 -10.79424644 -19.83444107]\n",
      "[-20.0019164  -41.07370761 -17.12485591 -13.87462999 -26.71140676]\n",
      "[-13.32015879 -32.53209522 -10.30557524  -9.72056258 -15.87557187]\n",
      "[-14.23444535 -35.43562035 -10.46325765  -9.31069276 -14.49430506]\n",
      "Epoch 3: Loss 0.35899999737739563\n",
      "75\n",
      "0.6844616761109454\n",
      "all: Val loss: 0.17599999904632568 Thresh: 0.35 F1: 0.684 R: 0.716 P: 0.656 D: 0.5 H: 0.5 Error: 10.608\n",
      "0.6811720107407992\n",
      "Saving model with 0.6844616761109454\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26643 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "3000\n",
      "starting epoch 4, alpha: 0.03, beta: 0.0 drop: 0.994 Learning rate: 0.07951959397907236\n",
      "[-11.38948579 -30.16882663  -8.36766748  -7.37480241 -12.61170786]\n",
      "[-23.80461939 -39.14627688 -18.70383663 -13.65807403 -22.6230479 ]\n",
      "[-24.77018708 -44.25447667 -19.42813174 -12.30383719 -23.42580952]\n",
      "[-17.35497014 -25.21748613 -15.96480526  -9.69098657 -16.53764853]\n",
      "[-27.21148942 -44.05798386 -23.86384828 -19.01967656 -32.21096819]\n",
      "[-17.39894213 -36.61627671 -12.50770743  -8.76512585 -18.20932009]\n",
      "75\n",
      "0.7238939561844685\n",
      "all: Val loss: 0.16300000250339508 Thresh: 0.35 F1: 0.724 R: 0.897 P: 0.607 D: 0.5 H: 0.5 Error: 8.611\n",
      "0.7022024471635151\n",
      "[-14.25846462 -36.93362344 -11.38845621 -10.35755307 -20.77399392]\n",
      "[-23.21237183 -34.53875294 -19.56753694 -12.30873837 -21.06503286]\n",
      "[-16.40622178 -20.34364029 -15.78017075 -10.52391848 -18.64640938]\n",
      "[-60.51736491 -50.06355833 -58.93023195 -31.56351643 -54.49010195]\n",
      "[-23.06452375 -32.52832477 -20.95368146 -15.95639407 -30.32358044]\n",
      "Epoch 4: Loss 0.34599998593330383\n",
      "75\n",
      "0.7121119464728947\n",
      "all: Val loss: 0.17299999296665192 Thresh: 0.35 F1: 0.712 R: 0.717 P: 0.707 D: 0.5 H: 0.5 Error: 10.563\n",
      "0.7083857682187392\n",
      "Saving model with 0.7121119464728947\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26636 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "4000\n",
      "starting epoch 5, alpha: 0.04, beta: 0.0 drop: 0.98 Learning rate: 0.07925021242509539\n",
      "[-14.23864106 -28.00331937 -12.43496233  -8.50346122 -17.07991045]\n",
      "[ -8.24665722 -21.2212346   -7.04862141  -6.52105386 -10.33757189]\n",
      "[ -9.65847756 -22.30618113  -8.27848899  -7.05504192 -11.20856395]\n",
      "[ -9.22138974 -29.36531307  -7.96872095  -8.91932486 -15.30162774]\n",
      "[-21.89205487 -50.93905401 -18.80307295 -17.40299284 -32.1087706 ]\n",
      "[-28.87187623 -43.78957393 -22.27721792 -12.76717025 -28.02864307]\n",
      "75\n",
      "0.7241241861907083\n",
      "all: Val loss: 0.1589999943971634 Thresh: 0.35 F1: 0.724 R: 0.952 P: 0.584 D: 0.5 H: 0.5 Error: 8.726\n",
      "0.6871970081721224\n",
      "[-13.27311932 -35.24938823 -11.7921485  -11.71234241 -21.46074717]\n",
      "[-23.89190468 -34.60277188 -19.61456333 -12.06597461 -23.7569996 ]\n",
      "[ -5.20512364 -14.65791008  -4.160508    -4.27535496  -6.06166252]\n",
      "[-10.73586366 -26.80063739  -8.35733526  -7.94121453 -14.76061305]\n",
      "[-18.34555174 -37.82897567 -15.4995248  -12.80262726 -24.09691344]\n",
      "Epoch 5: Loss 0.33799999952316284\n",
      "75\n",
      "0.7413829562275323\n",
      "all: Val loss: 0.15600000321865082 Thresh: 0.35 F1: 0.741 R: 0.908 P: 0.626 D: 0.5 H: 0.5 Error: 8.475\n",
      "0.7179623007990111\n",
      "Saving model with 0.7413829562275323\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26644 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "5000\n",
      "starting epoch 6, alpha: 0.05, beta: 0.0 drop: 0.9660000000000001 Learning rate: 0.07892179482319295\n",
      "[-20.75594399 -39.52236585 -16.62174761 -10.81878464 -26.69754979]\n",
      "[-17.16334528 -34.64175826 -13.8121348  -10.87405967 -18.2863261 ]\n",
      "[-15.68858405 -33.96620889 -13.89713531 -10.50324896 -21.84691338]\n",
      "[-17.64831966 -33.62086911 -15.87025177 -12.74415623 -29.27394722]\n",
      "[ -9.06425006 -19.74395965  -7.6984955   -7.46801289 -11.27854692]\n",
      "[ -8.59443281 -16.28798021  -7.55389339  -7.0274156   -9.99298497]\n",
      "75\n",
      "0.7410311847264467\n",
      "all: Val loss: 0.16300000250339508 Thresh: 0.35 F1: 0.741 R: 0.761 P: 0.722 D: 0.5 H: 0.5 Error: 9.533\n",
      "0.7369193342858246\n",
      "[ -7.13171463 -14.34626923  -6.3834492   -6.26028463  -8.79437333]\n",
      "[ -8.47911969 -22.89914848  -7.7413534   -7.6529776  -13.52412223]\n",
      "[-10.68489463 -27.83656587  -9.32809416  -8.71334313 -16.90656023]\n",
      "[-12.78508987 -35.69088551 -12.39983442 -12.65027794 -24.12731384]\n",
      "[-23.84116957 -42.69119053 -20.85458049 -16.57042868 -34.8459274 ]\n",
      "Epoch 6: Loss 0.32899999618530273\n",
      "75\n",
      "0.7513607663836273\n",
      "all: Val loss: 0.15800000727176666 Thresh: 0.35 F1: 0.751 R: 0.763 P: 0.74 D: 0.5 H: 0.5 Error: 9.063\n",
      "0.7474441499432034\n",
      "Saving model with 0.7513607663836273\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26633 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "6000\n",
      "starting epoch 7, alpha: 0.06, beta: 0.0 drop: 0.9520000000000001 Learning rate: 0.07853483514463522\n",
      "[ -6.55753758 -22.36974089  -5.92602275  -6.05458091  -8.26721085]\n",
      "[ -8.24795271 -20.25262254  -7.40911617  -7.20366516 -10.05741939]\n",
      "[ -8.09407079 -31.21366883  -8.06768029  -9.62339989 -14.80355871]\n",
      "[-21.28780277 -41.28764176 -19.75168467 -12.9141376  -29.44918252]\n",
      "[-10.5943435  -25.17069932 -10.2310576  -10.77525917 -13.47063342]\n",
      "[-11.46289506 -20.1312897  -10.48861049  -9.56577862 -12.35272297]\n",
      "75\n",
      "0.7441769731965199\n",
      "all: Val loss: 0.15199999511241913 Thresh: 0.35 F1: 0.744 R: 0.82 P: 0.681 D: 0.5 H: 0.5 Error: 9.128\n",
      "0.7254448903482105\n",
      "[-34.07604199 -34.40293311 -28.53349505 -19.04988359 -28.26351207]\n",
      "[ -7.98662293 -24.43570128  -7.9474108   -7.90656103 -13.75386688]\n",
      "[-12.3877709  -37.73357848 -10.90953712 -10.23302556 -19.33398221]\n",
      "[-64.64552674 -49.60007719 -60.14355409 -29.60057728 -39.40906085]\n",
      "[ -9.84327775 -19.10389349  -9.49496235  -9.47331734 -10.51897513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss 0.3370000123977661\n",
      "75\n",
      "0.8127519508857576\n",
      "all: Val loss: 0.15600000321865082 Thresh: 0.35 F1: 0.813 R: 0.842 P: 0.785 D: 0.5 H: 0.5 Error: 7.103\n",
      "0.8106502025853753\n",
      "Saving model with 0.8127519508857576\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26641 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "7000\n",
      "starting epoch 8, alpha: 0.07, beta: 0.0 drop: 0.9380000000000001 Learning rate: 0.07808991541353662\n",
      "[-61.5873659  -58.26557755 -59.35715067 -41.6925631  -78.12182966]\n",
      "[ -7.7349952  -21.62233698  -7.44953575  -8.23866503 -10.86470229]\n",
      "[-16.25797073 -29.3256281  -13.82798291 -10.0764034  -15.02600242]\n",
      "[ -9.50394291 -24.37643974  -9.21726187  -9.54907981 -10.94775577]\n",
      "[ -6.96916477 -16.40461638  -6.40459857  -5.9092955   -7.76473408]\n",
      "[ -9.86343352 -25.04519565  -9.57620186 -10.29243156 -16.60664632]\n",
      "75\n",
      "0.8148398991609285\n",
      "all: Val loss: 0.14300000667572021 Thresh: 0.35 F1: 0.815 R: 0.861 P: 0.774 D: 0.5 H: 0.5 Error: 7.292\n",
      "0.8103055533736548\n",
      "[-11.58206803 -31.50800038 -10.48455479  -9.67937258 -20.4742698 ]\n",
      "[ -8.63465375 -26.37008517  -8.3541205   -9.53819694 -11.68841993]\n",
      "[-14.38953737 -34.60448809 -12.34863282 -11.80321939 -18.80968873]\n",
      "[-74.4618064  -55.07206903 -67.85289084 -30.53358892 -71.37543007]\n",
      "[-13.82128712 -39.77372283 -12.64443474 -11.6350215  -26.14117429]\n",
      "Epoch 8: Loss 0.3149999976158142\n",
      "75\n",
      "0.831787235736752\n",
      "all: Val loss: 0.13600000739097595 Thresh: 0.35 F1: 0.832 R: 0.939 P: 0.746 D: 0.5 H: 0.5 Error: 6.282\n",
      "0.8142152834839769\n",
      "Saving model with 0.831787235736752\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26642 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "8000\n",
      "starting epoch 9, alpha: 0.08, beta: 0.0 drop: 0.924 Learning rate: 0.07758770483143634\n",
      "[-25.49297034 -37.9016032  -22.58038795 -13.65959544 -43.42450089]\n",
      "[-13.14603592 -34.89652208 -11.78115582 -10.01876611 -15.49518875]\n",
      "[ -8.42120295 -27.04350287  -7.98674691  -8.12198559 -10.23199212]\n",
      "[ -9.77207021 -28.44714504  -9.23736737  -9.08669689 -13.40775712]\n",
      "[ -7.18579794 -27.65473865  -7.17656704  -6.91428385 -21.55586131]\n",
      "[-12.8645674  -27.63927749 -11.84194559  -9.14784585 -18.40542281]\n",
      "75\n",
      "0.8443769482855087\n",
      "all: Val loss: 0.13300000131130219 Thresh: 0.35 F1: 0.844 R: 0.958 P: 0.755 D: 0.5 H: 0.5 Error: 5.872\n",
      "0.8264870193887611\n",
      "[-10.49279732 -34.51683158  -9.54524279  -9.26774509 -15.01457363]\n",
      "[ -9.70054339 -25.1375339   -9.55072352  -8.43473715 -27.00283494]\n",
      "[-11.79007115 -40.34875329  -9.83665163  -8.3687307  -27.20830754]\n",
      "[-11.01769793 -26.47144288 -10.12648989  -9.53216404 -13.29617643]\n",
      "[-19.94419192 -46.4360332  -20.52681762 -20.78270173 -48.65828664]\n",
      "Epoch 9: Loss 0.30300000309944153\n",
      "75\n",
      "0.8332530120481928\n",
      "all: Val loss: 0.13099999725818634 Thresh: 0.35 F1: 0.833 R: 0.951 P: 0.741 D: 0.5 H: 0.5 Error: 6.207\n",
      "0.8139198080026482\n",
      "Saving model with 0.8332530120481928\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26640 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "9000\n",
      "starting epoch 10, alpha: 0.09, beta: 0.0 drop: 0.91 Learning rate: 0.07702895877075562\n",
      "[ -9.01953952 -33.85209812  -8.48592498  -9.55572855 -15.07807921]\n",
      "[ -6.36735023 -31.12276142  -6.00083945  -6.52029128 -18.92693097]\n",
      "[ -9.99045077 -39.87226967  -9.67219442  -7.8075193  -41.38951049]\n",
      "[ -8.22856317 -27.32691038  -7.53294269  -6.65565261 -17.66794446]\n",
      "[ -7.68224579 -24.20151317  -7.48333426  -7.76381681  -8.94784137]\n",
      "[ -6.35683245 -37.50867844  -6.29597026  -7.15965759 -23.43058139]\n",
      "75\n",
      "0.8593810295593115\n",
      "all: Val loss: 0.12700000405311584 Thresh: 0.35 F1: 0.859 R: 0.934 P: 0.796 D: 0.5 H: 0.5 Error: 5.605\n",
      "0.8501446633110064\n",
      "[ -6.52774718 -22.52312664  -6.57490282  -6.616588   -12.68323557]\n",
      "[-10.82427678 -43.38098051  -9.80533711 -10.64602644 -27.45612975]\n",
      "[ -8.01060972 -28.30396109  -7.76628554  -8.28326664 -12.47100362]\n",
      "[ -7.07491065 -17.61732711  -7.03607521  -6.98595342  -9.68966469]\n",
      "[ -9.28643811 -19.81808484  -9.26171341  -9.63290936 -11.89471076]\n",
      "Epoch 10: Loss 0.29100000858306885\n",
      "75\n",
      "0.8404406402333704\n",
      "all: Val loss: 0.13500000536441803 Thresh: 0.35 F1: 0.84 R: 0.908 P: 0.782 D: 0.5 H: 0.5 Error: 6.396\n",
      "0.8240319361277445\n",
      "Saving model with 0.8404406402333704\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26641 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "10000\n",
      "starting epoch 11, alpha: 0.1, beta: 0.0 drop: 0.896 Learning rate: 0.07641451763864587\n",
      "[ -8.75215199 -39.71050246  -8.52305427  -9.04721995 -42.20029718]\n",
      "[ -5.55544456 -25.49319783  -5.2447791   -6.00708872  -8.60728819]\n",
      "[ -8.11965952 -21.24296416  -8.02128056  -8.43949354 -11.43992425]\n",
      "[ -7.38471333 -28.4880495   -7.0639371   -7.53933517  -9.91919004]\n",
      "[ -7.1894133  -33.34603057  -6.77572148  -6.78026451 -13.02950626]\n",
      "[-10.9139905  -38.24032489 -10.530681    -9.26976383 -27.82969314]\n",
      "75\n",
      "0.8543310223922225\n",
      "all: Val loss: 0.12600000202655792 Thresh: 0.35 F1: 0.854 R: 0.966 P: 0.766 D: 0.5 H: 0.5 Error: 5.717\n",
      "0.8347591175146329\n",
      "[ -9.4302213  -35.61126426  -9.00275157  -9.2749529  -15.46349793]\n",
      "[ -6.06400976 -30.92940228  -5.77811252  -7.20836037  -9.84067753]\n",
      "[-12.15260765 -40.60676236 -10.6472599   -8.93973991 -33.77851081]\n",
      "[-11.91016274 -28.3538775  -10.59787258 -10.00136919 -14.06126022]\n",
      "[-17.72240087 -45.9361914  -15.97864214 -15.3503602  -37.3293448 ]\n",
      "Epoch 11: Loss 0.28299999237060547\n",
      "75\n",
      "0.8532761836726596\n",
      "all: Val loss: 0.125 Thresh: 0.35 F1: 0.853 R: 0.96 P: 0.768 D: 0.5 H: 0.5 Error: 5.628\n",
      "0.8308510638297872\n",
      "Saving model with 0.8532761836726596\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26636 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "11000\n",
      "starting epoch 12, alpha: 0.11, beta: 0.0 drop: 0.882 Learning rate: 0.07574530561293649\n",
      "[ -8.96975359 -40.4332866   -8.71787138  -9.32940518 -26.36784385]\n",
      "[ -8.41641435 -30.34965946  -7.76335685  -6.67299485 -12.45161657]\n",
      "[ -7.25439033 -32.68185372  -6.61229047  -6.68241923 -15.65208082]\n",
      "[ -6.61118295 -17.44619161  -6.31056747  -6.58414352  -8.36347424]\n",
      "[ -7.9125158  -32.02302853  -7.69521886  -8.55186794 -13.35653845]\n",
      "[-10.06387031 -40.94165053  -9.40852019  -9.31216302 -23.77824072]\n",
      "75\n",
      "0.8525908739365815\n",
      "all: Val loss: 0.12399999797344208 Thresh: 0.35 F1: 0.853 R: 0.929 P: 0.788 D: 0.5 H: 0.5 Error: 5.824\n",
      "0.8416600159616919\n",
      "[ -5.68151431 -27.00398669  -5.5866874   -5.68109511  -7.78675383]\n",
      "[ -5.63621679 -26.86537545  -5.48300466  -5.72429394 -10.68025213]\n",
      "[ -9.96634081 -39.58969685  -9.55641063  -9.73210554 -20.18729093]\n",
      "[ -5.27348993 -27.22716245  -5.1209055   -6.19765219  -8.93543524]\n",
      "[ -5.92351775 -24.73182116  -5.82721392  -6.69012185  -8.01466047]\n",
      "Epoch 12: Loss 0.27399998903274536\n",
      "75\n",
      "0.8651458721291123\n",
      "all: Val loss: 0.12099999934434891 Thresh: 0.35 F1: 0.865 R: 0.947 P: 0.797 D: 0.5 H: 0.5 Error: 5.461\n",
      "0.8514299370668366\n",
      "Saving model with 0.8651458721291123\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26665 samples\n",
      "64.35081472502124\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "12000\n",
      "starting epoch 13, alpha: 0.12, beta: 0.0 drop: 0.8680000000000001 Learning rate: 0.07502232925208363\n",
      "[ -7.19023098 -29.9544509   -6.99097526  -6.0589836  -16.90276846]\n",
      "[ -8.52379072 -39.00404522  -8.04654375  -8.40371151 -15.65943537]\n",
      "[ -7.21969802 -36.76318703  -6.75931898  -6.95700971 -19.23841399]\n",
      "[ -6.11541855 -29.54162662  -5.90287788  -6.70784919 -13.25618552]\n",
      "[-10.60927452 -34.18403493  -9.43771427  -9.44431813 -12.94442741]\n",
      "[ -7.24590792 -33.12854452  -7.00744016  -7.83267696 -19.21319946]\n",
      "75\n",
      "0.8543178374930229\n",
      "all: Val loss: 0.11999999731779099 Thresh: 0.35 F1: 0.854 R: 0.966 P: 0.766 D: 0.5 H: 0.5 Error: 5.637\n",
      "0.8362864374948966\n",
      "[ -6.88588586 -24.01577278  -6.76397179  -7.37298207  -8.87508049]\n",
      "[ -7.11345834 -35.88359329  -7.05325607  -7.27611096 -20.72742937]\n",
      "[ -5.61902581 -22.18730643  -5.36575701  -5.61545546  -8.05190485]\n",
      "[ -7.45723917 -38.74223856  -7.36301833  -8.31770687 -17.52445175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -8.0706136  -35.18198251  -7.68138158  -9.0505631  -22.82674792]\n",
      "Epoch 13: Loss 0.26600000262260437\n",
      "75\n",
      "0.8601904159917828\n",
      "all: Val loss: 0.11699999868869781 Thresh: 0.35 F1: 0.86 R: 0.962 P: 0.778 D: 0.5 H: 0.5 Error: 5.391\n",
      "0.8467064901517598\n",
      "Saving model with 0.8601904159917828\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26646 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "13000\n",
      "starting epoch 14, alpha: 0.13, beta: 0.0 drop: 0.8540000000000001 Learning rate: 0.07424667598121067\n",
      "[-11.71252414 -38.89248006 -11.46944862 -10.23868718 -27.92864445]\n",
      "[ -6.27811998 -29.32161613  -5.98109459  -7.05038245 -11.8033154 ]\n",
      "[ -8.56415861 -34.0209567   -7.91004504  -8.27717085 -19.61512456]\n",
      "[ -6.99955383 -36.15608507  -6.5843324   -8.18755037 -19.89128213]\n",
      "[ -5.92154447 -29.99564257  -5.68311026  -7.15836527 -10.63272387]\n",
      "[ -6.16819488 -24.98357964  -5.94950774  -6.43437158  -7.13860821]\n",
      "75\n",
      "0.8483897481879998\n",
      "all: Val loss: 0.12200000137090683 Thresh: 0.35 F1: 0.848 R: 0.889 P: 0.811 D: 0.5 H: 0.5 Error: 6.221\n",
      "0.8408759124087591\n",
      "[ -8.06641612 -33.99932377  -7.56725566  -7.97746492 -12.86153781]\n",
      "[ -5.86546742 -28.17213481  -5.57774339  -6.23978352  -9.91522001]\n",
      "[ -5.25400639 -30.9566586   -5.05555515  -6.19529467 -11.12053866]\n",
      "[-11.36143664 -44.68660675  -9.22251532  -7.60734629 -26.43358163]\n",
      "[ -5.33564142 -28.7009212   -4.98696517  -5.88007277  -8.21223226]\n",
      "Epoch 14: Loss 0.2590000033378601\n",
      "75\n",
      "0.8678165084426408\n",
      "all: Val loss: 0.12700000405311584 Thresh: 0.35 F1: 0.868 R: 0.88 P: 0.856 D: 0.5 H: 0.5 Error: 5.775\n",
      "0.8705785679602586\n",
      "all: Val loss: 0.12700000405311584 Thresh: 0.4 F1: 0.871 R: 0.912 P: 0.833 D: 0.5 H: 0.5 Error: 5.38\n",
      "Saving model with 0.8705785679602586\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26651 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "14000\n",
      "starting epoch 15, alpha: 0.14, beta: 0.0 drop: 0.8400000000000001 Learning rate: 0.07341951245651746\n",
      "[ -4.9616925  -33.44962922  -4.8227055   -5.93783196 -10.84404542]\n",
      "[ -8.20889649 -40.1614423   -7.85114057  -9.35525402 -14.87167916]\n",
      "[ -4.97510299 -29.65548992  -4.81566446  -5.94388982 -10.57626307]\n",
      "[ -6.16090188 -34.03452116  -5.71380723  -6.59574459  -9.88442697]\n",
      "[ -6.94538908 -20.09442344  -6.72965682  -7.14768399  -7.13890837]\n",
      "[ -6.20657917 -34.60872634  -5.5767989   -7.10367868 -18.07388861]\n",
      "75\n",
      "0.8690098290434849\n",
      "all: Val loss: 0.12099999934434891 Thresh: 0.35 F1: 0.869 R: 0.935 P: 0.812 D: 0.5 H: 0.5 Error: 5.501\n",
      "0.8569856553350363\n",
      "[ -5.84750978 -28.62082443  -5.70369752  -6.59820632  -8.97379058]\n",
      "[ -6.55632132 -37.54459221  -6.01773712  -6.84649978 -22.24797051]\n",
      "[ -5.3253259  -29.95379804  -5.17045922  -6.31726767 -11.84787175]\n",
      "[ -6.93432029 -28.21530815  -6.80270292  -7.65834478  -8.70978808]\n",
      "[ -8.19457794 -32.03500146  -7.87904129  -7.40885464 -12.81395065]\n",
      "Epoch 15: Loss 0.25099998712539673\n",
      "75\n",
      "0.8827642336576523\n",
      "all: Val loss: 0.11900000274181366 Thresh: 0.35 F1: 0.883 R: 0.915 P: 0.852 D: 0.5 H: 0.5 Error: 5.13\n",
      "0.8820874578071074\n",
      "Saving model with 0.8827642336576523\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26645 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "15000\n",
      "starting epoch 16, alpha: 0.15, beta: 0.0 drop: 0.8260000000000001 Learning rate: 0.0725420828105187\n",
      "[ -7.52372869 -41.60677146  -7.38227468  -7.7168197  -35.54952748]\n",
      "[ -5.93553198 -32.27972479  -5.69732241  -6.95525733 -10.33728494]\n",
      "[ -9.74399487 -36.21608426  -9.81482979  -9.29093625 -27.26994657]\n",
      "[ -5.81221304 -26.9342187   -5.85894062  -6.44516568  -9.97988287]\n",
      "[ -7.35686181 -37.69235316  -7.1516262   -8.35728405 -16.07560266]\n",
      "[ -5.92174068 -34.60852817  -5.56942027  -6.3043914  -10.71294534]\n",
      "75\n",
      "0.867367939629545\n",
      "all: Val loss: 0.11599999666213989 Thresh: 0.35 F1: 0.867 R: 0.93 P: 0.813 D: 0.5 H: 0.5 Error: 5.339\n",
      "0.8575336042513284\n",
      "[ -3.84292575 -32.6116618   -3.61016767  -4.69924005  -9.31140583]\n",
      "[ -6.00054121 -33.69233006  -5.73354929  -8.01100141 -13.66461725]\n",
      "[ -5.22785804 -27.29327723  -4.88766371  -6.37145505  -6.77863575]\n",
      "[ -6.93102002 -39.34852079  -6.2874264   -8.5515292  -20.07395952]\n",
      "[ -6.49828844 -33.87212366  -6.09689592  -6.84327742 -12.88086012]\n",
      "Epoch 16: Loss 0.24400000274181366\n",
      "75\n",
      "0.8855187020158088\n",
      "all: Val loss: 0.11900000274181366 Thresh: 0.35 F1: 0.886 R: 0.941 P: 0.836 D: 0.5 H: 0.5 Error: 4.983\n",
      "0.8764992648765766\n",
      "Saving model with 0.8855187020158088\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26629 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "16000\n",
      "starting epoch 17, alpha: 0.16, beta: 0.0 drop: 0.812 Learning rate: 0.07161570678075037\n",
      "[ -6.00297007 -34.97566272  -5.4167898   -6.3996315  -11.14031429]\n",
      "[ -5.41713569 -24.10239532  -5.18245451  -5.96418621  -6.38457406]\n",
      "[ -6.78766008 -30.1755556   -5.98012358  -5.76558092 -15.89965881]\n",
      "[ -4.84706823 -27.85991955  -4.76511486  -6.29379074  -8.69177966]\n",
      "[ -5.56616406 -36.53183569  -5.28139031  -7.29770885 -10.99168558]\n",
      "[ -5.7114762  -34.97537031  -5.01481526  -6.06042539 -13.41211042]\n",
      "75\n",
      "0.8657335001563967\n",
      "all: Val loss: 0.11400000005960464 Thresh: 0.35 F1: 0.866 R: 0.956 P: 0.791 D: 0.5 H: 0.5 Error: 5.259\n",
      "0.8530234044252735\n",
      "[ -5.14156764 -26.79979176  -5.12732751  -6.05558753  -8.14702274]\n",
      "[ -6.56182264 -27.61141697  -6.2700101   -7.16747681  -7.71404532]\n",
      "[ -6.85002582 -37.61376667  -6.48309175  -7.86459619 -20.76437373]\n",
      "[ -7.60759781 -31.33119159  -6.98133166  -6.994003   -10.15835003]\n",
      "[ -7.14329557 -35.94789914  -6.79256715  -8.64382648  -9.1276278 ]\n",
      "Epoch 17: Loss 0.23800000548362732\n",
      "75\n",
      "0.8832294359130303\n",
      "all: Val loss: 0.11299999803304672 Thresh: 0.35 F1: 0.883 R: 0.944 P: 0.83 D: 0.5 H: 0.5 Error: 4.849\n",
      "0.8754755065600497\n",
      "Saving model with 0.8832294359130303\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26642 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "17000\n",
      "starting epoch 18, alpha: 0.17, beta: 0.0 drop: 0.798 Learning rate: 0.07064177772475912\n",
      "[ -6.28419716 -37.06209336  -5.97606101  -7.14593499 -17.63079842]\n",
      "[ -4.73385329 -30.76885303  -4.50264547  -5.44654371  -7.04090653]\n",
      "[ -7.04996893 -30.10881151  -6.47889993  -7.93092707  -8.71156129]\n",
      "[ -6.05132714 -26.71857114  -5.74566479  -6.93223534  -8.24807192]\n",
      "[ -5.79968572 -31.33274349  -5.47619754  -6.75797186  -8.68869375]\n",
      "[ -3.77028408 -30.11812435  -3.54418171  -4.30614857 -22.66548507]\n",
      "75\n",
      "0.881160526818095\n",
      "all: Val loss: 0.11900000274181366 Thresh: 0.35 F1: 0.881 R: 0.946 P: 0.825 D: 0.5 H: 0.5 Error: 5.118\n",
      "0.8721057571964955\n",
      "[-10.75003983 -35.66604228  -9.13076258  -8.74055375 -15.3995132 ]\n",
      "[ -4.71713528 -35.37879688  -4.34277064  -6.52824403 -12.0963488 ]\n",
      "[ -5.16170172 -29.56824082  -4.72907406  -6.56830203  -7.16241899]\n",
      "[ -6.76722323 -33.19812514  -6.5480072   -8.51154768  -9.88232284]\n",
      "[ -4.90169589 -32.62241895  -4.45350184  -5.89782398 -11.53013062]\n",
      "Epoch 18: Loss 0.23100000619888306\n",
      "75\n",
      "0.8800762631077217\n",
      "all: Val loss: 0.11599999666213989 Thresh: 0.35 F1: 0.88 R: 0.944 P: 0.825 D: 0.5 H: 0.5 Error: 5.099\n",
      "0.8701029313921177\n",
      "Saving model with 0.8800762631077217\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26641 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "18000\n",
      "starting epoch 19, alpha: 0.18, beta: 0.0 drop: 0.784 Learning rate: 0.06962176052436019\n",
      "[ -5.4611148  -28.75731181  -5.14797763  -6.52002832  -7.59882808]\n",
      "[ -6.65058561 -30.13130083  -6.35141305  -7.92367826  -7.35896315]\n",
      "[ -4.75715098 -32.33679233  -4.64468428  -5.99044887 -10.0991045 ]\n",
      "[ -7.8701612  -40.2077285   -7.0019184  -10.25326346 -31.19292244]\n",
      "[ -5.75380228 -38.01284144  -5.27498746  -8.6223758  -15.47406894]\n",
      "[ -6.10135505 -26.43459816  -5.86428879  -7.08785869  -8.30758333]\n",
      "75\n",
      "0.8770365931308288\n",
      "all: Val loss: 0.11500000208616257 Thresh: 0.35 F1: 0.877 R: 0.922 P: 0.837 D: 0.5 H: 0.5 Error: 5.418\n",
      "0.8728901136755082\n",
      "[ -5.88461237 -32.66638159  -5.30843558  -6.75112277  -8.79544382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5.16714673 -30.09691021  -5.00402077  -6.0541253  -10.37372278]\n",
      "[ -6.25785458 -36.41473415  -5.87185682  -9.15242434 -11.85366192]\n",
      "[ -4.83941326 -25.83263285  -4.69899719  -6.01974747  -5.54911576]\n",
      "[ -4.61556981 -30.78491691  -4.11036921  -4.69003913 -34.6115924 ]\n",
      "Epoch 19: Loss 0.22599999606609344\n",
      "75\n",
      "0.8749467487703807\n",
      "all: Val loss: 0.1120000034570694 Thresh: 0.35 F1: 0.875 R: 0.955 P: 0.807 D: 0.5 H: 0.5 Error: 4.897\n",
      "0.8650292212920551\n",
      "Saving model with 0.8749467487703807\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26645 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "19000\n",
      "starting epoch 20, alpha: 0.19, beta: 0.0 drop: 0.77 Learning rate: 0.06855718938231598\n",
      "[ -4.84221681 -34.89131207  -4.79834484  -6.32803914  -9.11744185]\n",
      "[ -4.35581882 -29.34226178  -3.95449722  -5.92169187  -8.49553362]\n",
      "[ -4.74439084 -19.5699681   -4.5227864   -5.11931442  -5.81464618]\n",
      "[ -4.68496653 -29.15498465  -4.42231957  -6.12727136  -7.09157352]\n",
      "[ -5.4563585  -30.40364969  -5.2343274   -7.64517444  -9.14843447]\n",
      "[ -8.79033719 -37.22828341  -7.83048421  -8.40938225 -15.52272139]\n",
      "75\n",
      "0.8804532141205488\n",
      "all: Val loss: 0.11100000143051147 Thresh: 0.35 F1: 0.88 R: 0.956 P: 0.816 D: 0.5 H: 0.5 Error: 4.88\n",
      "0.871452682811948\n",
      "[-10.41118408 -37.93701268  -7.33740923  -6.64216371 -15.33392911]\n",
      "[ -4.83403528 -31.78418724  -4.62394135  -5.8622106  -10.18680179]\n",
      "[ -5.88647018 -30.66698716  -5.0519835   -6.42659743 -10.50059339]\n",
      "[ -4.90947764 -35.95374059  -4.2984222   -6.96873303 -13.31271234]\n",
      "[ -5.0793257  -37.75941716  -4.58606235  -7.21708755 -14.31089764]\n",
      "Epoch 20: Loss 0.21899999678134918\n",
      "75\n",
      "0.8846949327817993\n",
      "all: Val loss: 0.11500000208616257 Thresh: 0.35 F1: 0.885 R: 0.916 P: 0.856 D: 0.5 H: 0.5 Error: 5.343\n",
      "0.8813815516003476\n",
      "Saving model with 0.8846949327817993\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26642 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "20000\n",
      "starting epoch 21, alpha: 0.2, beta: 0.0 drop: 0.756 Learning rate: 0.06744966551474935\n",
      "[ -4.29987631 -32.18213197  -4.07126558  -6.04449879 -12.30381545]\n",
      "[ -5.44159841 -35.87717403  -5.06593874  -5.71441252 -25.46947129]\n",
      "[ -4.68325314 -31.14913719  -4.41962739  -5.94103789 -10.73306881]\n",
      "[ -5.0458631  -32.46480178  -4.89757587  -6.55993473  -8.26164213]\n",
      "[ -7.19479322 -38.14941189  -6.86943349  -8.32602379 -17.18956941]\n",
      "[ -4.48305528 -30.15189019  -4.09631808  -5.05383884 -19.21777661]\n",
      "75\n",
      "0.8803669159022585\n",
      "all: Val loss: 0.11299999803304672 Thresh: 0.35 F1: 0.88 R: 0.956 P: 0.816 D: 0.5 H: 0.5 Error: 4.995\n",
      "0.8705678643107315\n",
      "[ -5.28490221 -35.08006754  -4.5819618   -5.81919742 -19.62760793]\n",
      "[ -6.1737555  -34.27668675  -5.20599332  -5.98132081 -10.58115929]\n",
      "[ -5.37385326 -29.7732383   -5.07199034  -5.97181934 -13.4918944 ]\n",
      "[ -4.07194951 -30.04875732  -3.66459425  -6.1968756  -11.36996554]\n",
      "[ -3.46367706 -24.80464719  -3.26869844  -4.30460837  -4.02668483]\n",
      "Epoch 21: Loss 0.21299999952316284\n",
      "75\n",
      "0.8784274349270103\n",
      "all: Val loss: 0.11100000143051147 Thresh: 0.35 F1: 0.878 R: 0.956 P: 0.813 D: 0.5 H: 0.5 Error: 4.762\n",
      "0.8697397945124591\n",
      "Saving model with 0.8784274349270103\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26635 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "21000\n",
      "starting epoch 22, alpha: 0.2, beta: 0.0 drop: 0.742 Learning rate: 0.06630085474276255\n",
      "[ -4.707909   -29.21168249  -4.49807948  -6.70378293  -6.35893357]\n",
      "[ -4.09654151 -31.38937599  -3.93969733  -5.38257789  -7.52721052]\n",
      "[ -5.47602943 -36.92372678  -4.88790775  -7.33873488 -14.25652635]\n",
      "[ -5.45219633 -39.06189717  -5.09206765  -7.69943698 -15.76244529]\n",
      "[ -4.70425891 -24.29730021  -4.50664226  -6.02384824  -4.80936826]\n",
      "[-15.04596405 -38.24171707 -11.09567053  -9.15023262 -21.07899846]\n",
      "75\n",
      "0.884680393362016\n",
      "all: Val loss: 0.1120000034570694 Thresh: 0.35 F1: 0.885 R: 0.957 P: 0.823 D: 0.5 H: 0.5 Error: 4.682\n",
      "0.8756807585315206\n",
      "[ -8.30144335 -35.0806121   -7.12451266  -8.09623973  -9.90489274]\n",
      "[-18.07715928 -36.90110687 -15.23095413 -10.0019999  -33.87531457]\n",
      "[ -4.69417675 -23.0536796   -4.55851044  -5.84681068  -5.04495149]\n",
      "[ -4.54671581 -30.27631986  -4.23921459  -6.14255925  -7.15752372]\n",
      "[ -5.32126506 -28.95941546  -4.65489094  -5.92043984  -6.88426148]\n",
      "Epoch 22: Loss 0.21199999749660492\n",
      "75\n",
      "0.8826056932402363\n",
      "all: Val loss: 0.1120000034570694 Thresh: 0.35 F1: 0.883 R: 0.953 P: 0.822 D: 0.5 H: 0.5 Error: 4.663\n",
      "0.8726973426354159\n",
      "Saving model with 0.8826056932402363\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26639 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "22000\n",
      "starting epoch 23, alpha: 0.2, beta: 0.0 drop: 0.728 Learning rate: 0.06511248498688395\n",
      "[ -5.59360458 -30.72001815  -5.10550245  -5.61366026 -11.27806473]\n",
      "[ -4.18359668 -25.82592157  -3.80869131  -5.35560862  -5.50032191]\n",
      "[ -5.92911912 -23.91361278  -5.64251399  -6.89972317  -7.21495352]\n",
      "[ -4.95550207 -35.27963058  -4.48102069  -6.29851581 -14.30407617]\n",
      "[ -6.38633264 -40.80400173  -5.61628044  -8.06590431 -21.73849603]\n",
      "[ -5.47429445 -33.94486162  -5.45068557  -7.5634065   -7.4216711 ]\n",
      "75\n",
      "0.8897582057300399\n",
      "all: Val loss: 0.1120000034570694 Thresh: 0.35 F1: 0.89 R: 0.93 P: 0.853 D: 0.5 H: 0.5 Error: 4.963\n",
      "0.8862685431723089\n",
      "[ -4.69031358 -36.13138105  -4.42246743  -6.3861492  -13.64252871]\n",
      "[ -3.82868997 -31.93062977  -3.528998    -5.31564613  -7.68489138]\n",
      "[ -4.37315009 -26.56877885  -4.10969379  -4.80057488  -5.00973053]\n",
      "[ -7.50166406 -39.57414516  -6.10146367  -7.15000955 -21.8931312 ]\n",
      "[ -7.05429763 -43.31684605  -8.27516354  -8.1338474  -64.46212468]\n",
      "Epoch 23: Loss 0.210999995470047\n",
      "75\n",
      "0.8768115942028984\n",
      "all: Val loss: 0.11299999803304672 Thresh: 0.35 F1: 0.877 R: 0.964 P: 0.804 D: 0.5 H: 0.5 Error: 5.043\n",
      "0.8651033386327504\n",
      "Saving model with 0.8768115942028984\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26648 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "23000\n",
      "starting epoch 24, alpha: 0.2, beta: 0.0 drop: 0.714 Learning rate: 0.06388634366811145\n",
      "[ -6.72804531 -31.74506885  -6.44277061  -7.95921592  -8.58525409]\n",
      "[ -4.80136546 -33.00427211  -4.70281967  -6.74136263  -8.42137835]\n",
      "[ -4.77075516 -35.15751398  -4.21454684  -5.81373393 -12.55810382]\n",
      "[ -4.14547692 -29.17691612  -3.89636332  -4.89714076  -8.75489386]\n",
      "[ -5.31409401 -38.44045524  -5.10653379  -7.79814766 -14.05284073]\n",
      "[ -4.33384426 -24.94103433  -3.60548238  -4.90153204  -7.53423584]\n",
      "75\n",
      "0.8889306662656038\n",
      "all: Val loss: 0.11400000005960464 Thresh: 0.35 F1: 0.889 R: 0.938 P: 0.845 D: 0.5 H: 0.5 Error: 4.824\n",
      "0.8836869618696186\n",
      "[ -4.61445755 -34.88285475  -4.17916011  -6.18041039 -11.01952201]\n",
      "[ -5.16270049 -29.01598113  -4.36342356  -5.74349743  -6.52792566]\n",
      "[ -5.20614933 -38.46458291  -4.76842872  -7.82442601 -17.7095483 ]\n",
      "[ -4.83790299 -29.70265462  -4.55543703  -6.66909698  -7.06608946]\n",
      "[ -4.63858739 -30.99501451  -4.28139646  -5.59656845  -7.77810578]\n",
      "Epoch 24: Loss 0.20800000429153442\n",
      "75\n",
      "0.8839655365335124\n",
      "all: Val loss: 0.1120000034570694 Thresh: 0.35 F1: 0.884 R: 0.925 P: 0.847 D: 0.5 H: 0.5 Error: 5.199\n",
      "0.8811775472849299\n",
      "Saving model with 0.8839655365335124\n",
      "0.1 LR, SWA: False\n",
      "31005\n",
      "[10503, 2648, 1833, 1549, 1592, 1671, 2006, 2267, 6936]\n",
      "31005\n",
      "This batch uses 26636 samples\n",
      "64.35081472502125\n",
      "987\n",
      "993\n",
      "[]\n",
      "This batch uses 993 of samples in previous batch\n",
      "24000\n",
      "starting epoch 25, alpha: 0.2, beta: 0.0 drop: 0.7 Learning rate: 0.06262427501946155\n",
      "[ -4.45008923 -35.08061312  -4.250355    -6.92007818 -12.38058784]\n",
      "[ -5.62732627 -36.8796022   -5.2486042   -7.35755742 -12.68808747]\n",
      "[ -5.8853702  -34.58610697  -5.35047261  -7.71400126  -9.90116785]\n",
      "[ -4.05642433 -32.83551364  -3.8507599   -6.28606215  -7.41633925]\n",
      "[-12.33404489 -44.29224244  -9.84141398 -14.26486174 -34.78667982]\n",
      "[ -5.35194229 -32.89657446  -4.7000309   -7.7840518   -8.46988796]\n",
      "75\n",
      "0.8967041198501873\n",
      "all: Val loss: 0.10999999940395355 Thresh: 0.35 F1: 0.897 R: 0.942 P: 0.855 D: 0.5 H: 0.5 Error: 4.571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8921879174140365\n",
      "[ -4.17839173 -36.48393382  -4.12566005  -6.94288234  -8.65713036]\n",
      "[ -4.39566894 -26.62892177  -4.19808319  -5.67947495  -5.04351109]\n",
      "[ -4.3651953  -36.32140412  -3.83439201  -5.28550346 -12.69437245]\n",
      "[ -7.18492653 -39.97875181  -6.19437485 -10.4220363  -19.61374636]\n"
     ]
    }
   ],
   "source": [
    "#### Epoch 1 to 30-50 should be 0.1\n",
    "# Epoch 30-50+ should be 0.05\n",
    "# Epoch 80+ should be 0.01\n",
    "import tqdm\n",
    "\n",
    "best_val = 0.6\n",
    "fine_tune = False\n",
    "countries['all'] = [0, len(test_x)]\n",
    "ft_epochs = 0\n",
    "train_ids = [x for x in range(len(train_y)) ]\n",
    "batch, uniques = equibatch(train_ids)\n",
    "f1 = 0.2\n",
    "for i in range(2, 81):\n",
    "    max_keep_rate = 0.5\n",
    "    SWA = False\n",
    "    fine_tune = False\n",
    "    ft_learning_rate = 0.1# np.min([i * 0.02, 0.1])# 0.1#np.max([(.05) * (1 - (ft_epochs * .05)), 0.01])\n",
    "    ft_epochs += 1\n",
    "    #if i <= 100:\n",
    "    #    ada  = 1 - ((100 - i) / (100 - 81))\n",
    "    #    keeprate = 0.55 + (ada * 0.13)\n",
    "    #\n",
    "    if i >= 3:\n",
    "        #SWA = True\n",
    "        max_keep_rate = 0.5\n",
    "    if i >= 60:\n",
    "        max_keep_rate = 0.5\n",
    "     #   max_keep_rate = np.minimum(0.4, max_keep_rate)\n",
    "\n",
    "    #   SWA = True\n",
    "    #    ft_learning_rate = 0.02\n",
    "    #    max_keep_rate = 0.67\n",
    "    al = np.min( [0.01 * (i - 1), 0.2] )\n",
    "    print(f\"{ft_learning_rate} LR, SWA: {SWA}\")\n",
    "    be = 0.0\n",
    "    test_al = al\n",
    "    if fine_tune == True:\n",
    "        op = ft_op\n",
    "        print(f\"FINE TUNING WITH {ft_learning_rate} LR\")\n",
    "    else:\n",
    "        op = train_step\n",
    "        \n",
    "    train_ids = [x for x in range(len(train_y))]\n",
    "    randomize = train_ids\n",
    "    randomize, uniques_i = equibatch(train_ids)\n",
    "    overlap = len([x for x in uniques_i if x in uniques])\n",
    "    print(f\"This batch uses {overlap} of samples in previous batch\")\n",
    "    uniques = uniques_i\n",
    "    \n",
    "    loss = train_loss\n",
    "    BATCH_SIZE = 32\n",
    "    test_ids = [x for x in range(0, len(test_x))]\n",
    "    losses = []\n",
    "    \n",
    "    keeprate = np.max(((1.05 - (i * 0.014)\n",
    "                               ), max_keep_rate))\n",
    "    keeprate = np.min((keeprate, 1))\n",
    "    warm_up_steps = (i - 1) * 1000\n",
    "    print(warm_up_steps)\n",
    "    def calc_cosine_decay(epoch, maxepoch, offset):\n",
    "        import math\n",
    "        return 0.5 * (1 + math.cos(math.pi * (epoch - offset) / (maxepoch - offset)))\n",
    "    if i < 100:           \n",
    "        cosdec = calc_cosine_decay(i, 81, 0)\n",
    "    adam_lr =  8e-4 * cosdec#(1 - ((i / 100))) * 1e-4\n",
    "    ft_learning_rate = 8e-2 * cosdec#(1 - ((i / 100))) * 1e-2\n",
    "    \n",
    "    print(f\"starting epoch {i}, alpha: {al}, beta: {be} drop: {keeprate}\"\n",
    "         f\" Learning rate: {ft_learning_rate}\")\n",
    "    \n",
    "    n_batch = int(len(randomize) // BATCH_SIZE)\n",
    "    for k in range(int(len(randomize) // BATCH_SIZE)):#tqdm.notebook.trange(int(len(randomize) // BATCH_SIZE)):\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        x_batch, y_batch = augment_batch(batch_ids, BATCH_SIZE)\n",
    "        warm_up_steps += 1\n",
    "        if warm_up_steps < 2000:\n",
    "            keeprate = 0.6 + (0.4 * (warm_up_steps / 2000))\n",
    "            keeprate = np.min((keeprate, 1))\n",
    "        if warm_up_steps < 6000:\n",
    "            adam_lr = ((warm_up_steps) / 6000) * 1e-4\n",
    "            ft_learning_rate= ((warm_up_steps) / 6000) * 1e-2\n",
    "        if i == 1:\n",
    "            if k % 20 == 0:\n",
    "                print(keeprate, adam_lr)\n",
    "        opt, tr = sess.run([op, loss], # op needs to be train_step for SAM\n",
    "                          feed_dict={inp: x_batch,\n",
    "                                     length: np.full((BATCH_SIZE,), LEN),\n",
    "                                     labels: y_batch,\n",
    "                                     is_training: True,\n",
    "                                     loss_weight: 1.0,\n",
    "                                     keep_rate: keeprate,\n",
    "                                     alpha: al,\n",
    "                                     beta_: be,\n",
    "                                     init_lr: adam_lr,\n",
    "                                     ft_lr: ft_learning_rate,\n",
    "                                     })\n",
    "        losses.append(tr)\n",
    "        if k % 100 == 0 and k > 0:\n",
    "            fp = evaluate_on_plots(x_batch_test2)      \n",
    "        if k % 600 == 0 and k > 0:\n",
    "            #fp = evaluate_on_plots(x_batch_test2)\n",
    "            #if fp < 0.0:\n",
    "            val_loss, f1, error, haus, dice = calculate_metrics('all', al = 0.33, canopy_thresh = 75)\n",
    "            if f1 >= 0.90 and SWA:\n",
    "                os.mkdir(f\"{model_path}/2322-{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}-{str(k)}/\")\n",
    "                save_path = saver.save(sess, f\"{model_path}/2322-{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}-{str(k)}/model\")\n",
    "                sess.run(swa_op)\n",
    "\n",
    "    print(f\"Epoch {i}: Loss {np.around(np.mean(losses[:-1]), 3)}\")\n",
    "    #if SWA:\n",
    "        ##sess.run(save_weight_backups)\n",
    "    #val_loss, f1, error, haus, dice = calculate_metrics('all', al = test_al, canopy_thresh = 75)\n",
    "        #if f1 >= 0.905 or val_loss < 0.0995:\n",
    "        ##    sess.run(swa_op)\n",
    "            #sess.run(save_weight_backups)\n",
    "            #sess.run(swa_to_weights)\n",
    "    run_metrics = False\n",
    "    metrics[0, i] = np.mean(losses[:-1])\n",
    "    if f1 > 0.9:#(i > 80) and (i % 1) == 0:\n",
    "        run_metrics = True\n",
    "    elif i % 1 == 0:\n",
    "        run_metrics = True\n",
    "    if run_metrics:\n",
    "        #if SWA:\n",
    "            #sess.run(save_weight_backups)\n",
    "            #sess.run(swa_to_weights)\n",
    "        val_loss, f1, error, haus, dice = calculate_metrics('all', al = test_al, canopy_thresh = 75)\n",
    "        metrics[1, i] = val_loss\n",
    "        metrics[2, i] = error\n",
    "        metrics[3, i] = haus\n",
    "        metrics[4, i] = dice\n",
    "        metrics[5, i] = f1\n",
    "        if f1 >= 0.899 and SWA:\n",
    "            sess.run(swa_op)\n",
    "        if f1 < (best_val - 0.002):\n",
    "            ft_epochs += 1\n",
    "        if f1 > (best_val - 0.02):\n",
    "            print(f\"Saving model with {f1}\")\n",
    "            np.save(f\"{model_path}/metrics.npy\", metrics)\n",
    "            os.mkdir(f\"{model_path}/RESWA26-{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}/\")\n",
    "            save_path = saver.save(sess, f\"{model_path}/RESWA26-{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}/model\")\n",
    "            if f1 > best_val:\n",
    "                best_val = f1\n",
    "    #if SWA:\n",
    "        #if f1 > 0.9:\n",
    "     #   sess.run(restore_weight_backups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = np.copy(train_y[-464:-(464-192)])\n",
    "np.mean(br)\n",
    "br[br < 0.4] = 0.\n",
    "#br[br < 0.6] = br[br < 0.6] ** 0.5\n",
    "#print(np.mean(br))\n",
    "train_y[-464:-(464-192)] = br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(save_weight_backups)\n",
    "#sess.run(swa_to_weights)\n",
    "val_loss, f1, error, haus, dice = calculate_metrics('all', al = 0.33, canopy_thresh = 75)\n",
    "save_path = saver.save(sess, f\"{model_path}/swaout43-high-{str(f1*100)[:2]}-{str(f1*100)[3]}/model\")\n",
    "#sess.run(restore_weight_backups)\n",
    "#i = '29m'\n",
    "#np.save(f\"{model_path}/metrics.npy\", metrics)\n",
    "#os.mkdir(f\"{model_path}/swaft-{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}/\")\n",
    "#save_path = saver.save(sess, f\"{model_path}/swaft-{str(i)}-{str(f1*100)[:2]}-{str(f1*100)[3]}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unapply2(x):\n",
    "        _max = 0.35\n",
    "        _min = 0.\n",
    "        midrange = (_max + _min) / 2\n",
    "        rng = 0.35\n",
    "        return x * (rng / 2) + midrange\n",
    "    \n",
    "def _apply2(x):\n",
    "    _max = 0.4\n",
    "    _min = 0.\n",
    "    x = np.clip(x, 0, 0.4)\n",
    "    midrange = (_max + _min) / 2\n",
    "    rng = 0.4\n",
    "    return (x - midrange) / (rng / 2)\n",
    "\n",
    "train_x[..., 10] = _unapply2(train_x[..., 10])\n",
    "train_x[..., 10] = _apply2(train_x[..., 10])\n",
    "test_x[..., 10] = _unapply2(test_x[..., 10])\n",
    "test_x[..., 10] = _apply2(test_x[..., 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "\n",
    "\n",
    "test_ids = [x for x in range(0, len(test_x))]\n",
    "\n",
    "def multiplot(matrices, nrows = 2, ncols = 4):\n",
    "    '''Docstring\n",
    "    \n",
    "         Parameters:\n",
    "          matrices (list):\n",
    "          nrows (int):\n",
    "          \n",
    "         Returns:\n",
    "          None\n",
    "    '''\n",
    "    fig, axs = plt.subplots(ncols=4, nrows = nrows)\n",
    "    fig.set_size_inches(18, 4*nrows)\n",
    "    to_iter = [[x for x in range(i, i + ncols + 1)] for i in range(0, nrows*ncols, ncols)]\n",
    "    for r in range(1, nrows + 1):\n",
    "        min_i = min(to_iter[r-1])\n",
    "        max_i = max(to_iter[r-1])\n",
    "        for i, matrix in enumerate(matrices[min_i:max_i]):\n",
    "            sns.heatmap(data = matrix, ax = axs[r - 1, i], vmin = 0., vmax = 0.9, cbar = False)\n",
    "            axs[r - 1, i].set_xlabel(\"\")\n",
    "            axs[r - 1, i].set_ylabel(\"\")\n",
    "            axs[r - 1, i].set_yticks([])\n",
    "            axs[r - 1, i].set_xticks([])\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(len(train_y))]\n",
    "diffs = np.zeros((len(train_ids), ))\n",
    "for idx in tnrange(0, len(train_ids) - 16, 20):\n",
    "    x_input = train_x[idx:idx + 20].reshape(20, 13, 28, 28, n_bands)\n",
    "    y = sess.run([fm], feed_dict={inp: x_input,\n",
    "                                  #inp_median: median_input,\n",
    "                                  length: np.full((20,), 12),\n",
    "                                  is_training: False,\n",
    "                                  })\n",
    "    y = np.array(y).reshape(20, 14, 14)\n",
    "    y[y > 0.4] = 1.0\n",
    "    y[y < 0.4] = 0.\n",
    "    diff = np.sum(y, axis = (1, 2)) - np.sum(train_y[idx: idx + 20], axis = (1, 2))\n",
    "    diffs[idx : idx + 20] = diff\n",
    "    \n",
    "\n",
    "data['diffs'] = diffs[:len(data)]\n",
    "data.to_csv(\"data_diffs-swa75.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = [x for x in range(train_x.shape[0])]\n",
    "#train_ids = [6792,7200,10805,12009,12037,12063,12158,12302]\n",
    "#train_ids = bad_veg\n",
    "start = 0\n",
    "#start = len(train_ids) - 70\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_ids = [train_ids[start], train_ids[start + 1], train_ids[start + 2],\n",
    "             train_ids[start + 3], train_ids[start + 4],\n",
    "             train_ids[start + 5], train_ids[start + 6], train_ids[start + 7]]\n",
    "preds = []\n",
    "trues = []\n",
    "\n",
    "print(matrix_ids)\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    x_input = x_batch_test2[idx].reshape(1, 5, 28, 28, n_bands)\n",
    "    \n",
    "    y = sess.run([fm], feed_dict={inp: x_input,\n",
    "                                  length: np.full((1,), 4),\n",
    "                                  is_training: False,\n",
    "                                    })\n",
    "    y = np.array(y).reshape(14, 14)    \n",
    "    preds.append(y)\n",
    "    print(np.mean(y))\n",
    "    true = y_batch_test2[idx].reshape(14, 14)\n",
    "    trues.append(true)\n",
    "    \n",
    "start += 8\n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4] + trues[4:] + preds[4:]\n",
    "multiplot(to_plot, nrows = 4, ncols = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train_ids = np.argwhere(diffs > 100)\n",
    "\n",
    "matrix_ids = [train_ids[start], train_ids[start + 1], train_ids[start + 2],\n",
    "             train_ids[start + 3], train_ids[start + 4],\n",
    "             train_ids[start + 5], train_ids[start + 6], train_ids[start + 7]]\n",
    "preds = []\n",
    "trues = []\n",
    "#print(start//4)\n",
    "print(matrix_ids)\n",
    "#sess.run(swa_to_weights)\n",
    "#sess.run(restore_weight_backups)\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    x_input = train_x[idx].reshape(1, 13, 28, 28, n_bands)\n",
    "    \n",
    "    #median_input = calc_median_input(x_input)\n",
    "    y = sess.run([fm], feed_dict={inp: x_input,\n",
    "                                  length: np.full((1,), 12),\n",
    "                                  is_training: False,\n",
    "                                    })\n",
    "    y = np.array(y).reshape(14, 14)    \n",
    "    preds.append(y)\n",
    "    true = train_y[idx].reshape(14, 14)\n",
    "    \n",
    "    #print(idx, (list(data.iloc[idx, 1])[0], list(data.iloc[idx, 2])[0]))#, np.array(new_model[idx[0]]),\n",
    "        #  np.array(old_model)[idx[0]])\n",
    "    #print(idx, data.iloc[idx, 0], data.iloc[idx, 1],      data.iloc[i, 2],)\n",
    "    trues.append(true)\n",
    "    \n",
    "start += 8\n",
    "\n",
    "to_plot = trues[0:4] + preds[0:4] + trues[4:] + preds[4:]\n",
    "multiplot(to_plot, nrows = 4, ncols = 4)\n",
    "# 17382, 17409,17423\n",
    "\n",
    "# Remove 17527\n",
    "#17810, 17820\n",
    "# rotation remove 17741, 17817, 17824, 17838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove4 = np.argwhere(np.mean(train_y[-146:], axis = (1, 2)) < 0.66).flatten()\n",
    "remove4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p37)",
   "language": "python",
   "name": "conda_tensorflow_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
